[{"categories":["Statistics"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\rPseudo-random number sampling\rFormula for Monte Carlo generation\rIn case of continuous distribution\rIn case of discrete distribution\r\rrunif()를 이용하여 다른 분포 만들어 보기\r이항분포\r다항 분포\r정규분포\r카이제곱 분포\rStudent’s t 분포\rF분포\r지수분포\r포아송 분포\r감마분포\r베타분포\r\r\rPseudo-random number sampling\rMonte-Carlo simulations를 하기 위해서는 알려진 확률분포를 따르는 random number를 만들 수 있어야 한다. 그래서 1950년도부터 pseudo-random number sampling algorithm이 개발되었다.\nGNU Scientific Library에 다양한 algorithm이 나와있다고 하나, C언어로 작성되어 있기에 확인해 볼 수 없었다. 그래서 비교적 계산속도는 늦어지겠지만, R함수를 이용해 다양한 distribution에서 random number를 만드는 함수를 만들어 보았다. 그리고 만들어 본 것들을 R에 내장된 stats 패키지와 비교해 보았다.\n\rFormula for Monte Carlo generation\r먼저 hogg책 4.8장에 나와있는 공식을 정리하였다. discret distribution도 만들고자 수업시간에 넘어간 Accept-Reject Algorithm까지 정리하였다.\nIn case of continuous distribution\rTheorem 4.8.1\nU~(0, 1) 이고 F가 continuous distirbution fucntion이라면, random variable X = F − 1(U)는 F의 distribution function을 가진다.\n(F가 strictly monotone아닌 경우에도 성립)\n\rIn case of discrete distribution\rAccept-Reject Algorithm 4.8.1\n먼저 우리는 pdf가 f(x)를 따르는 X를 구하고자 한다.\nY~pdfg(y)이고 U~unif(0, 1)일때, Y와 U가 독립이고 f(x) ≤ M g(x)일 경우 (단 M은 상수)\nY와 U를 만들고\n\r$U \\le \\frac{f(Y)}{M\\ g(Y)}$인경우 X = Y라고 하면\n\r\rX는 pdf f(x)를 가진다.\n(pdf를 normalizing하는 constants를 무시하고 알고리즘 사용 가능)\n\r\rrunif()를 이용하여 다른 분포 만들어 보기\r사실 runif()도 만들어 사용하고 싶었다. 사실 이 부분은 통계학이라기 보다는 컴퓨터 과학이나 처음부터 만들려고 찾아봤다. R에서는 “Mersenne-Twister” 방법을 사용한다고 한다. 무려 219937 − 1만큼 반복되어 유의미하나 624만의 반복된 수를 알면 seed수를 알 수 있다고 한다.\r그 코드(C)가 영어 위키백과에도 나와있으나, 이를 R로 구현시 크기가 커 문제가 있다고 하고, 이를 다시 unif 분포로 만들기 어려워, 상대적으로 간단한 Linear congruential generator 방식을 이용해 runif()를 만들어 봤다. 이 방식은 초기값에 따라 난수의 질이 달라질 수 있고 마지막 난수만 알면 그 다음의 sequence를 예측 할 수 있기에 암호학 적으로 안전하지 않다고 한다. 먼저 방식은 다음과 같다.\n양수 m과 0 \u003c a \u003c m, 0 ≤ c \u003c m, 0 ≤ X0 \u003c m 를 만족하는 a, c, X0에 대해서 난수는 아래와 같다.\n\nXn + 1 = (aXn + c) mod m\n코드로 구현하면 다음과 같다.\nlcp_unif =function(n, m,a,c,seed){\rresult =rep(0,n)\rresult[1] =(a*seed+c) %%m\rfor(i in 2:n){\rresult[i] =(a*result[i-1]+c) %%m\r}\rreturn( result /m )\r}\r모수 m, a, c 를 정하는 다양한 방식이 있으나, POSIX [de]rand48에서 사용하는 값을 사용하였다.\nrandom =lcp_unif(1000, m=2**48,a= 25214903917,c=11,seed=2020)\r\rtb =tibble(r=runif(1000),\rlcp=lcp_unif(1000, m=2**48,a= 25214903917,c=11,seed=2020)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\rtb %\u003e%ggplot(aes(x=y,col=func)) +geom_freqpoly()\r하지만 이 방식은 앞서 말한것 처럼 연속된 난수들간에 상관관계가 존재하기에 좋은품질의 유사난수라고 하기는 어렵다. 따라서 우리가 사용하려고 하는 몬테 카를로 시뮬레이션에 적합하지 않다. 따라서 “Mersenne-Twister”기반인 runif(1)함수를 바탕으로 아래의 함수를 만들어 보았다. (물론 Mersenne-Twister도 동일한 문제에서 자유로울 수는 없으나, 상대적인 관점에서 사용하게 되었다.)\n\r이항분포\rlcp_binom =function(n, size, prob){\rresult=rep(0,n)\rfor(i in 1:n){\rx=0\rfor(j in 1:size){\ru =runif(1)\rif( u \u003cprob){ result[i]=result[i]+1 }\r}\r}\rreturn(result)\r}\rn=10000인 B(10,0.4)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rbinom(10000,10,0.4),\rlcp=lcp_binom(10000,10,0.4)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram(breaks=0:10-0.5,aes(y=..density..))+facet_wrap(~func)\r\r다항 분포\rlcp_multinom =function(n, size, prob){\rresult=matrix(0,ncol=n,nrow=length(prob))\rprob =c(prob,1)\rfor(i in 1:n){\rx=0\rfor(j in 1:size){\ru =runif(1)\rif( u \u003c=prob[1] ){ result[1,i]=result[1,i]+1 }\rfor(k in 2:length(prob)){\rif( prob[k-1]\u003cu \u0026u \u003c=prob[k] ){ result[k-1,i]=result[k-1,i]+1 }\r}\r}\r}\rreturn(result)\r}\rn=10인 multinormial(0.1,0.2,0.8)인 분포이다.\nset.seed(2020)\rrmultinom(10,5,c(0.1,0.2,0.8))\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r## [1,] 1 0 0 0 0 1 1 0 2 0\r## [2,] 0 1 0 1 1 1 1 1 1 0\r## [3,] 4 4 5 4 4 3 3 4 2 5\rlcp_multinom(10,5,c(0.1,0.2,0.8))\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r## [1,] 2 1 2 1 0 1 0 1 0 1\r## [2,] 0 4 0 3 2 4 3 4 4 3\r## [3,] 3 0 3 1 3 0 2 0 1 1\r\r정규분포\r정규분포의 경우 2개의 모수 μ와 σ2가 필요하므로 2개의 unif가 필요하다. 방법으로는 책에 나와 있는 Box–Muller transform 방법과, 이를 조금 더 심화한 Marsaglia polar method, 그리고 더 심화하여 가장 최신의 방법인 Ziggurat algorithm이 있다.\n먼저 책에 나와 있는 Box–Muller방법을 이용해 만들어 보았다.\nlcp_norm1 =function(n, mean = 0, sd = 1){\rresult=rep(0,n)\rfor(i in 1:n){\ru1 =runif(1)\ru2 =runif(1)\rt =sqrt(-2*log(u1))*cos(2*pi*u2)\rresult[i]=t*sd +mean\r}\rreturn(result)\r}\r그리고 Box–Muller transform 방법을 심화시킨 Marsaglia polar method이다. 이는 unif를 따르는 2개의 U를 -1과 1사이의 좌표에 포함되는 x,y로 만들고 이들중 단위원에 포함되는 x,y를 바탕으로 한다. 앞에서 cos(2πU)를 $x\\over {\\sqrt s}$형태로 바꾸어 코사인 함수를 간접적으로 사용하게 된 것이다. 코드를 만들어 보면 아래와 같다.\nlcp_norm2 =function(n, mean = 0, sd = 1){\rresult=rep(0,n)\rfor(i in 1:n){\rs=1\rwhile(s\u003c=0 |s\u003e=1){\rx =runif(1)*2-1 # runif(1)만을 사용하기 위해 runif(1,-1,1)을 일부로 사용하지 않음\ry =runif(1)*2-1\rs =x**2 +y**2\r}\rt =sqrt(-2*log(s)/s) *x\rresult[i]=t*sd +mean\r}\rreturn(result)\r}\rn=10000인 N(1, 0.5)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rnorm(10000,1,0.5), lcp_BoxMuller=lcp_norm1(10000,1,0.5), lcp_Marsaglia=lcp_norm2(10000,1,0.5)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp_BoxMuller\",\"lcp_Marsaglia\"))\rtb %\u003e%ggplot(aes(x=y,col=func))+geom_freqpoly(aes(y=..density..))+facet_wrap(~func)+geom_vline(aes(xintercept = 1) , linetype = \"dashed\")\r위의 두 방법에서 알 수 있는 것과 같이 log와 sqrt 그리고 삼각함수 등이 사용되기에 계산에 시간이 걸린다는 단점이 있다. 이를 보완한 방법이 Ziggurat algorithm이다. 정규분포의 pdf를 같은 면적으로 분할한 후 그 layer 중 하나를 바탕으로 random generating을 한 후 합치는 방식이다. 코드는 생략하였다.\n\r카이제곱 분포\riid인 표준정규분포를 제곱하고 더하면 쉽게 만들 수 있다. Marsaglia polar method 방식의 정규분포를 이용하였다\nlcp_chisq =function(n, df){\rresult=rep(0,n)\rfor(i in 1:n){\rresult[i]=sum(lcp_norm2(df)**2)\r}\rreturn(result)\r}\rn=10000인 χ2(3)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rchisq(10000,3),\rlcp=lcp_chisq(10000,3)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram(breaks=0:10-0.5, aes(y=..density..))+facet_wrap(~func)\r\rStudent’s t 분포\rt분포도 정규분포와 카이제곱 분포를 통해 쉽게 만들 수 있다. $Z\\over \\sqrt {V/\\nu}$이므로 lcp_norm2와 lcp_chisq를 사용하여 만들었다.\nlcp_t =function(n, df){\rresult =lcp_norm2(n) /sapply(lcp_chisq(n, df),sqrt) *\rsqrt(df)\rreturn(result)\r}\rn=10000인 t(2)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rt(10000,2),\rlcp=lcp_t(10000,2)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram(breaks=-10:10/2, aes(y=..density..))+facet_wrap(~func)\r\rF분포\rF분포도 정의 $V_1/k_1\\over V_2/k_2$에 의해 lcp_chisq를 통해 간단히 만들 수 있다.\nlcp_f =function(n, df1, df2){\rresult =(lcp_chisq(n,df1)/df1) /(lcp_chisq(n, df2)/df2)\rreturn(result)\r}\rn=10000인 F(10,2)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rf(10000,10,2),\rlcp=lcp_f(10000,10,2)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram(breaks=-0:50/10, aes(y=..density..))+facet_wrap(~func)\r\r지수분포\rcontinuous 분포로 간단하게 만들 수 있다. (뒤에 만들 감마 분포에 alpha에 1을 대입해도 가능하다.)\nlcp_exp =function(n,rate=1){\rresult=rep(0,n)\rfor(i in 1:n){\rresult[i]=-(1/rate)*log(1-runif(1))\r}\rreturn(result)\r}\rn=10000인 exp(1.2)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rexp(10000,1.2),\rlcp=lcp_exp(10000,1.2)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram(breaks=0:40/10-0.1, aes(y=..density..))+facet_wrap(~func)\r\r포아송 분포\rHogg책에 나와있는, 지수분포를 활용한 방식으로 알고리즘을 구성해 보았다. 즉 위에 지수분포를 활용해 rejection을 이용하는 방식이다\nlcp_pois =function(n,lambda){\rresult=rep(-1,n)\rfor(i in 1:n){\rt=0\rwhile(t\u003c=1){\ru =runif(1)\ry =-(1/lambda)*log(1-u)\rt =t+y\rresult[i]=result[i]+1\r}\r}\rreturn(result)\r}\rn=10000인 pois(3)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rpois(10000,3),\rlcp=lcp_pois(10000,3)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram(breaks=0:10-0.5, aes(y=..density..))+facet_wrap(~func)\r\r감마분포\rComputer Methods for Sampling from Gamma, Beta, Poisson and Binomial Distributions\r(1973) J. H. Ahrens, Halifax, and U. Dieter, Graz에서 사용한 방식을 참고한다. 먼저 Gamma(1,1)분포의 pdf는 e − x형태로 간단한 지수함수형태이다. 따라서 iid한 Gamma(1,1)를 alpha개 더하고 beta(rate parameter)를 곱하면 Gamma(alpha,beta)가 될 것이다.\nlcp_gamma1 =function(n, shape, rate = 1, scale = 1/rate){\rif (!missing(rate) \u0026\u0026!missing(scale)) {\rif (abs(rate *scale -1) \u003c1e-15) \rwarning(\"specify 'rate' or 'scale' but not both\")\relse stop(\"specify 'rate' or 'scale' but not both\")\r}\rresult=rep(0,n)\rfor(j in 1:n){\ri=1\rp=1\rwhile(i!=shape+1){\rp=p*runif(1)\ri=i+1\r}\rresult[j]=-log(p)*scale\r}\rreturn(result)\r}\rn=10000인 Gamma(4, 2)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rgamma(10000,4,rate=2),\rlcp=lcp_gamma1(10000,4,rate=2)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram( aes(y=..density..))+facet_wrap(~func)\r그러나 shape parameter alpha가 정수인 경우에만 가능한 방법이다. 따라서 0 \u003c a ≤ 1인 경우에는 다른 방식이 필요하다.\nlcp_gamma_sub =function(shape, rate = 1, scale = 1/rate){\ri=1\rp=1\rresult=NA\rwhile(is.na(result)){\ru=runif(1)\rb=(exp(1)+shape)/exp(1)\rp=b*u\rif(p\u003e1){\rx=-log((b-p)/shape)\rif(runif(1)\u003c=x**(shape-1)){result=x}\r}else{\rx=p**(1/shape)\rif(runif(1)\u003c=exp(-x)){result=x}\r}\r}\rreturn(result)\r}\r\rlcp_gamma_sub(0.3) ; lcp_gamma_sub(0.3); lcp_gamma_sub(0.3) ; lcp_gamma_sub(0.3)\r## [1] 1.512437\r## [1] 0.0005884823\r## [1] 0.04528588\r## [1] 0.6667582\r이제 이 둘을 합쳐 모든 shape parameter alpha에 대해 성립하도록 만들어 보면 다음과 같다.\nlcp_gamma2 =function(n, shape, rate = 1, scale = 1/rate){\rresult=rep(0,n)\rm =shape %/%1\rf =shape -m\rfor(j in 1:n){\rif(m==0){\ry=0\r}else{\ry =lcp_gamma1(1,m)\r}\rif(f==0){\rz=0\r}else{\rz =lcp_gamma_sub(f)\r}\rresult[j]=(y+z)*scale\r}\rreturn(result)\r}\rn=10000인 Gamma(3.3, 3.3)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rgamma(10000,3.3,rate=2.2),\rlcp=lcp_gamma2(10000,3.3,rate=2.2)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram( aes(y=..density..))+facet_wrap(~func)\r추가적으로 위의 방식은 rejection이 많아 계산이 오래걸릴 수있다. A Convenient Way of Generating Gamma Random Variables Using Generalized Exponential Distribution (2007) Debasis Kundu \u0026 Rameshwar D. Gupta에 따르면나온 동일하게 기본적으로 Accept-Reject Algorithm방식을 활용하지만 rejection을 줄이는, 즉 좀더 빠르게 만들 수 있는 알고리즘을 알 수 있다.\n\r베타분포\r베타분포는 2개의 감마분포를 통해 계산할 수 있다. 즉 각각의 shape parameter가 α β인 두 표준감마분포 Xα Xβ에 대해 $X_\\alpha\\over X_\\alpha +X_\\beta$는 Beta(α, β)를 따르게 된다. 따라서 lcp_gamma2를 사용하여 코드는 다음과 같다.\nlcp_beta =function(n, shape1, shape2){\rx1 =lcp_gamma2(n,shape1)\rx2 =lcp_gamma2(n,shape2)\rreturn(x1/(x1+x2))\r}\rn=10000인 Beta(0.5, 0.5)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rbeta(10000,0.5, 0.5),\rlcp=lcp_beta(10000,0.5, 0.5)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram( aes(y=..density..))+facet_wrap(~func)\rn=10000인 Beta(2, 4)인 분포이다.\nset.seed(2020)\rtb =tibble(r=rbeta(10000,2,4),\rlcp=lcp_beta(10000,2,4)) %\u003e%gather(key=\"func\",value=\"y\",c(\"r\",\"lcp\"))\r\rtb %\u003e%ggplot(aes(x=y,fill=func))+geom_histogram( aes(y=..density..))+facet_wrap(~func)\r\r","description":"","tags":["R","Statistics","Monte Carlo generation","Monte Carlo","distribution"],"title":"Monte Carlo generation","uri":"/posts/stat/1/"},{"categories":["R"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\rR 마크다운의 기본원리\rYAML 헤더\r코드청크\r기본 문법\r\r\rR 마크다운의 기본원리\r먼저 R마크다운은 .md가 아니라 .Rmd이다. 즉 Rmd가 knitr패키지에 의해 md가 되고 이 파일을 pandoc이 처리하게 된다.\n하지만 수식이나 띄어쓰기 한글 등에서 종종 자주 문제가 발생한다. 실제로 Rmd 출력시 다양한 형태 심지어 ppt도 가능하나, 매우 형편없다. 개인적으로는 html_document이나 word_document가 좋다. 현 페이지는 blogdown::html_page를 사용하였다.\n\rYAML 헤더\r---\rlayout:page\rtitle:\"제목\"\rsubtitle:\"부제목\"\rauthor:\rname:\"[이청파](https://github.com/leechungpa/)\"\rdate:\"2020-06-23\"\routput:\rhtml_document:\rtoc:yes\rtoc_float:true\rhighlight:tango\rcode_folding:hide\rnumber_section:true\rself_contained:true\reditor_options:\rchunk_output_type:console\r---\r내가 선호하는 코드이다. date의 경우 Sys.Date로 가능하지만, 직접 입력하는걸 선호한다. 이 블로그의 코드는 아래와 같다.\n---\rtitle:R 마크다운\rauthor:이청파\rdate:'2020-06-26'\rslug:'r/4'\rcategories:\r-R\rtags:\r-R\r-R studio\r-R 마크다운\reditor_options:\rchunk_output_type:inline\routput:\rblogdown::html_page:\rtoc:true\rhighlight:tango\rnumber_section:true\rself_contained:true\r---\r야물 해더에 params : 를 이용할 수 있으나 잘 사용하지 않으므로 생략한다.\n\r코드청크\r코드청크는 쉽게 ctrl + alt + i 눌러서 만들자. 힘들게 하나하나 치지말고\n# ```{r setup, include=FALSE}\r# knitr::opts_chunk$set(echo = TRUE)\r# ```\r일반적으로 제일 위에 있는 코드이다. setup은 코드 별명으로 다른것 지정가능하나 setup일 경우 기본적으로 다른 코드 실행 전 한번 실행된다.\n뒤에는 코드옵션들이다. eval inclue echo message wranning message ressults등이 있다. 각 별명 옆에 적용가능하나 setup에서 볼 수 있는 것처럼 을 통해 글로벌옵션을 변경가능하다.\n\reval = FALSE : 코드만 표시\n\rinclude = FALSE : 코드만 실행\n\recho = FALSE : 코드만 생략 (warning,message 포함됨)\n\rresults = “hide” : 결과만 생략\n\r\r아래는 작업시 일반적으로 쓰는 코드이다.\n# ```{r setup, include=FALSE}\r# knitr::opts_chunk$set(error = TRUE)\r# ```\r추가적으로 cache = TRUE를 통해 해당청크(전역 불가능 : 의미가 없음)를 통해 오래걸리는 특정청크가 knitr할때마다 시간이 소비되는 것을 막을 수 있다. 하지만 해당청크에서만 적용되므로 cache = TRUE, dependson='다른청크별명'을 통해 함께 변화된 부분이 있을 경우 반영될 수 있게 만들어야 한다. Rmd 외적인 파일도 추적하고 싶으면 cache.extra=file.info(\"파일명\")를 사용하면 된다.\nknitr::clean_cache()를 통해 복잡할 경우 지울 수 있다.\n최종시에는 아래와 같이 사용한다.\n# ```{r setup, include=FALSE}\r# knitr::opts_chunk$set(warning = FALSE, message = FALSE)\r# ```\r\r기본 문법\rHelp - Cheatsheets - R markdown cheat sheet 를 확인해 보자\n진하게, 기울기, 번호 등등이 가능하다. 더 복잡한 것을 원하면 CSS을 공부하자. 물론 word_document인경우 워드프로세스를 공부하자.\n\r","description":"","tags":["R","R studio","R 마크다운"],"title":"R 마크다운","uri":"/posts/r/4/"},{"categories":["R","data mining"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\rR 기반의 데이터 마이닝 정리\r예측력 회귀 모형 비교 기준\r예측력 분류 모형 비교 기준\r\rRegression\r모형 해석\r모형의 타당성 검토\r가법모형과 승법모형\r코드\r\rLogistic Regression\r모형 해석\r코드\r\rNeural Network\rsensitivity analysis\rhidden nodes : combination function + activation (squashing) function\routput layer nodes\rtraining the networks\r실용적인 tips\r코드\r\rDecision Tree\rCART 의사결정 나무 특징\r분류 나무의 분할 방법 : 불순도 측정\rCART의 feture selection 기준\r대안 분할 surrogate split\r\rPruning 가지치기\r코드\r\rEnsemble Methods\rBagging : Boostrapping AGGregatING\r과정\r코드\r\rBoosting\rAdaBoost\r코드\r\rGradient Boost\r\rRandom Forest\r과정\r개별 분류 모델의 결과 aggregating 방법\rOOB Error : Out Of Bag error\r변수 중요도\r코드\r코드 : 회귀분석의 경우\r\rClustering : Unsupervised Learnging\r표준화 standardization\r군집화의 기준 : distance\rdistance measures\r\rHierarchical clustering 계층적 군집분석\r과정\rdendrogram 고드름 그림\r코드\r\rK-means clustering\r과정\rK 결정법\rElbow Point\rsilhouette\r코드\r\r\rK-medioid clustering\r코드\r\rDensity-based clustering\r2개의 모수\r코드\r\r\r\r\rR 기반의 데이터 마이닝 정리\r설명력 위주 모형 : 통계모형\n\r예측력은 떨어지나 모형을 이해하기 좋음\n\r비교기준 : R2\n\rX ~ Y 의 인과관계를 얼마나 설명하는지\n\r선형이므로 오버피팅을 무시해도됨. 즉 R2가 중요\n\r\r\r예측력 위주 모형 : 예측력은 뛰어나지만 이해와 해석이 어려움\n\r미래의 Y값을 얼마나 잘 예측하는지가 기준\n\r머신러닝은 비선형 모델로 오버피팅의 위험\n\rtrain set과 test set을 분할하여 overfiting 막음\r\r\r예측력 회귀 모형 비교 기준\r아래의 기준들은 전부 test data를 사용해 계산함\n예측 결정계수 R2\r\r\nR2 = corr(y, ŷ)2\n평균절대오차 MAE : 절대적인 오차의평균을 이용\r\r\n$$MAE = {1 \\over n} \\sum |y - \\hat y |$$\nMean absolute percentage error MAPE : 실제값 대비 얼마나 예측값이 차이가 있었는지 %로 표현\r\r\n$$MAPE = {100\\% \\over n} \\sum {|y - \\hat y | \\over |y|}$$\nMean squred error MSE : 절대값이 아닌 제곱을 취한 지표\r\r\n$$MAPE = {100\\% \\over n} \\sum (y - \\hat y )^2$$\n\r예측력 분류 모형 비교 기준\rAccuracy 계열\r\r단점 : 작위성이 있다 (측도에 따라, cut-off에 따라 순위변동 가능)\n모형 작성시 : sensitivity(TPR), specificity(TNR)\n현장 적용시 : PPV, NPV\n\r정확도 : $a+d \\over a+b+c+d$\n\r민감도 : TPR\n\r특이도 : TNR\n\r정밀도 : PPV\n\rF-1 score : TPR과 PPV의 조화평균\n\rBCR : TPR과 TNR의 기하평균\n\r\rROC 계열\r\rROC : 같은 기술이라면 특이도와 민감도는 반비례관계를 이용한 그래프\n- x축 : 1-특이도\r- y축 : 민감도\rAUROC : area under the ROC curve\n- 일반적으로 적당한 기준 : 75% 이상\r- 최소 기준 : 65% 이상\rLift Chart 계열\r\r3가지 종류(Response, Captured Response, Lift) 중 어느것을 사용하던 결과는 동일\nLift Chart : 발생확률이 작은 순으로 정렬 후 구간화\n- x축 : 구간화된 sample size\r- y축 : 반응률 Response 또는 반응검출률 Captured Response 또는 향상도 Lift\r- 반응률 Response : 상위등급이 높게나오고 급락하면 good\r- 향상도 Lift : 상위등급은 1보다 크고, 하위등급은 0에 가까울수록 good\rcummulative Lift Chart : 갑자기 오르거나 내리는등 일관되지 않은 부분이 존재하여 cumulative 선호\n- 누적 반응률 Response : 경사가 급하면 좋고, unif하면 나쁨\r- 누적 향상도 Lift : 1로 하강하며 상위등급이 클수록 good\r- 누적 반응 검출률 captured response : 유일하게 상승\rCumulative accruacy profile CAP : 지니계수 개념\n- 클수록 좋음\r- ( 완벽한 분류시스템 누적반응검출률 면적 - 모형의 누적반응검출률 면적 ) / ( 완벽한 분류시스템 누적반응검출률 면적 - 랜덤한 누적반응검출률 면적 )\rProfit chart : maximize profit = income - cost\nK-S 통계량량\r\r불량 누적분포와 우량 누적분포의 차이가 가장 큰 값\n- 차이가 클수록 좋음\rcf. 이론적 분포함수 : 누적분포함수, 실제 data의 분포함수 : 경험분포함수\n\r\rRegression\r간섭효과 comfuounding effect 제거 방안\r\r\r통제 control : 상수화 -\u003e 일정하게 고정\n\r데이터 확장 : X변수 추가 수집 -\u003e 간섭효과를 양성화\n\r\r범주형 독립 변수\r\r\r지시변수 사용 : 0 또는 1\r\rR2 증가 방법\r\r유의한 X변수 발굴\n- 독립적이고 다양한 X변수일 수록 유리\r범주형 X변수의 교호작용 반영\n변수 선택\r\r중요한 소수의 예측변수를 찾아낸는 것이 중요 : 일반적으로 AIC 기준\n\rall subsets\n\rbackwrd elimination\n\rforward selection\n\rstepwise elimination\n\r\r모형 해석\r기울기 β\r\r수학적 해석보다는 실무적 해석이 중요\n절편은 데이터 범위를 벗어난 경우 의미없으므로 해석하지 않는다.\n모수β의 p-value : 기울기 유의성 검정\r\rp-value가 0.05보다 작으면 H0 : β = 0을 기각 : 즉 유의한 기울기\n결정계수 R2\r\rY의 총 변동량 중에서 X에 의해서 설명된 분량 : 즉 회귀모형의 설명력\n- $R^2$이 1에 가까울 수록 완전히 설명\rAdjusted R2 : X변수의 수가 많을수록 좋아지는 R2의 overfitting의 문제를 반영\n모형의 p-value : 회귀모형의 유효성\r\rp-value가 0.05보다 작으면 H0 : all βi = 0을 기각 : 즉 적어도 하나 이상의 설명변수가 유의하다\ncf. R2와 p-value의 관련성\n- 높은 R-squre, 낮은 p-value : 데이터 품질이 높은 경우\r- 낮은 R-squre, 낮은 p-value : X변수 추가 발굴 (금융 데이터)\r- 낮은 R-squre, 높은 p-value :유의하지 않은 X변수로 구성된 회귀분석\r- 높은 R-squre, 높은 p-value : 불가능\rR2는 분야별 유연한 기준 필요\n- 의학 약학 분야와 같이 실험데이터는 인과관계 단순 : R-squre 높게 나옴\r- 금융 경제 분야와 같이 관찰 데이터는 많은 변수와 인과관계 복잡 : R-squre 낮게 나옴\r\r모형의 타당성 검토\r선형회귀의 기본 가정 : p-value 계산시 F분포를 이용하기에 필요\n\r정규성 : 오차항의 분포가 평균이 0인 정규성\n\rnormal QQ plot\r\r등분산성 : 오차항의 분산이 동일\n\rŷ에 따른 잔차 그래프(residuals plot)가 메가폰 형태 같은 것이 없어야 한다.\n\rŷ 증가시 R2 상승의 경우 Y변수 변환 필요 : log(or sqrt) scaling\n\r분산은 일정하나 ŷ 증가시 추세가 존재할 경우 추가 X변수 발굴 필요\n\r\r독립성 : 오차항들이 서로 독립\n\r\r잔차 분석 : 모형 추정 후 오차의 추정치인 잔차를 통해 위의 가정들을 검토 가능\n\r가법모형과 승법모형\r가법모형 : 더하기만 있는 형태로 곱하기는 없음\n승법모형 : 교호작용과 상호작용 포함\n- 고차(교호작용)가 유의하면 저차가 유의하지 않아도 포함 : 즉 교호작용이 유의하면 main effect에서 유의하지 않아도 포함\r- 참고로 $$X$$와 $$X^2$$ 사이의 다중 공산성은 거의 없음 : 다중 공산성은 직선의 관계에서 강하게 발생\r\r코드\r### Regression ###\r\r# indicator variables\rusedcar2$Ind1\u003c-as.numeric(usedcar2$Color =='white')\rusedcar2$Ind2\u003c-as.numeric(usedcar2$Color =='silver')\r\r# training and test data\rset.seed(1234)\ri =sample(1:nrow(usedcar2), round(nrow(usedcar2)*0.7)) #70% for training data, 30% for testdata\rtrain =usedcar2[i,] \rtest =usedcar2[-i,]\r\r\r\r# regression fitting\rlm_used\u003c-lm(Price ~Odometer +Ind1 +Ind2 +Ind1:Odometer, data=train)\rsummary(lm_used)\r## ## Call:\r## lm(formula = Price ~ Odometer + Ind1 + Ind2 + Ind1:Odometer, ## data = train)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -680.08 -161.16 2.81 167.01 775.62 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 1.699e+04 2.639e+02 64.405 \u003c 2e-16 ***\r## Odometer -6.410e-02 6.817e-03 -9.403 9.72e-14 ***\r## Ind1 -9.388e+02 4.680e+02 -2.006 0.049018 * ## Ind2 3.321e+02 8.983e+01 3.697 0.000451 ***\r## Odometer:Ind1 2.689e-02 1.213e-02 2.217 0.030155 * ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 281.6 on 65 degrees of freedom\r## Multiple R-squared: 0.7268, Adjusted R-squared: 0.71 ## F-statistic: 43.24 on 4 and 65 DF, p-value: \u003c 2.2e-16\r# Trellis plot : 범주별 그래프\rlibrary(lattice)\rmypanel \u003c-function(x, y) {\rpanel.xyplot(x, y)\rpanel.loess(x, y, col=\"red\", lwd=2, lty=2) #loess : 비선형 회귀분석의 이름 \rpanel.lmline(x, y, col=\"black\", lwd=2, lty=3)\r}\rxyplot(Price~Odometer|Color,data=usedcar2,panel=mypanel) # color라는 범주별 그래프를 그려줌\r# regression diagnostics\rplot(lm_used,which=2) ## QQ plot\rplot(lm_used,which=1) ## fitted values vs. residuals\r# stepwise\rls_st =step(lm_used, direction='both')\r## Start: AIC=794.46\r## Price ~ Odometer + Ind1 + Ind2 + Ind1:Odometer\r## ## Df Sum of Sq RSS AIC\r## \u003cnone\u003e 5152676 794.46\r## - Odometer:Ind1 1 389490 5542166 797.56\r## - Ind2 1 1083262 6235938 805.81\r### Model Evaluation for Regression ###\r# predicted values\rpred1 =predict(ls_st, newdata=test, type='response')\r\r# predictive R^2\rcor(test$Price, pred1)^2\r## [1] 0.674462\r# MAE\rmean(abs(test$Price -pred1))\r## [1] 235.4435\r# MAPE\rmean(abs(test$Price -pred1)/abs(test$Price))*100\r## [1] 1.57497\r# RMSE\rsqrt(mean((test$Price -pred1)^2))\r## [1] 288.0425\r\r\rLogistic Regression\r설명력위주의 분류분석 : 종속변수가 범주형 변수\n선형모델 : cut-off에 따른 hyperplane 분류경계선 자체는 선형\n- 오분류 많이 발생 가능\rcut-off기준 : 일반적으로 0.5이나, 불균형자료(imbalanced data)의 경우 P(y=1)가 cut-off값이 됨\n\nlogit P(y = i) = β0 + β1x\n\r장점\n\rX가 범주형인경우 one-hot encoding을 통해 지시변수로 사용 가능\r\r단점\n\rNA가 많은 경우 사용 불가능\n\r교호작용의 있을 경우 해석이 어려움\n\r\r\r모형 해석\rβ\r\rX1이 1 커지면 eβ1배 만큼 오즈가 변함 : 오즈비 개념\nβ의 p-value\n\rAUROC\n\r\rR2가 없으므로, 대신 AUROC 사용\ndeviance의 p-value\r\r모형의 유의성 확인으로 null model의 deviance와 fitted model의 deviance 비교\n\r코드\r### Logistic Regression ###\rcomplete=complete.cases(directmail)\rtable(complete)\r## complete\r## FALSE TRUE ## 273 9727\rdirectmail1\u003c-directmail[complete,]\rdim(directmail1)\r## [1] 9727 9\r# training and test data\rset.seed(1234)\ri =sample(1:ncol(directmail1), round(ncol(directmail1)*0.7)) #70% for training data, 30% for testdata\rtrain =directmail1[i,] \rtest =directmail1[-i,]\r\r# fitting full model\rfull_model =glm(RESPOND~. , family=\"binomial\",data=train)\rsummary(full_model)\r## ## Call:\r## glm(formula = RESPOND ~ ., family = \"binomial\", data = train)\r## ## Deviance Residuals: ## [1] 0 0 0 0 0 0\r## ## Coefficients: (3 not defined because of singularities)\r## Estimate Std. Error z value Pr(\u003e|z|)\r## (Intercept) -2.457e+01 5.126e+06 0 1\r## AGE 1.728e-16 2.443e+04 0 1\r## BUY18 -3.387e-14 3.080e+05 0 1\r## CLIMATE NA NA NA NA\r## FICO -3.081e-16 5.703e+03 0 1\r## INCOME -4.179e-15 1.511e+04 0 1\r## MARRIED 5.936e-14 9.330e+05 0 1\r## OWNHOME NA NA NA NA\r## GENDERM NA NA NA NA\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 0.000e+00 on 5 degrees of freedom\r## Residual deviance: 2.572e-10 on 0 degrees of freedom\r## AIC: 12\r## ## Number of Fisher Scoring iterations: 23\r# model's significance\rempty_model =glm(RESPOND~1, family=\"binomial\", data=train)\ranova(full_model, empty_model, test=\"Chisq\")\r## Analysis of Deviance Table\r## ## Model 1: RESPOND ~ AGE + BUY18 + CLIMATE + FICO + INCOME + MARRIED + OWNHOME + ## GENDER\r## Model 2: RESPOND ~ 1\r## Resid. Df Resid. Dev Df Deviance Pr(\u003eChi)\r## 1 0 2.572e-10 ## 2 5 2.572e-10 -5 0 1\r# variable selection\rstep_model =step(full_model, direction='both') # direction='backward' is default\r## Start: AIC=12\r## RESPOND ~ AGE + BUY18 + CLIMATE + FICO + INCOME + MARRIED + OWNHOME + ## GENDER\r## ## ## Step: AIC=12\r## RESPOND ~ AGE + BUY18 + CLIMATE + FICO + INCOME + MARRIED + OWNHOME\r## ## ## Step: AIC=12\r## RESPOND ~ AGE + BUY18 + CLIMATE + FICO + INCOME + MARRIED\r## ## ## Step: AIC=12\r## RESPOND ~ AGE + BUY18 + FICO + INCOME + MARRIED\r## ## Df Deviance AIC\r## - AGE 1 2.572e-10 10\r## - BUY18 1 2.572e-10 10\r## - FICO 1 2.572e-10 10\r## - INCOME 1 2.572e-10 10\r## - MARRIED 1 2.572e-10 10\r## \u003cnone\u003e 2.572e-10 12\r## ## Step: AIC=10\r## RESPOND ~ BUY18 + FICO + INCOME + MARRIED\r## ## Df Deviance AIC\r## - BUY18 1 2.572e-10 8\r## - FICO 1 2.572e-10 8\r## - INCOME 1 2.572e-10 8\r## - MARRIED 1 2.572e-10 8\r## \u003cnone\u003e 2.572e-10 10\r## + AGE 1 2.572e-10 12\r## + OWNHOME 1 2.572e-10 12\r## + GENDER 1 2.572e-10 12\r## ## Step: AIC=8\r## RESPOND ~ FICO + INCOME + MARRIED\r## ## Df Deviance AIC\r## - FICO 1 2.572e-10 6\r## - INCOME 1 2.572e-10 6\r## - MARRIED 1 2.572e-10 6\r## \u003cnone\u003e 2.572e-10 8\r## + AGE 1 2.572e-10 10\r## + BUY18 1 2.572e-10 10\r## + OWNHOME 1 2.572e-10 10\r## + GENDER 1 2.572e-10 10\r## ## Step: AIC=6\r## RESPOND ~ INCOME + MARRIED\r## ## Df Deviance AIC\r## - INCOME 1 2.572e-10 4\r## - MARRIED 1 2.572e-10 4\r## \u003cnone\u003e 2.572e-10 6\r## + AGE 1 2.572e-10 8\r## + BUY18 1 2.572e-10 8\r## + FICO 1 2.572e-10 8\r## + OWNHOME 1 2.572e-10 8\r## + GENDER 1 2.572e-10 8\r## ## Step: AIC=4\r## RESPOND ~ MARRIED\r## ## Df Deviance AIC\r## - MARRIED 1 2.572e-10 2\r## \u003cnone\u003e 2.572e-10 4\r## + AGE 1 2.572e-10 6\r## + BUY18 1 2.572e-10 6\r## + FICO 1 2.572e-10 6\r## + INCOME 1 2.572e-10 6\r## + OWNHOME 1 2.572e-10 6\r## + GENDER 1 2.572e-10 6\r## ## Step: AIC=2\r## RESPOND ~ 1\r## ## Df Deviance AIC\r## \u003cnone\u003e 2.572e-10 2\r## + AGE 1 2.572e-10 4\r## + BUY18 1 2.572e-10 4\r## + FICO 1 2.572e-10 4\r## + INCOME 1 2.572e-10 4\r## + MARRIED 1 2.572e-10 4\r## + OWNHOME 1 2.572e-10 4\r## + GENDER 1 2.572e-10 4\r# predicted probability\rprob_pred1 =predict(step_model, newdata=test, type='response')\r\r\r# odds ratio\rexp(coef(step_model))\r## (Intercept) ## 2.143345e-11\ry_pred1 =as.numeric(prob_pred1 \u003e0.075)\rtab1=table(test$RESPOND, y_pred1)\rprint(tab1)\r## y_pred1\r## 0\r## 0 8992\r## 1 729\rsum(diag(tab1))/sum(tab1)\r## [1] 0.9250077\r# ROC curve\rlibrary(pROC)\r## Type 'citation(\"pROC\")' for a citation.\r## ## Attaching package: 'pROC'\r## The following objects are masked from 'package:stats':\r## ## cov, smooth, var\rroccurve1 \u003c-roc(test$RESPOND ~prob_pred1)\r## Setting levels: control = 0, case = 1\r## Setting direction: controls \u003c cases\rplot(roccurve1, col=\"red\", print.auc=TRUE, print.auc.adj=c(1,-7), auc.polygon=TRUE)\r# Scored data\rscored_dat =cbind(prob_pred1,test$RESPOND)\rhead(scored_dat)\r## prob_pred1 ## 3 2.143345e-11 0\r## 7 2.143345e-11 0\r## 9 2.143345e-11 0\r## 11 2.143345e-11 0\r## 12 2.143345e-11 0\r## 13 2.143345e-11 0\rhead(scored_dat[order(-prob_pred1),],30)\r## prob_pred1 ## 3 2.143345e-11 0\r## 7 2.143345e-11 0\r## 9 2.143345e-11 0\r## 11 2.143345e-11 0\r## 12 2.143345e-11 0\r## 13 2.143345e-11 0\r## 14 2.143345e-11 1\r## 15 2.143345e-11 0\r## 16 2.143345e-11 1\r## 17 2.143345e-11 0\r## 18 2.143345e-11 0\r## 19 2.143345e-11 0\r## 20 2.143345e-11 0\r## 21 2.143345e-11 1\r## 22 2.143345e-11 0\r## 23 2.143345e-11 0\r## 24 2.143345e-11 0\r## 25 2.143345e-11 0\r## 26 2.143345e-11 0\r## 27 2.143345e-11 0\r## 28 2.143345e-11 0\r## 29 2.143345e-11 0\r## 30 2.143345e-11 0\r## 31 2.143345e-11 0\r## 32 2.143345e-11 0\r## 33 2.143345e-11 0\r## 34 2.143345e-11 0\r## 35 2.143345e-11 0\r## 36 2.143345e-11 0\r## 37 2.143345e-11 0\r# List Chart 1\rlibrary(BCA)\rlayout(matrix(c(1,2), 2, 1))\rtest$RESPOND=factor(test$RESPOND)\rlift.chart(\"step_model\", data=test, targLevel=\"1\", trueResp=0.07, type=\"incremental\", sub=\"Test\")\rlift.chart(\"step_model\", data=test, targLevel=\"1\", trueResp=0.07, type=\"cumulative\", sub=\"Test\")\rBCA 패키지는 Y값이 factor\ntargLevel=“1” : 1을 센다는 의미\ntype=“incremental” : 누적이 아닌경우\ntype=“cumulative” : 누적인 경우\ntrueResp=0.07 : 임의의 가이드라인\n# List Chart 2\rlibrary(ROCR)\rpred1 =prediction(prob_pred1, test$RESPOND)\rperf1 =performance(pred1,\"lift\",\"rpp\")\rplot(perf1, main=\"lift chart\")\r# plot(perf2, add=TRUE, lty=2, col=\"red\")\rlegend(0.7, 3, legend=c(\"Model 1\", \"Model 2\"),col=c(\"black\", \"red\"), lty=1:2)\r\r\r# K-S statistics\rks.test(prob_pred1[test$RESPOND==1], prob_pred1[test$RESPOND==0])\r\r\rNeural Network\r비선형 통계 모형으로 universal approximator 범용 근사기\n장점\n\r예측력은 좋음\n\r회귀 모형, 분류 모형 둘다 가능\n\r\r단점\n\r추정해야 할 값들이 많음\n\r좋은 결과를 위해 많이 대입해 시도해봐야 함하며\r\r학습에 많은 시간이 걸림\n\rblack box : 해석이 어려움\n\roverfitting의 위험이 큼\n\rinput values가 반드시 numeric\n\r입증verification이 어렵다\n\r\rsensitivity analysis\rNN을 해석하기 위한 방법이지만, 다변량 분석을 단변량 분석처럼 하기에 추천하지 않는 방법\n모든 input value의 평균값을 NN모형에 대입\n\r하나의 x변수를 바꿀때(min에서 max로) 변화하는 모형에 따른 output의 변화 측정\n\r하나의 x변수를 조금씩 늘려가면서 민감도 그림을 그릴 수 도 있음\r\rsensitive inputs을 중요한 변수로 판단함\n\r\r\rhidden nodes : combination function + activation (squashing) function\r활성함수로 보통 sigmoid(logitstic) functions와 hypertangent functions 사용\n\rsigmoid functions $sigmoid(x)={e^x\\over {1+e^x}}$ : 0~1 값 반환\n\rhypertangent functions $tanh(x)={1-e^x\\over {1+e^x}}$ : -1~1값 반환\n\r\r\routput layer nodes\r회귀 목적 : identity activation function만을 사용\n분류 목적 : sigmoid function 사용\n- 범주형의 경우 class마다 확률인 0~1을 가지는 하나의 output node\r\rtraining the networks\r오차에 비례하는 objective function를 줄어들게 학습함\ncross entropy를 작게 만드는 계수 추정\n\r초기 weight 설정하여 error 계산\n\rgradient descent method에 따라 weight를 조정하며 error가 더이상 줄어들지 않는 지점까지 판단\n\r\r\r실용적인 tips\r단순하면서 예측을 잘하는 모형이 좋은 모형\n\r먼저 통계모형을 사용해(no hidden layer) 만들어 보기\n\r\r\r그리고 노드를 하나씩 추가하며 성능 확인하기\n\rgeneralization error(일종의 unseen data의 error)가 증가하기 전까지 추가 및 변형\n\r\rNN는 input data에 매우 민감한 방법 : 좋은 데이터 타입 필요\r\r\r분산이 비슷한 연속형 변수들\n\r적절한 변수 개수\n\r범주형 변수는 지시변수를 사용하고, 개수가 지시변수별로 비슷해야 한다.\n\r\r모든 변수가 0~1 또는 -1~1로 scale\r\r\r따라서 normalize by z-scores 필요 : $new\\ x={x-min\\over max-min}$\n\r정규화 normalize : 0~1사이의 수로 만들어 줌\n\r표준화 standardize : 평균0, 표준편차 1로 만들어줌\n\r\rcategorical variable의 경우 지시변수 사용\n\rordinal variables의 경우 equal spacing 사용 (0~1 or -1~1을 구간화)\n\r변수 선택도 좋은 방법 : decision tree를 만들어 variable importance를 계산해 변수 선택\n\r선형모형이 아닌, decision tree 같은 비선형 모형을 바탕으로 변수 선택해야 함\r\r\routlier가 있으면 성능을 저하시킴\r\r\r코드\r### Neural Network ###\rcomplete=complete.cases(directmail)\rdirectmail1\u003c-directmail[complete,]\rnobs=nrow(directmail1)\r\rnor =function(x) {(x-min(x))/(max(x)-min(x))}\r\rdirectmail1$AGE \u003c-nor(directmail1$AGE)\rdirectmail1$CLIMATE \u003c-nor(directmail1$CLIMATE)\rdirectmail1$FICO \u003c-nor(directmail1$FICO)\rdirectmail1$INCOME \u003c-nor(directmail1$INCOME)\r\rdirectmail1$GENDER \u003c-as.numeric(directmail1$GENDER =='F')\r\r\r\r# training and test data\rset.seed(1234)\ri =sample(1:nobs, round(nobs*0.7)) #70% for training data, 30% for testdata\rtrain =directmail1[i,] ; test =directmail1[-i,]\r\r\rlibrary(neuralnet)\rset.seed(1234)\rnn1\u003c-neuralnet(RESPOND~AGE+BUY18+CLIMATE+FICO+INCOME+MARRIED+OWNHOME+GENDER, \rdata=train, hidden=c(2,2), stepmax = 1e+05, threshold = 0.1, \ract.fct='logistic', linear.output=F) \rprint(nn1$weights)\r## [[1]]\r## [[1]][[1]]\r## [,1] [,2]\r## [1,] -1.5007954 -0.5678670\r## [2,] 3.7583278 6.3630250\r## [3,] 3.0686893 -6.9450319\r## [4,] -4.8835950 6.3408508\r## [5,] -0.7893066 1.0792173\r## [6,] -0.6800452 0.8605907\r## [7,] -1.5235982 0.7830834\r## [8,] 3.2645385 -4.8048407\r## [9,] 0.3456037 -0.5042786\r## ## [[1]][[2]]\r## [,1] [,2]\r## [1,] -31.02980 0.2934988\r## [2,] 56.36981 -0.4801595\r## [3,] 105.96162 -0.6858643\r## ## [[1]][[3]]\r## [,1]\r## [1,] -1.343509\r## [2,] -3.277906\r## [3,] 4.915556\rhead(nn1$net.result[[1]])\r## [,1]\r## 7663 0.06660508\r## 8238 0.06654296\r## 7362 0.09112853\r## 8308 0.06659674\r## 7475 0.06024468\r## 9452 0.07474474\rplot(nn1)\r뉴럴 네트워크는 초기값을 이용하기에 set.seed로 고정\nhidden = c(3,3) : 히든 노드의 수\nstepmax = 1e+06 : 반복 최대 횟수 (수렴하지 않으면 stepmax 까지)\nthreshold = 0.01 : 오차가 수렴하는 조건 (cross entropy \u003c 0.01)\nact.fct = ‘logistic’ : activation 함수\nlinear.output = F : Y변수가 연속형(회귀분석)이면 T, 분류분석이면 F\n# comparison\rpred1 \u003c-compute(nn1,covariate=test[,-1])\rhead(pred1$net.result,10)\r## [,1]\r## 2 0.06170793\r## 4 0.08481002\r## 6 0.08420344\r## 7 0.07952745\r## 8 0.05923194\r## 13 0.05210743\r## 14 0.04533934\r## 20 0.07912428\r## 22 0.06195479\r## 23 0.08710547\r다른것과 다르게 predict가 아닌 compute를 사용해 예측\ncovariate = test데이터에서 반응변수(“RESPOND”)를 제외한 행렬\nlibrary(pROC)\rroccurve1 \u003c-roc(test$RESPOND ~as.vector(pred1$net.result))\r## Setting levels: control = 0, case = 1\r## Setting direction: controls \u003c cases\rplot(roccurve1, col=\"red\", print.auc=TRUE, print.auc.adj=c(1,7), auc.polygon=TRUE)\r\r\rDecision Tree\r의사결정나무는 전체 데이터 집합을 partition\n간단한 모형 underfitting : 설명력 높고 예측력 낮다.\n복잡한 모형 overfitting일 : 설명력 낮고 예측력 높다. -\u003e 완전히 성장한 나무는 overfitting 되기 쉬움\n장점\n- Y가 범주형이든 연속형이등 가능 : 분류 나무, 회귀나무\r- 로지스틱 모형에 비해 더 계산이 빠름 : 신속한 판단 가능\r- 단변량 분할 one varaiable at a time\r- small n, large p data 가능\r- X 변수\r- 변수개수에 영향을 덜받음\r- 변수의 중요도 파악 가능 - 이상치 및 결측치 영향 최소화\r- 범주형 X변수 처리 용이\r- 지시변수 불필요\r- X 변수가 범주형이면 교효작용 찾는데 탁월한 의사결정 나무 유리\r- 범주들의 대범주화하여 X변수 선택 기능\r- 타 분석방법에 앞서 전처리 과정으로 사용가능\r- 신경망등 다른 분석에 선택된 변수 사용가능\r- 해성의 용이성\r- Y 변수가 소수 그룹일경우도 처리가 용이함\r단점\n- 분류경계선 hyperplabne 근처에서는 오분류 가능\r- 다른 방법에 비해 분류 정확도(예측도)가 낮을 수 있음\rCART 의사결정 나무 특징\r노드가 항상 2개로 분할\n숫자형 X변수의 경우 부등호 사용, 범주형 X변수의 경우 부분집합 포함 여부 사용\n불순도를 이용하여 greedy serach\n매 분할마다 하나의 X변수만 사용\n분류 나무의 분할 방법 : 불순도 측정\r불순도 함수\r\r\rGini impurity 지니 불순도 : $1-\\sum^{K}_{j=1} P_j^2$\n\rK : Y의 범주 개수, P_i : i번째 범주에 포함될 확률\n\r지니불순도 최댓값 0.5 : 완전히 균일분포(all P_i =1/K)인 경우\n\r지니불순도 최솟값 0 : 불순도가 작을수록 좋음\n\r\r엔트로피 Entropy : $-\\sum^{K}_{j=1} P_j log(P_j)$\n\r이탈도 Deviance : $-2\\sum^{K}_{j=1} n_j log(P_j)$\n\r\r불순도 측정 : 분할 이전의 노드의 불순도와, 분할 후 각각의 노드별 불순도를 측정\n\r분할 후 각각의 노드별 분순도를 표본수에 따라 가중평균\n\r분할 이후와 이전이 불순도가 감소했는지를 측정\n\rgreedy search를 통해 최적의 분할을 찾음\n\r\r\rCART의 feture selection 기준\rparameter를 조정을 통해 최적 찾음\nmaxdepth : leaf node의 최대 깊이를 제한\n\rmaxdepth 클수록 tree 커짐\r\rminsplit : 각 노드별 최소한의 관측치 수 제한\n\rminsplit 작을수록 tree 커짐\r\r최소 향상도 cp : complexity parmeter 분할시 최소한으로 작아져야 하는 불순도\n\rcp 작을수록 tree 커짐\r\r\r\r대안 분할 surrogate split\rmissing value가 있을 경우 다른 추가적인 대안 분할 기준\n- main split과 비슷한 속성이 있는 것을 surrogate split으로 이용함\r- NA일 경우 imputation(평균, 중앙값으로 NA 대체) 불필요\r\r\rPruning 가지치기\roverfitting을 막기위해 불필요한 가지 제거\n데이터를 3개지로 분할 : training data, pruning data, test data\npruning data를 사용\n\rpruning data는 test데이터가 아닌 다른 데이터\r\r교차검증 cross-validation로 예측오차를 계산\n\rtest data중 일부를 pruning data로 사용하고 이 과정을 여러번 반복\r\r예측오차가 가장 작은 모형 선택\n\r\r\r코드\r# training and test data\rset.seed(1234)\ri =sample(1:nrow(hmeq), round(nrow(hmeq)*0.6)) #60% for training data, 40% for test data\rtrain =hmeq[i,] \rtest =hmeq[-i,]\r\rlibrary(rpart)\r# default tree\rtree0 \u003c-rpart(BAD ~., data = train, method=\"class\")\r\rlibrary(rpart.plot)\rprp(tree0, type=4, extra=2, digits=3) # 시각화\rsummary(tree0)\r## Call:\r## rpart(formula = BAD ~ ., data = train, method = \"class\")\r## n= 3576 ## ## CP nsplit rel error xerror xstd\r## 1 0.03608480 0 1.0000000 1.0000000 0.03276489\r## 2 0.02232747 4 0.8389716 0.8714479 0.03109437\r## 3 0.01353180 6 0.7943166 0.8619756 0.03096181\r## 4 0.01217862 9 0.7496617 0.8565629 0.03088544\r## 5 0.01014885 13 0.7009472 0.8389716 0.03063408\r## 6 0.01000000 15 0.6806495 0.8322057 0.03053610\r## ## Variable importance\r## DELINQ DEBTINC CLNO LOAN VALUE NINQ MORTDUE YOJ JOB CLAGE ## 36 16 10 9 9 6 6 3 2 2 ## REASON ## 2 ## ## Node number 1: 3576 observations, complexity param=0.0360848\r## predicted class=0 expected loss=0.2066555 P(node) =1\r## class counts: 2837 739\r## probabilities: 0.793 0.207 ## left son=2 (2813 obs) right son=3 (763 obs)\r## Primary splits:\r## DELINQ \u003c 0.5 to the left, improve=115.06830, (327 missing)\r## DEBTINC \u003c 45.15491 to the left, improve= 73.83211, (763 missing)\r## DEROG \u003c 0.5 to the left, improve= 70.98588, (406 missing)\r## LOAN \u003c 5050 to the right, improve= 29.63802, (0 missing)\r## CLAGE \u003c 146.9731 to the right, improve= 27.65414, (181 missing)\r## Surrogate splits:\r## CLNO \u003c 47.5 to the left, agree=0.769, adj=0.011, (202 split)\r## ## Node number 2: 2813 observations, complexity param=0.0360848\r## predicted class=0 expected loss=0.1397085 P(node) =0.7866331\r## class counts: 2420 393\r## probabilities: 0.860 0.140 ## left son=4 (2776 obs) right son=5 (37 obs)\r## Primary splits:\r## DEBTINC \u003c 44.67086 to the left, improve=51.59042, (468 missing)\r## DEROG \u003c 1.5 to the left, improve=21.94472, (303 missing)\r## CLNO \u003c 2.5 to the right, improve=18.44249, (125 missing)\r## CLAGE \u003c 174.7841 to the right, improve=16.86937, (180 missing)\r## LOAN \u003c 5050 to the right, improve=16.23187, (0 missing)\r## ## Node number 3: 763 observations, complexity param=0.0360848\r## predicted class=0 expected loss=0.4534731 P(node) =0.2133669\r## class counts: 417 346\r## probabilities: 0.547 0.453 ## left son=6 (700 obs) right son=7 (63 obs)\r## Primary splits:\r## DELINQ \u003c 4.5 to the left, improve=27.68855, (4 missing)\r## DEBTINC \u003c 43.64351 to the left, improve=24.35699, (295 missing)\r## NINQ \u003c 2.5 to the left, improve=23.61305, (25 missing)\r## DEROG \u003c 0.5 to the left, improve=18.75757, (103 missing)\r## CLAGE \u003c 152.0833 to the right, improve=14.19029, (1 missing)\r## ## Node number 4: 2776 observations, complexity param=0.01217862\r## predicted class=0 expected loss=0.129683 P(node) =0.7762864\r## class counts: 2416 360\r## probabilities: 0.870 0.130 ## left son=8 (2729 obs) right son=9 (47 obs)\r## Primary splits:\r## CLNO \u003c 2.5 to the right, improve=19.34495, (125 missing)\r## LOAN \u003c 5050 to the right, improve=16.33724, (0 missing)\r## DEROG \u003c 1.5 to the left, improve=15.66174, (302 missing)\r## CLAGE \u003c 91.55739 to the right, improve=13.27388, (180 missing)\r## VALUE \u003c 21505 to the right, improve=12.14257, (25 missing)\r## ## Node number 5: 37 observations\r## predicted class=1 expected loss=0.1081081 P(node) =0.01034676\r## class counts: 4 33\r## probabilities: 0.108 0.892 ## ## Node number 6: 700 observations, complexity param=0.0360848\r## predicted class=0 expected loss=0.4128571 P(node) =0.1957494\r## class counts: 411 289\r## probabilities: 0.587 0.413 ## left son=12 (637 obs) right son=13 (63 obs)\r## Primary splits:\r## NINQ \u003c 3.5 to the left, improve=21.94178, (23 missing)\r## DEBTINC \u003c 43.64351 to the left, improve=20.35856, (253 missing)\r## CLAGE \u003c 152.0833 to the right, improve=16.58194, (1 missing)\r## DEROG \u003c 0.5 to the left, improve=15.53125, (93 missing)\r## JOB splits as LRLRLRR, improve=14.05560, (0 missing)\r## ## Node number 7: 63 observations\r## predicted class=1 expected loss=0.0952381 P(node) =0.01761745\r## class counts: 6 57\r## probabilities: 0.095 0.905 ## ## Node number 8: 2729 observations, complexity param=0.01014885\r## predicted class=0 expected loss=0.1220227 P(node) =0.7631432\r## class counts: 2396 333\r## probabilities: 0.878 0.122 ## left son=16 (2652 obs) right son=17 (77 obs)\r## Primary splits:\r## LOAN \u003c 5050 to the right, improve=17.522390, (0 missing)\r## DEROG \u003c 1.5 to the left, improve=14.739050, (302 missing)\r## CLAGE \u003c 172.5612 to the right, improve=12.230540, (145 missing)\r## VALUE \u003c 21505 to the right, improve=10.899230, (25 missing)\r## NINQ \u003c 4.5 to the left, improve= 7.890703, (260 missing)\r## ## Node number 9: 47 observations, complexity param=0.01217862\r## predicted class=1 expected loss=0.4255319 P(node) =0.01314318\r## class counts: 20 27\r## probabilities: 0.426 0.574 ## left son=18 (29 obs) right son=19 (18 obs)\r## Primary splits:\r## VALUE \u003c 75083.5 to the left, improve=10.564930, (0 missing)\r## YOJ \u003c 1.5 to the left, improve=10.563490, (11 missing)\r## JOB splits as LRRRR-R, improve= 9.478723, (0 missing)\r## MORTDUE \u003c 43381.5 to the left, improve= 8.437500, (15 missing)\r## NINQ \u003c 4.5 to the right, improve= 7.347144, (0 missing)\r## Surrogate splits:\r## JOB splits as LRRLR-R, agree=0.809, adj=0.500, (0 split)\r## REASON splits as LRL, agree=0.702, adj=0.222, (0 split)\r## LOAN \u003c 15350 to the left, agree=0.681, adj=0.167, (0 split)\r## MORTDUE \u003c 50478.5 to the left, agree=0.660, adj=0.111, (0 split)\r## NINQ \u003c 0.5 to the right, agree=0.660, adj=0.111, (0 split)\r## ## Node number 12: 637 observations, complexity param=0.02232747\r## predicted class=0 expected loss=0.3736264 P(node) =0.178132\r## class counts: 399 238\r## probabilities: 0.626 0.374 ## left son=24 (625 obs) right son=25 (12 obs)\r## Primary splits:\r## DEBTINC \u003c 43.64351 to the left, improve=17.02290, (213 missing)\r## LOAN \u003c 6050 to the right, improve=12.93077, (0 missing)\r## JOB splits as LRLRLLR, improve=12.45810, (0 missing)\r## DELINQ \u003c 1.5 to the left, improve=12.31275, (4 missing)\r## CLAGE \u003c 225.4314 to the right, improve=12.20299, (1 missing)\r## ## Node number 13: 63 observations\r## predicted class=1 expected loss=0.1904762 P(node) =0.01761745\r## class counts: 12 51\r## probabilities: 0.190 0.810 ## ## Node number 16: 2652 observations\r## predicted class=0 expected loss=0.112368 P(node) =0.7416107\r## class counts: 2354 298\r## probabilities: 0.888 0.112 ## ## Node number 17: 77 observations, complexity param=0.01014885\r## predicted class=0 expected loss=0.4545455 P(node) =0.02153244\r## class counts: 42 35\r## probabilities: 0.545 0.455 ## left son=34 (48 obs) right son=35 (29 obs)\r## Primary splits:\r## MORTDUE \u003c 45250 to the right, improve=13.065200, (11 missing)\r## VALUE \u003c 76762 to the right, improve=10.079490, (2 missing)\r## YOJ \u003c 8 to the left, improve= 6.400122, (8 missing)\r## CLAGE \u003c 77.9912 to the right, improve= 6.058741, (12 missing)\r## NINQ \u003c 0.5 to the left, improve= 5.928205, (12 missing)\r## Surrogate splits:\r## VALUE \u003c 57109 to the right, agree=0.955, adj=0.870, (11 split)\r## CLNO \u003c 12.5 to the right, agree=0.727, adj=0.217, (0 split)\r## LOAN \u003c 2700 to the right, agree=0.712, adj=0.174, (0 split)\r## JOB splits as LLLRLLR, agree=0.712, adj=0.174, (0 split)\r## CLAGE \u003c 77.9912 to the right, agree=0.682, adj=0.087, (0 split)\r## ## Node number 18: 29 observations, complexity param=0.01217862\r## predicted class=0 expected loss=0.3103448 P(node) =0.00810962\r## class counts: 20 9\r## probabilities: 0.690 0.310 ## left son=36 (20 obs) right son=37 (9 obs)\r## Primary splits:\r## YOJ \u003c 1.5 to the left, improve=8.555556, (11 missing)\r## VALUE \u003c 48922 to the right, improve=5.517689, (0 missing)\r## CLNO \u003c 0.5 to the left, improve=5.517689, (0 missing)\r## REASON splits as LLR, improve=5.198107, (0 missing)\r## LOAN \u003c 11600 to the right, improve=3.767328, (0 missing)\r## Surrogate splits:\r## VALUE \u003c 44400 to the right, agree=0.833, adj=0.571, (11 split)\r## REASON splits as LRR, agree=0.778, adj=0.429, (0 split)\r## NINQ \u003c 0.5 to the left, agree=0.778, adj=0.429, (0 split)\r## CLNO \u003c 0.5 to the left, agree=0.778, adj=0.429, (0 split)\r## LOAN \u003c 14700 to the left, agree=0.722, adj=0.286, (0 split)\r## ## Node number 19: 18 observations\r## predicted class=1 expected loss=0 P(node) =0.005033557\r## class counts: 0 18\r## probabilities: 0.000 1.000 ## ## Node number 24: 625 observations, complexity param=0.02232747\r## predicted class=0 expected loss=0.3616 P(node) =0.1747763\r## class counts: 399 226\r## probabilities: 0.638 0.362 ## left son=48 (574 obs) right son=49 (51 obs)\r## Primary splits:\r## LOAN \u003c 6050 to the right, improve=13.16430, (0 missing)\r## CLAGE \u003c 82.75537 to the right, improve=13.07259, (1 missing)\r## JOB splits as LRLRLLR, improve=12.30388, (0 missing)\r## VALUE \u003c 60068 to the right, improve=11.87397, (30 missing)\r## DELINQ \u003c 1.5 to the left, improve=11.60892, (4 missing)\r## ## Node number 25: 12 observations\r## predicted class=1 expected loss=0 P(node) =0.003355705\r## class counts: 0 12\r## probabilities: 0.000 1.000 ## ## Node number 34: 48 observations\r## predicted class=0 expected loss=0.2708333 P(node) =0.01342282\r## class counts: 35 13\r## probabilities: 0.729 0.271 ## ## Node number 35: 29 observations\r## predicted class=1 expected loss=0.2413793 P(node) =0.00810962\r## class counts: 7 22\r## probabilities: 0.241 0.759 ## ## Node number 36: 20 observations\r## predicted class=0 expected loss=0 P(node) =0.005592841\r## class counts: 20 0\r## probabilities: 1.000 0.000 ## ## Node number 37: 9 observations\r## predicted class=1 expected loss=0 P(node) =0.002516779\r## class counts: 0 9\r## probabilities: 0.000 1.000 ## ## Node number 48: 574 observations, complexity param=0.0135318\r## predicted class=0 expected loss=0.3310105 P(node) =0.1605145\r## class counts: 384 190\r## probabilities: 0.669 0.331 ## left son=96 (333 obs) right son=97 (241 obs)\r## Primary splits:\r## DELINQ \u003c 1.5 to the left, improve=11.324740, (4 missing)\r## DEROG \u003c 0.5 to the left, improve= 9.756626, (72 missing)\r## CLAGE \u003c 82.75537 to the right, improve= 9.440944, (1 missing)\r## LOAN \u003c 35850 to the left, improve= 9.396460, (0 missing)\r## JOB splits as LRLRLLR, improve= 9.121343, (0 missing)\r## Surrogate splits:\r## CLNO \u003c 34.5 to the left, agree=0.600, adj=0.038, (4 split)\r## REASON splits as RLL, agree=0.596, adj=0.030, (0 split)\r## JOB splits as LRLLLRL, agree=0.596, adj=0.030, (0 split)\r## LOAN \u003c 6750 to the right, agree=0.589, adj=0.013, (0 split)\r## CLAGE \u003c 55.202 to the right, agree=0.588, adj=0.008, (0 split)\r## ## Node number 49: 51 observations, complexity param=0.01217862\r## predicted class=1 expected loss=0.2941176 P(node) =0.01426174\r## class counts: 15 36\r## probabilities: 0.294 0.706 ## left son=98 (17 obs) right son=99 (34 obs)\r## Primary splits:\r## CLNO \u003c 23.5 to the right, improve=11.294120, (0 missing)\r## MORTDUE \u003c 42850 to the right, improve= 8.000000, (6 missing)\r## VALUE \u003c 54842.5 to the right, improve= 6.696429, (3 missing)\r## CLAGE \u003c 157.0398 to the right, improve= 6.352941, (0 missing)\r## YOJ \u003c 4.5 to the right, improve= 4.235294, (1 missing)\r## Surrogate splits:\r## CLAGE \u003c 151.0333 to the right, agree=0.824, adj=0.471, (0 split)\r## YOJ \u003c 6.5 to the right, agree=0.745, adj=0.235, (0 split)\r## DELINQ \u003c 2.5 to the right, agree=0.725, adj=0.176, (0 split)\r## VALUE \u003c 59018.5 to the right, agree=0.706, adj=0.118, (0 split)\r## MORTDUE \u003c 52017 to the right, agree=0.686, adj=0.059, (0 split)\r## ## Node number 96: 333 observations\r## predicted class=0 expected loss=0.2492492 P(node) =0.09312081\r## class counts: 250 83\r## probabilities: 0.751 0.249 ## ## Node number 97: 241 observations, complexity param=0.0135318\r## predicted class=0 expected loss=0.4439834 P(node) =0.06739374\r## class counts: 134 107\r## probabilities: 0.556 0.444 ## left son=194 (203 obs) right son=195 (38 obs)\r## Primary splits:\r## VALUE \u003c 126206.5 to the left, improve=8.916962, (17 missing)\r## LOAN \u003c 35550 to the left, improve=8.496324, (0 missing)\r## CLAGE \u003c 110.3866 to the right, improve=7.138055, (0 missing)\r## MORTDUE \u003c 67970.5 to the left, improve=4.980707, (11 missing)\r## JOB splits as -LLLLLR, improve=4.457637, (0 missing)\r## Surrogate splits:\r## MORTDUE \u003c 104486.5 to the left, agree=0.915, adj=0.441, (12 split)\r## CLNO \u003c 54 to the left, agree=0.875, adj=0.176, (5 split)\r## JOB splits as -LLLLLR, agree=0.871, adj=0.147, (0 split)\r## REASON splits as RLL, agree=0.866, adj=0.118, (0 split)\r## CLAGE \u003c 445.9928 to the left, agree=0.866, adj=0.118, (0 split)\r## ## Node number 98: 17 observations\r## predicted class=0 expected loss=0.2352941 P(node) =0.004753915\r## class counts: 13 4\r## probabilities: 0.765 0.235 ## ## Node number 99: 34 observations\r## predicted class=1 expected loss=0.05882353 P(node) =0.00950783\r## class counts: 2 32\r## probabilities: 0.059 0.941 ## ## Node number 194: 203 observations, complexity param=0.0135318\r## predicted class=0 expected loss=0.3842365 P(node) =0.05676734\r## class counts: 125 78\r## probabilities: 0.616 0.384 ## left son=388 (162 obs) right son=389 (41 obs)\r## Primary splits:\r## MORTDUE \u003c 48036.5 to the right, improve=6.814823, (11 missing)\r## CLAGE \u003c 110.3866 to the right, improve=5.615953, (0 missing)\r## VALUE \u003c 58870 to the right, improve=4.238080, (13 missing)\r## DELINQ \u003c 2.5 to the left, improve=4.179322, (4 missing)\r## JOB splits as -LLRLLR, improve=3.869381, (0 missing)\r## Surrogate splits:\r## VALUE \u003c 61429 to the right, agree=0.818, adj=0.054, (6 split)\r## CLNO \u003c 8.5 to the right, agree=0.818, adj=0.054, (5 split)\r## LOAN \u003c 36050 to the left, agree=0.812, adj=0.027, (0 split)\r## CLAGE \u003c 313.9898 to the left, agree=0.812, adj=0.027, (0 split)\r## ## Node number 195: 38 observations\r## predicted class=1 expected loss=0.2368421 P(node) =0.0106264\r## class counts: 9 29\r## probabilities: 0.237 0.763 ## ## Node number 388: 162 observations\r## predicted class=0 expected loss=0.3148148 P(node) =0.04530201\r## class counts: 111 51\r## probabilities: 0.685 0.315 ## ## Node number 389: 41 observations\r## predicted class=1 expected loss=0.3414634 P(node) =0.01146532\r## class counts: 14 27\r## probabilities: 0.341 0.659\rrpart는 자동적으로 교호작용이 포함되므로 포함해서 작성하면 안됨\nmethod = ‘class’ : 분류나무\nmethod = ‘anova’ : 회귀나무 (Y가 연속형인 경우)\n# maximal tree\rset.seed(1234)\rmy.control \u003c-rpart.control(xval=10, cp=0.001, minsplit=35)\rtree1 \u003c-rpart(BAD ~., data = train, method=\"class\", control=my.control)\rplot(tree1, uniform=T, compress=T, margin=0.05)\rxval : cross-validation으로 몇번 교차검정할지를 의미\nprintcp(tree1)\r## ## Classification tree:\r## rpart(formula = BAD ~ ., data = train, method = \"class\", control = my.control)\r## ## Variables actually used in tree construction:\r## [1] CLAGE CLNO DEBTINC DELINQ DEROG JOB LOAN MORTDUE NINQ ## [10] REASON VALUE YOJ ## ## Root node error: 739/3576 = 0.20666\r## ## n= 3576 ## ## CP nsplit rel error xerror xstd\r## 1 0.0360848 0 1.00000 1.00000 0.032765\r## 2 0.0223275 4 0.83897 0.86739 0.031038\r## 3 0.0135318 6 0.79432 0.85521 0.030866\r## 4 0.0121786 9 0.74966 0.85656 0.030885\r## 5 0.0101488 12 0.71313 0.84303 0.030693\r## 6 0.0060893 14 0.69283 0.77402 0.029662\r## 7 0.0054127 16 0.68065 0.76184 0.029472\r## 8 0.0040595 18 0.66982 0.74290 0.029171\r## 9 0.0036085 22 0.65223 0.73613 0.029062\r## 10 0.0033829 29 0.62111 0.74290 0.029171\r## 11 0.0030447 39 0.57104 0.74425 0.029193\r## 12 0.0020298 44 0.55480 0.74966 0.029279\r## 13 0.0013532 46 0.55074 0.75372 0.029344\r## 14 0.0010000 47 0.54939 0.76184 0.029472\rcp : 복잡도 cost-complexity\nnsplit : number of slplit\nrel error : (train data의) 상대오차\nxerror : cross-validation의 오차\nplotcp(tree1)\r최종 노드의 수 = 분할의 수 + 1\n1 SE rule : xerror +- 1 * Xstd\n오차는 동일(위 같은 구간안에 있는 것들)하면서 간결한 모형 선택\n점선 아래의 오차들은 통계적으로 차이 없는 것들\n# pruning\rtree1.prun \u003c-prune(tree1, cp = 0.006)\rprint(tree1.prun)\r## n= 3576 ## ## node), split, n, loss, yval, (yprob)\r## * denotes terminal node\r## ## 1) root 3576 739 0 (0.79334452 0.20665548) ## 2) DELINQ\u003c 0.5 2813 393 0 (0.86029150 0.13970850) ## 4) DEBTINC\u003c 44.67086 2776 360 0 (0.87031700 0.12968300) ## 8) CLNO\u003e=2.5 2729 333 0 (0.87797728 0.12202272) ## 16) LOAN\u003e=5050 2652 298 0 (0.88763198 0.11236802) ## 32) DEROG\u003c 1.5 2575 265 0 (0.89708738 0.10291262) *\r## 33) DEROG\u003e=1.5 77 33 0 (0.57142857 0.42857143) ## 66) JOB=Other,Sales 44 12 0 (0.72727273 0.27272727) *\r## 67) JOB=,Mgr,Office,ProfExe,Self 33 12 1 (0.36363636 0.63636364) *\r## 17) LOAN\u003c 5050 77 35 0 (0.54545455 0.45454545) ## 34) MORTDUE\u003e=45250 48 13 0 (0.72916667 0.27083333) *\r## 35) MORTDUE\u003c 45250 29 7 1 (0.24137931 0.75862069) *\r## 9) CLNO\u003c 2.5 47 20 1 (0.42553191 0.57446809) ## 18) VALUE\u003c 75083.5 29 9 0 (0.68965517 0.31034483) *\r## 19) VALUE\u003e=75083.5 18 0 1 (0.00000000 1.00000000) *\r## 5) DEBTINC\u003e=44.67086 37 4 1 (0.10810811 0.89189189) *\r## 3) DELINQ\u003e=0.5 763 346 0 (0.54652687 0.45347313) ## 6) DELINQ\u003c 4.5 700 289 0 (0.58714286 0.41285714) ## 12) NINQ\u003c 3.5 637 238 0 (0.62637363 0.37362637) ## 24) DEBTINC\u003c 43.64351 625 226 0 (0.63840000 0.36160000) ## 48) LOAN\u003e=6050 574 190 0 (0.66898955 0.33101045) ## 96) DELINQ\u003c 1.5 333 83 0 (0.75075075 0.24924925) *\r## 97) DELINQ\u003e=1.5 241 107 0 (0.55601660 0.44398340) ## 194) VALUE\u003c 126206.5 203 78 0 (0.61576355 0.38423645) ## 388) MORTDUE\u003e=48036.5 162 51 0 (0.68518519 0.31481481) *\r## 389) MORTDUE\u003c 48036.5 41 14 1 (0.34146341 0.65853659) *\r## 195) VALUE\u003e=126206.5 38 9 1 (0.23684211 0.76315789) *\r## 49) LOAN\u003c 6050 51 15 1 (0.29411765 0.70588235) ## 98) CLNO\u003e=23.5 17 4 0 (0.76470588 0.23529412) *\r## 99) CLNO\u003c 23.5 34 2 1 (0.05882353 0.94117647) *\r## 25) DEBTINC\u003e=43.64351 12 0 1 (0.00000000 1.00000000) *\r## 13) NINQ\u003e=3.5 63 12 1 (0.19047619 0.80952381) *\r## 7) DELINQ\u003e=4.5 63 6 1 (0.09523810 0.90476190) *\rprp(tree1.prun, type=4, extra=2, digits=3)\r# comparison of trees\rprob0 \u003c-predict(tree0, newdata=test, type=\"prob\") \rprob1 \u003c-predict(tree1.prun, newdata=test, type=\"prob\") \rtype=“prob” : 분류\ntype=“vector” : 회귀\nlibrary(pROC)\rroccurve0 \u003c-roc(test$BAD ~prob0[,2])\r## Setting levels: control = 0, case = 1\r## Setting direction: controls \u003c cases\rroccurve1 \u003c-roc(test$BAD ~prob1[,2])\r## Setting levels: control = 0, case = 1\r## Setting direction: controls \u003c cases\rplot(roccurve1, col=\"red\", print.auc=TRUE, print.auc.adj=c(2.5,-8), auc.polygon=TRUE)\rplot(roccurve0, col=\"green\", add=TRUE, print.auc=TRUE, print.auc.adj=c(-1,-5))\r아래는 회귀나무\n## Regression tree\rlibrary(rpart)\rlibrary(rpart.plot)\r\r# default tree\rtree_default =rpart(Price ~Odometer +Color, data=usedcar2, method=\"anova\")\rprp(tree_default, type=4, extra=1, digits=3)\r# Pruning\rset.seed(1234)\rmy.control =rpart.control(xval=10, cp=0.001, minsplit = 5)\rtree_max =rpart(Price ~Odometer +Color, data=usedcar2, method=\"anova\", control=my.control)\rprintcp(tree_max)\r## ## Regression tree:\r## rpart(formula = Price ~ Odometer + Color, data = usedcar2, method = \"anova\", ## control = my.control)\r## ## Variables actually used in tree construction:\r## [1] Color Odometer\r## ## Root node error: 25739561/100 = 257396\r## ## n= 100 ## ## CP nsplit rel error xerror xstd\r## 1 0.5562810 0 1.000000 1.02029 0.110236\r## 2 0.1019455 1 0.443719 0.45653 0.069843\r## 3 0.0333941 2 0.341773 0.41029 0.059533\r## 4 0.0248981 3 0.308379 0.44397 0.057177\r## 5 0.0208226 4 0.283481 0.42798 0.053579\r## 6 0.0131915 6 0.241836 0.43228 0.055434\r## 7 0.0121626 7 0.228645 0.43330 0.053882\r## 8 0.0105416 8 0.216482 0.43131 0.054461\r## 9 0.0094785 9 0.205940 0.43300 0.055745\r## 10 0.0089991 10 0.196462 0.44330 0.058084\r## 11 0.0089429 12 0.178464 0.43867 0.058210\r## 12 0.0084675 14 0.160578 0.43745 0.058258\r## 13 0.0064467 18 0.125873 0.43800 0.062779\r## 14 0.0063479 20 0.112980 0.44632 0.064453\r## 15 0.0037235 21 0.106632 0.44614 0.063252\r## 16 0.0026928 22 0.102908 0.44974 0.063959\r## 17 0.0022088 23 0.100215 0.43844 0.063390\r## 18 0.0020300 24 0.098007 0.43734 0.063495\r## 19 0.0015420 25 0.095977 0.43452 0.062900\r## 20 0.0010353 26 0.094435 0.42770 0.061689\r## 21 0.0010000 28 0.092364 0.42499 0.060838\rplotcp(tree_max)\rtree_prun =prune(tree_max, cp=0.04)\rprp(tree_prun, type=4, extra=1, digits=3)\r\r\rEnsemble Methods\r- 로지스틱 : bias가 크지만 variance는 작음\r- 하나의 관측값이 변해도 큰 차이 없음\r- likelihood 함수에 의존하여 다양성 확보 어려움\r- 의사결정 나무, 머신러닝 : bias는 작으나 variance가 큼\r- 하나의 관측값에도 크게 변화 : instability 즉 hyperplane의 변동성이 크다\r- 다양성 확보 가능\r앙상블 : instability가 큰 모델을 여러번 학습하여 bias와 variance를 줄여 오류의 감소 추구\n- 분산 감소에 의한 오류 감소 : Bagging, Random Forest\r- 편향 감소에 의한 오류 감소 : Boosting(분산도 줄이지만 편향을 더 많이 줄임)\r- 분산과 편향 모두 감소에 의한 오류 감소 : Mixture of Exports\r다양성divesity를 어떻게 확보할 것인지가 중요, 그리고 이를 어떻게 결합aggregate할 것인가\n-\u003e 각각의 모델은 성능도 좋으면서 서로 다양한 형태를 나타내는 것이 가장 이상적\n\r배깅 : 데이터 변형을 통해 tree 생성\n\r부스팅 : 가중치 변형을 통해 tree 생성\n\r랜덤포레스트 : 배깅의 boostrap + 변수 임의 추출\n\r\r\rBagging : Boostrapping AGGregatING\rclassification on unweighted majority voting\n단점\n\r해석력을 잃어버림\r\r장점\n\r예측력을 높임\r\r과정\r전체 데이터 집합에서 각 학습 데이터 셋을 boostrapping 즉 복원추출하여 원래 데이터 수만큼 크기를 갖도록 생성\n\rBootstrap sample training set 복원추출된 데이터셋 :\r\r\r\nT(1), ... , T(B)\nboostrap을 바탕으로 의사 결정 나무 시행, 이 과정을 반복\n\rClassifiers for each sample :\r\r\r\nC1(x, T(1)), ... , CB(x, T(B))\n- 각 의사결정나무는 서로 다른 학습 데이터셋을 사용하게 됨\r최종 예측은 각 의사나무의 예측 결과를 다수결의 방법을 통해 취합 -\u003e 분산 감소\n\rnumber of times that classified as j :\r\r\r\n$$N_j=\\sum ^B_{b=1}I[ C_b(x,T^{(b)})=y_j]$$\n- Bagging : \r\nC(x) = argmaxjNj\n\r코드\rlibrary(rpart)\rlibrary(adabag)\r## Loading required package: caret\r## Loading required package: ggplot2\r## Loading required package: foreach\r## Loading required package: doParallel\r## Loading required package: iterators\r## Loading required package: parallel\rset.seed(1234)\ri =sample(1:nrow(german), round(nrow(german)*0.7)) #70% for training data, 30% for testdata\rgerman.train =german[i,] \rgerman.test =german[-i,]\r\r\rmy.control \u003c-rpart.control(xval=0, cp=0, minsplit=5, maxdepth=10)\rbag.train.german \u003c-bagging(y ~., data = german.train, mfinal=50, control=my.control)\rbagging을 할 때는 pruning 하지 않고 tree를 크게 만듦 : 분산이 커져 변동성이 증가하나 bias는 작아짐\nxval=0 : cross validation이 0이란 의므로 pruning 하지 않음\nmfinal : tree의 개수로 요즘에는 100~200이 기본\npred.bag.german \u003c-predict.bagging(bag.train.german, newdata=german.test)\rprint(bag.train.german$importance)\r## age check credit debtors duration employment ## 6.98289316 19.74036811 12.53993235 1.73453022 9.14184507 6.64297839 ## foreign history housing installment job numcredits ## 0.09733153 7.43690856 1.29543971 1.79045071 2.29440866 0.69766383 ## others personal property purpose residence residpeople ## 1.84126119 2.48535083 5.06881899 10.61329877 3.17103304 0.63697943 ## savings telephone ## 5.10899380 0.67951365\r변수 중요도로 숫자의 절대적 의미는 없고 상대적 의미만 있음\n비선형 모델의 변수중요도 : 각각의 tree마다의 변수중요도를 통합한 것 -\u003e 이 결과를 NN에도 사용 가능함\n1-sum( diag(pred.bag.german$confusion) ) /sum(pred.bag.german$confusion)\r## [1] 0.27\rhead(pred.bag.german$prob)\r## [,1] [,2]\r## [1,] 0.44 0.56\r## [2,] 0.54 0.46\r## [3,] 0.00 1.00\r## [4,] 0.34 0.66\r## [5,] 0.46 0.54\r## [6,] 0.06 0.94\rlibrary(pROC)\rroccurve \u003c-roc(german.test$y ~pred.bag.german$prob[,1])\r## Setting levels: control = bad, case = good\r## Setting direction: controls \u003e cases\rplot(roccurve)\rauc(roccurve)\r## Area under the curve: 0.7418\r\r\rBoosting\rclassification on unequal weighted training data\n오분류 관측치의 가중치를 높이며 분류를 반복, 각각의 분류를 합해 최종 분류를 계산하는 방법\n- 분류경계선 근처의 가중치가 커짐\r- 결국 variance와 bias 감소 (bias가 더 크게 감소)\r부스팅의 경우 tree를 작게 만듦\ntrain set과 test set을 7대3으로 분할하면 그 속성이 남아 있게되므로, 이러한 분할도 여러번 해보는 것이 좋음\nAdaBoost\r먼저 초기 가중치를 줌\r\r\rInitialized weight : wi = 1/N\r\r가중치를 사용하여 분류하며 가중치를 변경하는 과정 i=1를 i=M까지 반복\r\r\r가중치 wi를 이용하여 classifier Cm(x) 생성\n\r오차인 $err_m={\\sum^N_{i=1}w_iI(y_i\\not= C_m(X)i)) \\over \\sum^N_{i=1}w_i}$ 를 계산 : 오분류시 $I(y_i\\not= C_m(X_i))=1$, 정분류시 $I(y_i\\not= C_m(X_i))=0$\n\rtree의 가중치인 αm = log((1 − errm)/errm) 를 계산 : error가 적으면 α상승\n\r새로운 가중치 생성 $w_i=w_i\\ \\text{exp}[\\alpha_mI(y_i\\not= C_m(X_i))]$\n\r새로운 가중치 전체의 합이 1이 되도록 조정 : ∑wi = 1\n\r\r반복한 분류를 가중치를 반영하여 최종 분류를 만듦 $C_{AD}(X)=sign[\\sum^M_{m=1}\\alpha_mC_M(X)]$\r\r\r즉 bagging은 tree당 같은 가중치이지만, Adaboost는 tree별로 다른 가중치αm를 반영\r\r코드\r##### Boosting\rlibrary(rpart)\rlibrary(adabag)\rset.seed(1234)\rmy.control \u003c-rpart.control(xval=0, cp=0, maxdepth=1)\rboo.german \u003c-boosting(y ~., data = german, boos=T, mfinal=100, control=my.control)\r\rsummary(boo.german)\r\rboo.german$trees\r\rprint(boo.german$importance)\rimportanceplot(boo.german)\r\rpred.boo.german \u003c-predict.boosting(boo.german, newdata=german) # 원래는 데이터 분할 필요\rhead(pred.boo.german$prob,10)\r\rprint(pred.boo.german$confusion)\r1-sum(diag(pred.boo.german$confusion))/sum(pred.boo.german$confusion)\revol.german=errorevol(boo.german, newdata=german)\rplot.errorevol(evol.german)\r\r\rroccurve \u003c-roc(german$y ~pred.boo.german$prob[,1])\rplot(roccurve)\rauc(roccurve)\rxval=0 : 푸루닝 하지 않음\nmaxdepth=1 : 분할 한번뿐 -\u003e 일반적으로 boosting은 작을 수록 좋음 (보통 1~4\nboos=T : 가중치 반영시 bootstrap에서 가중치 높은 것을 여러번 더 뽑을 수 있게 하면 AdaBoost 방법을 이용해 boosting 할 수 있다.\n\r\rGradient Boost\rY = h1(X) + err1에서 오류를 다시 err1 = h2(X) + err2와 같이 분류하는 방법 : Fm(X) = Fm − 1(X) + wmhm(X)\n- 즉 error를 바탕으로 boosting을 계속하는 방법\rGredient Descent 알고리즘으로 최적 weight 계산\n각 함수별 최적 weight 찾으면 예측 정확도는 더 높아짐\n\r\rRandom Forest\r변수의 임의 추출을 통해 다양성 확보\n\rbootstrapping과 predictor subset selection을 동시에 적용하여 개별 tree의 다양성을 극대화\n\rtree들이 서로 독립이 되도록 하고자 함\n\r각 노드를 분할할 때, p개의 변수 중에서 탐색하지 않고, m개 (m\u003cp)의 변수 중에서 탐색하여 분할함\n\rm=p이면 배깅과 동일\r\rm의 값이 작으면 각 나무모형들 간의 상관관계가 감소함 -\u003e m이 너무 적으면 정확도가 낮아짐\n\r일반적인 앙상블의 크기 m : 분류 data인 경우 $m=\\sqrt{p}$, 회귀 data인 경우 $p\\over 3$\n\r\r과정\r\rboostrap 적용 : bagging과 동일\n\r변수의 부분집합 선택을 통한 다양성 확보 : X변수중 random하게 일부만 greedy search\n\r즉 decision tree에서 분할시 분할에 greedy search하게 되는 x변수가 매번 random하게 정해짐\n\r여러번 해도 모든 변수를 greedy search하면 중요한 변수는 뿌리 노드 근처에 자주 나옴\n\r\r\r\r개별 분류 모델의 결과 aggregating 방법\rmajority voting : $\\hat{Y}_{Ensemble}=\\text{argmax}_i(\\sum_{j=1}^n\\delta({Y}_j=i),\\ i\\in\\{0,1\\})$\nweighted (각 모델의 예측정확도 TrnAccj 사용) voting : $\\hat{Y}_{Ensemble}= \\text{argmax}_i({\\sum_{j=1}^n(TrnAcc_j)\\delta(Y_j=i)\\over\\sum_{j=1}^n(TrnAcc_j)},\\ i\\in\\{0,1\\})$\nweighted (각 class별 에측 확률) voting : $\\hat{Y}_{Ensemble}= \\text{argmax}_i({{1\\over n}\\sum_{j=1}^nP(Y_j=i)},\\ i\\in\\{0,1\\})$\n\rOOB Error : Out Of Bag error\r개별 학습 데이터셋 구성시 bootstrap 되지 않은 개체들을 검증용으로 사용\n이 값을 test 데이터로 삼으면, test error가 계산됨\n\r변수 중요도\r회귀분석과는 다르게 개별 변수가 통계적으로 얼마나 유의한지에 대한 정보(p-value)가 없음어 간접적인 방식으로 추정\n- 절대적인 개념이 아니라 상대적인 개념\r- 다른 모형에서도 OOB를 활용한 변수중요도개념 자주 활용\r원래 OOB 데이터 집합에 대해서 OOB Error ej 구함\n\rj 는 1부터 m개 까지의 각각의 tree\r\r특정 변수의 값을 임의로 뒤섞은 random permutation OOB 데이터 집합에 대해서 permutaion OOB Error pj를 구함\n\r다른 X변수와 Y변수를 제외하고, 오직 특정 X변수 하나만의 순서를 임의로 바꿈\n\r해당 특정 X변수가 noise 변수가 됨\n\r\rOOB Error 차이di = ei − pi의 평균과 분산을 계산\n\rM개의 모든 tree를 통해 계산한 OBB Error의 차이의 편균과 표준편차로 변수중요도 계산\n\ri번째 변수의 변수 중요도 :\r\r\r\n$$v_i={\\bar{d}\\over s_d}$$\n$\\bar{d}=\\sum_{j=1}^md_j/m$ 및 $s_d^2=\\sum_{j}^{m-1}(d_j-\\bar{d})^2/(m-1)$\n- 오류율의 차이가 클수록 해당변수가 tree에서 중요한 역할\r\r코드\rlibrary(randomForest)\rset.seed(1234)\ri =sample(1:nrow(german), round(nrow(german)*0.7)) #70% for training data, 30% for testdata\rgerman.train =german[i,] \rgerman.test =german[-i,]\r\rrf.train.german \u003c-randomForest(y ~., data = german.train, ntree=100, mtry=5, \rimportance=T, na.action=na.omit)\rpred.rf.german \u003c-predict(rf.train.german, newdata=german.test)\r\rtab=table(german.test$y,pred.rf.german, dnn=c(\"Actual\",\"Predicted\"))\rprint(tab)\r1-sum(diag(tab))/sum(tab)\rprob.rf.german \u003c-predict(rf.train.german, newdata=german.test, type=\"prob\")\rhead(prob.rf.german)\r\r코드 : 회귀분석의 경우\r### Random Forest ###\r## Regression\rnobs=nrow(usedcar2)\r\r# indicator variables\rusedcar2$Ind1\u003c-as.numeric(usedcar2$Color =='white')\rusedcar2$Ind2\u003c-as.numeric(usedcar2$Color =='silver')\r\r# training and test data\rset.seed(1234)\ri =sample(1:nobs, round(nobs*0.7)) #70% for training data, 30% for testdata\rtrain =usedcar2[i,] \rtest =usedcar2[-i,]\r\r# several models\rtmpmodel =lm(Price ~Odometer+Ind1+Ind2+Ind1:Odometer+Ind2:Odometer, data=train)\rmodel1 =step(tmpmodel, direction = 'both')\r## Start: AIC=796.19\r## Price ~ Odometer + Ind1 + Ind2 + Ind1:Odometer + Ind2:Odometer\r## ## Df Sum of Sq RSS AIC\r## - Odometer:Ind2 1 19462 5152676 794.46\r## \u003cnone\u003e 5133214 796.19\r## - Odometer:Ind1 1 394444 5527658 799.37\r## ## Step: AIC=794.46\r## Price ~ Odometer + Ind1 + Ind2 + Odometer:Ind1\r## ## Df Sum of Sq RSS AIC\r## \u003cnone\u003e 5152676 794.46\r## + Odometer:Ind2 1 19462 5133214 796.19\r## - Odometer:Ind1 1 389490 5542166 797.56\r## - Ind2 1 1083262 6235938 805.81\rsummary(model1)\r## ## Call:\r## lm(formula = Price ~ Odometer + Ind1 + Ind2 + Odometer:Ind1, ## data = train)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -680.08 -161.16 2.81 167.01 775.62 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 1.699e+04 2.639e+02 64.405 \u003c 2e-16 ***\r## Odometer -6.410e-02 6.817e-03 -9.403 9.72e-14 ***\r## Ind1 -9.388e+02 4.680e+02 -2.006 0.049018 * ## Ind2 3.321e+02 8.983e+01 3.697 0.000451 ***\r## Odometer:Ind1 2.689e-02 1.213e-02 2.217 0.030155 * ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 281.6 on 65 degrees of freedom\r## Multiple R-squared: 0.7268, Adjusted R-squared: 0.71 ## F-statistic: 43.24 on 4 and 65 DF, p-value: \u003c 2.2e-16\rlibrary(randomForest)\r## randomForest 4.6-14\r## Type rfNews() to see new features/changes/bug fixes.\r## ## Attaching package: 'randomForest'\r## The following object is masked from 'package:ggplot2':\r## ## margin\rmodel2 \u003c-randomForest(Price ~Odometer +Color, data = train, ntree=100, mtry=2,\rimportance=T, na.action=na.omit)\r\r# predicted values\rpred1 =predict(model1, newdata=test, type='response')\rpred2 =predict(model2, newdata=test, type='response')\r\r# predictive R^2\rcor(test$Price, pred1)^2\r## [1] 0.674462\rcor(test$Price, pred2)^2\r## [1] 0.6775222\r# MAE\rmean(abs(test$Price -pred1))\r## [1] 235.4435\rmean(abs(test$Price -pred2))\r## [1] 209.5104\r# MAPE\rmean(abs(test$Price -pred1)/abs(test$Price))*100\r## [1] 1.57497\rmean(abs(test$Price -pred2)/abs(test$Price))*100\r## [1] 1.408753\r# variable importance of Random Forest\rmodel2$importance\r## %IncMSE IncNodePurity\r## Odometer 318591.9 16475559\r## Color 22017.8 1372346\r\r\rClustering : Unsupervised Learnging\rX 변수 대부분 연속형이어야 함 : distance를 사용하므로 범주형의 경우 계산 불가 (지시변수도 가능하나 좋지 못한 방법)\n군집분석은 자료 사이의 거리를 이용하기에, 변수별 단위가 결과에 큰 영향 -\u003e 반드시 표준화 standardization 필요\n표준화 standardization\r관측치 표준화\n\r관측치가 사람의 경우 관측치 표준화 불필요\n\r파생변수로 관찰치 표준화가능\n\r파생변수 생성시 비율의 경우 관려적으로 log 취함\r\r\r변수 표준화\n\r각 변수의 관찰값으로부터 그 변수의 평균을 빼고, 그 변수의 표준편차로 나누는 것\n\r표준화된 자료는 모든 변수가 평균이 0이고 표준편차가 1\n\r\r\r\r군집화의 기준 : distance\r\r동일한 군집에 속하면 여러 속성이 비슷하고, 다른군집에 속한 관측치는 그렇지 않음\n\r유사성보다는 비유사성dissimilarity를 기준으로 하여 distance 사용\n\r\r\rdistance measures\r유클리드 Euclidean 거리 : $d(x,y)=(\\sum_{i=1}^p(x_i-y_i)^2)^{1/2}$ when p=2 (x, y)\n\rpairwise 거리\r\rMinkowski 거리 : $d(x,y)=(\\sum_{i=1}^p(x_i-y_i)^m)^{1/m}$\n\r일반화된 형태로, m = 2 이면 Euclidean\r\rManhalanobis 거리 : $d(x,y)=\\sqrt{(x-y)^TS^{-1}(x-y)}$\n\r공분산행렬 S : correlation 반영 - 다른 거리와 다르게 ind 가정 없어서 이론적으로는 가장 우월하나 실제 s 계산이 어려움\r\rManhattan Distance : $d_{Manhattan}(x,y)=\\sum^p_{i=1}|x-y|$\n\r\r\r\rHierarchical clustering 계층적 군집분석\r장점\n- 군집의 수를 알 필요가 없음 -\u003e 사후에 판단 가능\r- 해석에 용이\r- 덴드로그램을 통해 군집화 프로세스와 결과물을 표현가능\r단점\n- 계산속도가 느림 - nC2 즉 n^2에 비례하여 계산량 증가\r- 이상치에 대한 사전검토 필요\r- 이상치가 존재할 경우, 초기 단계에 잘못 분류된 군집은 분석이 끝날때까지 소속군집이 변하지 않음\r- Centroid 방법이 이상치에 덜 민감함\r과정\r1개의 entity를 가지는 관측치 그대로인 N개의 군집으로 시작\n\rNxN symmetric 거리 행렬 D = {dik}을 생성\n\r거리행렬 D의 원소중 가장 가까운 군집의 쌍 U와 V를 찾아 (UV)라는 하나의 군집으로 합침\n\r거리행렬 D중 새롭게 변화 되는 부분인 (UV) 와 다른 군집 W 사이의 거리 d(UV)W를 계산\n\rsingle linkage 단일연결법 :\r\r\r\nd(UV)W = min(dUW, dVW)\n- complete linkage 완전연결법 : \r\nd(UV)W = max(dUW, dVW)\n- average linkage 평균연결법 : \r\n$$d_{(UV)W}={\\sum^{n_{UV}}_{i=1}\\sum^{n_{W}}_{i=1}d_{ij}\\over n_{UV}n_W})$$\n- centroid method 중심점연결법 : \r\nd(UV)W = distance between the centroids of cluster UV and W\n- Ward's Method\r위의 과정을 N-1 반복하여 모든 관측치가 하나의 군집으로 바꿈\n\rDendro gram을 활용 : 어느 levels에서 어떻게 결합되어있는지 판단\n\r\r\rdendrogram 고드름 그림\r계층적 군집분석에만 dendrogram이 있음 : graphic to ilustrate the merges or divisions\n\r코드\r### Hierarchical Clustering\rzUSArrests=scale(USArrests) # scale() : 변수 표준화\r\rdist(zUSArrests) # dist() : 거리행렬\r## Alabama Alaska Arizona Arkansas California Colorado\r## Alaska 2.7037541 ## Arizona 2.2935197 2.7006429 ## Arkansas 1.2898102 2.8260386 2.7177583 ## California 3.2631104 3.0125415 1.3104842 3.7636409 ## Colorado 2.6510673 2.3265187 1.3650307 2.8310512 1.2876185 ## Connecticut 3.2152975 4.7399125 3.2628575 2.6076395 4.0663898 3.3279920\r## Delaware 2.0192927 3.6213633 1.9093696 1.8003239 3.0737852 2.5547456\r## Florida 2.2981353 2.9967642 1.7493928 3.3721968 2.0250039 2.4458600\r## Georgia 1.1314351 2.8194388 2.7871963 2.2117614 3.3780585 2.8649105\r## Hawaii 3.3885300 4.5301340 3.2621208 2.9723097 3.6589083 2.8233524\r## Idaho 2.9146623 4.0580555 3.5210071 1.7687255 4.4879436 3.4767685\r## Illinois 1.8734993 3.2670626 1.0825512 2.4626424 1.9117469 1.7898322\r## Indiana 2.0761411 3.3655952 2.6407486 1.4450503 3.4061273 2.3655622\r## Iowa 3.4878952 4.7251910 4.1157513 2.4252661 4.9708591 3.9406898\r## Kansas 2.2941096 3.6808173 2.7762838 1.5718411 3.6071725 2.6272281\r## Kentucky 1.8475879 3.5440903 3.3567681 1.0598104 4.2463809 3.2274013\r## Louisiana 0.7722224 2.9631431 2.2178519 2.0254276 3.0176625 2.6546743\r## Maine 3.4851115 4.8322605 4.2961903 2.3621893 5.2699843 4.2713441\r## Maryland 1.2896460 2.2777590 1.2117356 2.0582244 2.2312581 1.9667562\r## Massachusetts 2.9874810 4.3729925 2.5162281 2.6881270 3.2156499 2.6522793\r## Michigan 1.8814771 2.1154937 1.1940906 2.5895050 1.5146739 1.2363108\r## Minnesota 3.2314338 4.4266606 3.5388450 2.3300992 4.3123134 3.3283853\r## Mississippi 1.2831907 3.2554326 3.4551406 1.9318631 4.4200736 3.8491042\r## Missouri 1.6309686 2.5360573 1.5958731 1.6717500 2.2891751 1.3127406\r## Montana 2.3317271 3.6575988 3.3270869 1.2290066 4.2494176 3.1845338\r## Nebraska 2.6625170 3.9136902 3.1641791 1.7240495 4.0197242 3.0034613\r## Nevada 3.1024305 2.3443182 1.9260292 3.7086787 1.1968261 1.3988595\r## New Hampshire 3.5619825 4.8650686 4.2430411 2.4949861 5.1270892 4.1126287\r## New Jersey 2.6980230 4.1791832 2.1755787 2.7398478 2.7463023 2.3229870\r## New Mexico 1.5993970 2.0580889 1.0376848 2.3183196 1.8010201 1.5467439\r## New York 2.0723680 3.2903769 1.0725219 2.7478626 1.6787069 1.7363385\r## North Carolina 1.6043662 3.2403071 3.1478947 2.0717938 4.2802569 3.8649275\r## North Dakota 4.0614988 5.2110254 4.9319844 2.8756492 5.8660699 4.8014019\r## Ohio 2.2698519 3.5903348 2.3585705 1.9617104 3.0133425 2.1188236\r## Oklahoma 1.9570874 3.3416664 2.2648377 1.4224574 3.1488712 2.2263966\r## Oregon 2.3705678 2.6990696 2.0008664 1.8477626 2.6574019 1.5331980\r## Pennsylvania 2.5161340 4.1239537 2.9188907 1.9739986 3.7144562 2.8541709\r## Rhode Island 3.3951297 5.0629572 3.0570151 3.0883430 3.8883995 3.4810739\r## South Carolina 0.9157968 2.5640542 2.7992041 1.7074195 3.7546959 3.2131137\r## South Dakota 3.0835587 4.2467198 4.1020099 1.8724822 5.0529153 3.9667318\r## Tennessee 0.8407489 2.3362541 2.2989846 1.4254486 3.0119267 2.1972111\r## Texas 1.6463225 3.1527905 1.6448574 2.3505545 2.1698156 1.7947199\r## Utah 3.0906007 3.9480881 2.5244431 2.6049855 3.0701663 2.2461228\r## Vermont 3.9791527 4.8707876 5.1003665 2.7442984 6.0323504 4.8924735\r## Virginia 1.4859733 3.0492081 2.3106550 0.9971035 3.2159723 2.2622539\r## Washington 2.6481824 3.2715253 2.1399117 2.1313402 2.7746720 1.7897920\r## West Virginia 3.1243471 4.5004558 4.4974190 1.9951691 5.4883565 4.4210375\r## Wisconsin 3.5047330 4.8711543 3.9425867 2.6102451 4.7354960 3.7846917\r## Wyoming 1.8291027 3.4993456 2.6923028 0.9912639 3.7242766 2.8211492\r## Connecticut Delaware Florida Georgia Hawaii Idaho\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware 1.7568475 ## Florida 4.4700701 3.0614170 ## Georgia 3.9738227 2.9838715 2.1812958 ## Hawaii 1.3843291 2.4748807 4.3596338 3.8105218 ## Idaho 1.6354214 2.0382540 4.6999827 3.8005715 2.3658101 ## Illinois 2.7400560 1.5584719 1.7711863 2.3135778 2.7329756 3.2728945\r## Indiana 1.6147898 1.6973340 3.6150778 2.6924143 1.5460727 1.4923351\r## Iowa 1.5470089 2.6068606 5.2682765 4.2517889 2.1564575 0.8584962\r## Kansas 1.2280424 1.5510864 3.8424558 3.0071474 1.4648766 1.2103118\r## Kentucky 2.3346386 2.2514939 3.9474983 2.4408198 2.5203345 1.6565236\r## Louisiana 3.5329409 2.3266996 1.7529677 0.8592544 3.5687157 3.5283772\r## Maine 1.8792141 2.6560808 5.3946798 4.3334217 2.7160558 0.8486112\r## Maryland 3.4968269 1.9624834 1.4355204 1.8388691 3.6148670 3.4014584\r## Massachusetts 0.9468199 1.4382527 3.7753087 3.6706708 1.3276676 2.2201020\r## Michigan 3.7037870 2.5165292 1.3357020 1.9185489 3.4123472 3.7775301\r## Minnesota 0.9843793 2.1652930 4.7635252 3.9621842 1.4673850 1.0124936\r## Mississippi 4.1762631 3.0510628 3.0886673 1.5828594 4.4777223 3.6002946\r## Missouri 2.4383227 1.6723281 2.5182466 2.1021909 2.1832480 2.4697182\r## Montana 1.8584328 2.0306850 4.2696476 3.0967288 2.2488801 0.8286936\r## Nebraska 1.2116949 1.8113430 4.3082894 3.4295510 1.6628657 0.7515014\r## Nevada 4.5868149 3.5920897 1.9500388 2.9023041 4.0281974 4.7300228\r## New Hampshire 1.6169000 2.6744233 5.3778074 4.3427351 2.3112009 0.9249563\r## New Jersey 1.6108823 1.5808719 3.1900596 3.1989350 1.5050500 2.7425260\r## New Mexico 3.6233659 2.2271650 1.2965798 1.9015384 3.5506088 3.5883476\r## New York 3.0239174 1.8992106 1.5730970 2.3634498 2.9055803 3.5910319\r## North Carolina 4.1894604 2.7475286 2.9994188 2.3351307 4.7330517 3.5929592\r## North Dakota 2.5099838 3.3615239 6.0356613 4.8596758 3.1974906 1.4144557\r## Ohio 1.4443671 1.5838515 3.3897305 2.8043208 1.1494313 1.9647327\r## Oklahoma 1.4510623 1.1802929 3.3553471 2.7121515 1.6585736 1.5168111\r## Oregon 2.1756954 1.7742778 3.3399718 2.9998878 2.0031861 1.9757247\r## Pennsylvania 0.8721491 1.5894850 3.9389869 3.1817981 1.2119256 1.5171866\r## Rhode Island 1.0756115 1.6230495 4.2314871 4.1832075 2.0590981 2.4592705\r## South Carolina 4.0127954 2.7039667 2.5295912 1.3970074 4.2531214 3.4549959\r## South Dakota 2.2397424 2.6722813 5.1015141 3.8729745 2.8044891 0.8070290\r## Tennessee 3.2302375 2.3195070 2.3992285 1.0122252 3.0747375 2.9234395\r## Texas 2.8734475 2.0031365 1.8537984 1.7575559 2.5901696 3.3172180\r## Utah 1.2825907 1.8080931 3.9274528 3.7183994 1.0709720 2.0268663\r## Vermont 3.2066152 3.7144653 6.0766416 4.7091538 3.7208347 1.7797462\r## Virginia 1.9277004 1.4088230 3.1515587 2.2249559 2.0479238 1.6999289\r## Washington 1.6963486 1.6350170 3.5570666 3.3016469 1.5452901 1.8861921\r## West Virginia 2.7117590 3.0381601 5.3004067 3.8545331 3.2831874 1.4398440\r## Wisconsin 1.0354597 2.4410507 5.1085370 4.2281611 1.6666970 1.2105401\r## Wyoming 1.6218573 1.2586225 3.6325811 2.7329062 2.1883414 1.1687896\r## Illinois Indiana Iowa Kansas Kentucky Louisiana\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware ## Florida ## Georgia ## Hawaii ## Idaho ## Illinois ## Indiana 2.2027081 ## Iowa 3.7380070 1.7786548 ## Kansas 2.3228505 0.4287712 1.4699265 ## Kentucky 2.8478883 1.1790552 1.9426473 1.3020180 ## Louisiana 1.6535178 2.4957547 4.0359614 2.7284126 2.4221964 ## Maine 3.9342034 2.1029158 0.6457158 1.7913753 1.9925855 4.0901924\r## Maryland 1.3429997 2.5430878 4.0642448 2.7400943 2.8229479 1.2739137\r## Massachusetts 2.0080982 1.6615695 2.3510287 1.4343401 2.6284451 3.1524549\r## Michigan 1.3959090 2.6118471 4.3248636 2.9020920 3.1163494 1.6677999\r## Minnesota 3.1558788 1.3184866 0.7644384 0.9745872 1.9333640 3.6905974\r## Mississippi 3.0869477 3.0859068 4.1603272 3.2683740 2.3898884 1.6268879\r## Missouri 1.3552973 1.2203931 2.9398546 1.5192717 1.9677184 1.8362172\r## Montana 2.9659043 1.0033431 1.2403561 0.9170466 0.8523702 2.9444756\r## Nebraska 2.7962196 0.8570429 0.9821819 0.5279092 1.4219429 3.1706333\r## Nevada 2.3891753 3.5278633 5.2227312 3.8391728 4.1644286 2.8410670\r## New Hampshire 3.8490624 1.9278736 0.2058539 1.6084091 2.0093558 4.1168122\r## New Jersey 1.4562775 1.7638332 2.9122979 1.7071034 2.6914828 2.6826380\r## New Mexico 1.3393276 2.5909993 4.2131394 2.8356373 3.0007332 1.4911656\r## New York 0.3502188 2.4628527 4.0411586 2.6096016 3.1213366 1.7495096\r## North Carolina 3.0124311 3.3437548 4.2973973 3.4387635 2.8798080 1.9868618\r## North Dakota 4.6139615 2.6587932 1.0534375 2.3970805 2.4482563 4.6977846\r## Ohio 1.8124981 0.6976320 2.1610242 0.7817000 1.7726720 2.4996969\r## Oklahoma 1.8439860 0.5303259 1.9391446 0.5198728 1.4623483 2.3535566\r## Oregon 2.0743434 1.1780815 2.4662295 1.3426890 2.1388677 2.7490592\r## Pennsylvania 2.3134187 0.8412900 1.5708895 0.5456840 1.5944097 2.8440845\r## Rhode Island 2.5057761 2.3335609 2.5453686 2.0087021 3.0457816 3.5648047\r## South Carolina 2.6163680 2.8469842 4.1015324 3.0609333 2.4166385 1.3151908\r## South Dakota 3.8004708 1.8411735 0.9886706 1.6701106 1.5114990 3.7457555\r## Tennessee 1.9478353 1.8100316 3.4176329 2.1533060 1.7489942 1.1298534\r## Texas 0.8241352 2.0035762 3.6962443 2.2378289 2.5297839 1.3325285\r## Utah 2.2771632 1.4019666 2.1682069 1.2751603 2.5461745 3.3440990\r## Vermont 4.8624402 2.8667983 1.7298425 2.7298377 2.3888326 4.6795933\r## Virginia 1.8624960 0.6127246 2.1704984 0.8351949 1.0918624 1.9554079\r## Washington 2.0612962 1.1405746 2.2502832 1.1579118 2.2630242 2.9705622\r## West Virginia 4.1148082 2.2478563 1.5256890 2.1244674 1.5236299 3.7947215\r## Wisconsin 3.4790637 1.6806129 0.6318069 1.3242947 2.0950212 3.9559184\r## Wyoming 2.2643574 0.8898783 1.7194683 0.7588728 1.0694408 2.3837077\r## Maine Maryland Massachusetts Michigan Minnesota\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware ## Florida ## Georgia ## Hawaii ## Idaho ## Illinois ## Indiana ## Iowa ## Kansas ## Kentucky ## Louisiana ## Maine ## Maryland 4.1259083 ## Massachusetts 2.6920282 2.9743193 ## Michigan 4.5333420 1.0800988 3.0576915 ## Minnesota 1.2980362 3.6448929 1.6587245 3.7995101 ## Mississippi 4.0014591 2.2992240 4.1217248 2.9722824 4.1067600\r## Missouri 3.2055955 1.5705755 1.9810531 1.4068840 2.4088795\r## Montana 1.3271199 3.0249456 2.2919046 3.3348908 1.2662635\r## Nebraska 1.3218907 3.1309065 1.6863806 3.3478988 0.6083415\r## Nevada 5.5153139 2.2551337 3.8556049 1.2609417 4.6391114\r## New Hampshire 0.4995971 4.1663744 2.4573524 4.4646172 0.9279247\r## New Jersey 3.2532459 2.6263456 0.7977642 2.5678440 2.2254151\r## New Mexico 4.3460538 0.5353893 3.0274701 0.5782474 3.7377675\r## New York 4.2595904 1.4362170 2.2479437 1.2897453 3.4391596\r## North Carolina 4.0631653 2.0542355 4.0773401 3.0232021 4.2219622\r## North Dakota 0.7305609 4.7423030 3.3446903 5.1171939 1.8065731\r## Ohio 2.5455752 2.5061694 1.1567960 2.4459855 1.5216293\r## Oklahoma 2.1929825 2.2492942 1.3383233 2.4336743 1.4198434\r## Oregon 2.7813372 2.2466329 1.8709252 2.1626274 1.9270100\r## Pennsylvania 1.9197571 2.9585539 1.1337883 3.1048542 1.0106613\r## Rhode Island 2.7331079 3.4379146 0.9440940 3.7320501 2.0310592\r## South Carolina 4.0015575 1.6165582 3.8310425 2.3233363 3.9484630\r## South Dakota 0.7812991 3.7991896 2.8925136 4.1744724 1.4990317\r## Tennessee 3.5420469 1.5202431 2.9678843 1.5970196 3.1023238\r## Texas 3.9386296 1.5431868 2.2593978 1.2888621 3.1438264\r## Utah 2.6218087 3.0338001 0.9015809 2.9441421 1.4177147\r## Vermont 1.4253680 4.7430576 3.9277625 5.1250778 2.4019924\r## Virginia 2.3474650 2.0124420 1.8503795 2.2439957 1.7932233\r## Washington 2.6292546 2.5434911 1.3472994 2.4715215 1.5955418\r## West Virginia 1.1818120 4.0251562 3.3782752 4.4668346 2.0791705\r## Wisconsin 1.1485830 4.0091486 1.8882704 4.2034334 0.4940832\r## Wyoming 1.7665064 2.4041294 1.8201580 2.8324573 1.4845967\r## Mississippi Missouri Montana Nebraska Nevada\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware ## Florida ## Georgia ## Hawaii ## Idaho ## Illinois ## Indiana ## Iowa ## Kansas ## Kentucky ## Louisiana ## Maine ## Maryland ## Massachusetts ## Michigan ## Minnesota ## Mississippi ## Missouri 2.8692946 ## Montana 3.0015255 2.0313649 ## Nebraska 3.5269565 1.9651798 0.7389936 ## Nevada 4.1064793 2.3489003 4.3243112 4.2628916 ## New Hampshire 4.1895936 3.0885710 1.3329504 1.1300720 5.3871427\r## New Jersey 3.8894324 1.7079555 2.5912431 2.1246377 3.3464214\r## New Mexico 2.6557350 1.4579057 3.1915871 3.2494088 1.7234839\r## New York 3.2655822 1.5284764 3.2662661 3.0925340 2.1674148\r## North Carolina 1.1826891 3.0224849 3.2209267 3.6500186 4.1773437\r## North Dakota 4.4753078 3.7811273 1.8291157 1.9038740 6.0519445\r## Ohio 3.4148987 1.1327425 1.6436336 1.2654510 3.2930712\r## Oklahoma 3.0466140 1.0927654 1.2225315 0.9674809 3.4108696\r## Oregon 3.5033774 0.9974171 1.8044622 1.5727910 2.8581280\r## Pennsylvania 3.4971746 1.7793568 1.3246445 0.8483058 4.0392694\r## Rhode Island 4.3875001 2.7475003 2.6888576 2.1303973 4.6185330\r## South Carolina 0.7865674 2.3846001 2.9024302 3.3517226 3.4427701\r## South Dakota 3.5355186 2.8862448 0.8857149 1.2591419 5.1416772\r## Tennessee 1.8269569 1.2413874 2.2494023 2.5526834 2.6666268\r## Texas 2.8431727 1.1654171 2.8298991 2.7568751 2.2765693\r## Utah 4.2571173 1.7478909 2.0956369 1.4573012 3.5868975\r## Vermont 4.2046660 3.8803394 1.9261350 2.2952287 6.0437845\r## Virginia 2.5383053 0.9787310 1.1556682 1.2472262 3.3001850\r## Washington 3.8140404 1.2502752 1.8442691 1.3859985 3.1570805\r## West Virginia 3.3281129 3.2538044 1.2758193 1.8117833 5.4963193\r## Wisconsin 4.2987974 2.8171535 1.4916365 0.9719877 5.0751736\r## Wyoming 2.6813279 1.6073860 0.8150071 0.9268202 3.9202716\r## New Hampshire New Jersey New Mexico New York North Carolina\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware ## Florida ## Georgia ## Hawaii ## Idaho ## Illinois ## Indiana ## Iowa ## Kansas ## Kentucky ## Louisiana ## Maine ## Maryland ## Massachusetts ## Michigan ## Minnesota ## Mississippi ## Missouri ## Montana ## Nebraska ## Nevada ## New Hampshire ## New Jersey 3.0269198 ## New Mexico 4.3360809 2.6208087 ## New York 4.1586415 1.6344744 1.3324096 ## North Carolina 4.3157112 3.9418824 2.5348334 3.2163998 ## North Dakota 0.9231894 3.9166205 4.9450519 4.9325292 4.5836787\r## Ohio 2.3095495 1.1099823 2.4960904 2.0434995 3.6205693\r## Oklahoma 2.0697098 1.4711183 2.3426252 2.1367108 3.1366639\r## Oregon 2.6377191 1.9738854 2.1553130 2.2727718 3.5095191\r## Pennsylvania 1.6822035 1.4216058 3.0619915 2.5949374 3.6803956\r## Rhode Island 2.5813199 1.4668378 3.6032966 2.7682543 4.2185789\r## South Carolina 4.1596914 3.5826726 1.9596343 2.7755634 1.0476313\r## South Dakota 0.9874611 3.3318222 3.9969513 4.1124693 3.6955387\r## Tennessee 3.5298430 2.6339707 1.5528304 2.0847931 2.3374653\r## Texas 3.8178258 1.6226525 1.4418241 0.8457697 3.0857436\r## Utah 2.3304873 1.3141843 2.9843796 2.4826984 4.2680823\r## Vermont 1.6716127 4.4005416 4.9416825 5.1704762 4.3880034\r## Virginia 2.2878085 1.8255601 2.1341562 2.1439207 2.7517523\r## Washington 2.4214987 1.5759539 2.4796057 2.2747965 3.8055684\r## West Virginia 1.4648924 3.7402121 4.2681325 4.4279608 3.5978058\r## Wisconsin 0.7155628 2.4671212 4.1327758 3.7687073 4.4429456\r## Wyoming 1.7950754 2.0372127 2.6286722 2.5890441 2.7501141\r## North Dakota Ohio Oklahoma Oregon Pennsylvania\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware ## Florida ## Georgia ## Hawaii ## Idaho ## Illinois ## Indiana ## Iowa ## Kansas ## Kentucky ## Louisiana ## Maine ## Maryland ## Massachusetts ## Michigan ## Minnesota ## Mississippi ## Missouri ## Montana ## Nebraska ## Nevada ## New Hampshire ## New Jersey ## New Mexico ## New York ## North Carolina ## North Dakota ## Ohio 3.1448279 ## Oklahoma 2.8246690 0.6483903 ## Oregon 3.2862071 1.2407607 1.0734082 ## Pennsylvania 2.5555137 0.7781298 0.8180221 1.7293732 ## Rhode Island 3.4042300 1.9659747 1.9746699 2.6621371 1.6369255\r## South Carolina 4.5104172 3.1289884 2.7470931 3.0134453 3.3429642\r## South Dakota 1.0324944 2.4394250 2.0340486 2.4988870 1.9790714\r## Tennessee 4.0623149 2.0167804 1.8500296 2.0306758 2.4343114\r## Texas 4.5749422 1.6711510 1.8312655 2.1053000 2.2460705\r## Utah 3.1738212 1.0154223 1.2372916 1.2825152 1.2529078\r## Vermont 0.9824857 3.4825859 3.1010306 3.4262789 3.0270572\r## Virginia 2.9443461 0.9774388 0.5646254 1.2664430 1.1769236\r## Washington 3.1725909 0.9725013 0.9586525 0.5935343 1.3993323\r## West Virginia 1.2716808 2.8650371 2.4631736 3.0349855 2.3799278\r## Wisconsin 1.6216339 1.8649801 1.7916829 2.4088700 1.2204658\r## Wyoming 2.4170757 1.3086480 0.7366465 1.6013015 1.0684605\r## Rhode Island South Carolina South Dakota Tennessee Texas\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware ## Florida ## Georgia ## Hawaii ## Idaho ## Illinois ## Indiana ## Iowa ## Kansas ## Kentucky ## Louisiana ## Maine ## Maryland ## Massachusetts ## Michigan ## Minnesota ## Mississippi ## Missouri ## Montana ## Nebraska ## Nevada ## New Hampshire ## New Jersey ## New Mexico ## New York ## North Carolina ## North Dakota ## Ohio ## Oklahoma ## Oregon ## Pennsylvania ## Rhode Island ## South Carolina 4.1861320 ## South Dakota 3.1262712 3.5215978 ## Tennessee 3.5743861 1.4375120 3.0589938 ## Texas 2.8757996 2.4532276 3.7101039 1.4712840 ## Utah 1.7565845 3.8912317 2.6823382 2.8678113 2.4039834\r## Vermont 4.1104165 4.2668977 1.0856574 3.9356721 4.7444455\r## Virginia 2.4330133 2.2636538 2.0316897 1.3514491 1.6921625\r## Washington 2.1743525 3.3802314 2.5083824 2.3809584 2.1635337\r## West Virginia 3.5400858 3.4651680 0.7108812 3.1707450 3.9586581\r## Wisconsin 2.0779526 4.2190973 1.5437375 3.4257189 3.4539515\r## Wyoming 2.1726807 2.5059056 1.5644785 1.9298669 2.2564704\r## Utah Vermont Virginia Washington West Virginia Wisconsin\r## Alaska ## Arizona ## Arkansas ## California ## Colorado ## Connecticut ## Delaware ## Florida ## Georgia ## Hawaii ## Idaho ## Illinois ## Indiana ## Iowa ## Kansas ## Kentucky ## Louisiana ## Maine ## Maryland ## Massachusetts ## Michigan ## Minnesota ## Mississippi ## Missouri ## Montana ## Nebraska ## Nevada ## New Hampshire ## New Jersey ## New Mexico ## New York ## North Carolina ## North Dakota ## Ohio ## Oklahoma ## Oregon ## Pennsylvania ## Rhode Island ## South Carolina ## South Dakota ## Tennessee ## Texas ## Utah ## Vermont 3.6546040 ## Virginia 1.7612066 3.0638337 ## Washington 0.6940667 3.4804319 1.3809295 ## West Virginia 3.2680139 1.0380554 2.3353210 3.0846553 ## Wisconsin 1.8082282 2.3518637 2.1266497 2.0637823 2.0308890 ## Wyoming 1.8552036 2.6299335 0.7038309 1.5929546 1.8821600 1.7446366\rhc1=hclust(dist(zUSArrests),method=\"average\") # hclust() : hierarchical clustering\rplot(hc1, hang = -1)\rrect.hclust(hc1, k=5)\rcluster \u003c-cutree(hc1, k=5) #cutree() : 군집별 번호 매김\r\rcent \u003c-NULL\rfor (k in 1:5){\rcent \u003c-rbind(cent, colMeans(USArrests[cluster ==k,]))\r}\rcent\r## Murder Assault UrbanPop Rape\r## [1,] 14.671429 251.28571 54.28571 21.685714\r## [2,] 10.000000 263.00000 48.00000 44.500000\r## [3,] 10.883333 256.91667 78.33333 32.250000\r## [4,] 5.530435 129.43478 68.91304 17.786957\r## [5,] 2.700000 65.14286 46.28571 9.885714\r\r\rK-means clustering\r사전에 결정된 군집수 k에 기초하여 전체 데이터를 상대적으로 유사한 k개의 군집으로 구분\n장점\n- 신속한 계산결과로 대용량 데이터에 적합함\r- 군집분석 이외에도 분류,예측을 위한 선행작업, 이상치 처리작업등 다양한 분석에 사용 : 정제 천처리 등\r- 단독군집이 잘 안나옴\r- 계산속도가 빠르다 : n에 비례\r- 일반적으로 3번 정도 반복 -\u003e 3nk\r단점\n- 사전에 군집수 K를 결정하기 어려움 : 주관적 선택 필요\r- 군집결과의 해석이 용이하지 않을 수 있음\r- 초기값에 영향을 많이 받아 잘못 정하면 잘못된 결과 가능\r- 일반적으로 k-means는 군집수가 비슷하게 나옴 : 그래서 초기값에 영향을 많이 받음\r과정\r군집수 k를 결정한다.\n\r초기 k개 군집의 중심을 선택한다.\n\r각 관찰치를 그 중심과 가장 가까운 거리에 있는 군집에 할당한다.\n\r형성된 군집의 중심을 계산 : K-means 는 mean을 사용\n\r3-4의 과정을 기존의 중심과 새로운 중심의 차이가 없을 때까지 반복한다. -\u003e 재할당\n\r\r\rK 결정법\rK-평균 군집분석법의 결과는 초기 군집수 k의 결정에 민감하게 반응\n여러 가지의 k값으로 군집분석을 수행한 후 최적의 k값을 이용\n\rElbow point 계산하여 K 선택\n\rSilhouette plot 으로 K 선택\n\r\r시각화된 자료 그래프를 통하여 K를 결정\n\r자료의 시각화를 위하여는 차원의 축소가 필수적 : PCA 사용\r\r빅데이터에서 sampling한 데이터로 계층적 군집분석을 수행하여 K값을 선택\n\r계층적 군집분석 : 소용량 데이터만 가능\r\r\r\rElbow Point\rK를 결정하기 위한 한 방법으로 K-means에 적합함\nelbow point : 군집중심과 군집내 관찰값 간의 거리제곱의 합이 급격히 감소하다 완만해지는 부분\n\rsilhouette\r\n$$s(i)=\r{b(i)-a(i)\\over \\text{max}\\{a(i),b(i)\\}}=\r\\begin{cases}\r1-a(i)/b(i)\\ , \u0026 \\mbox{if }a(i)\u003cb(i) \\\\\r0\\ , \u0026 \\mbox{if }a(i)=b(i) \\\\\rb(i)/a(i)-1\\ , \u0026 \\mbox{if }a(i)\u003eb(i)\r\\end{cases}$$\n- 분모의 max : 관찰치의 수가 많을 수록 s(i)가 커지는 것을 방지\r- 1에 가까울수록 군집화가 잘 된 관찰값임\r- a(i) : 개체 i로부터 같은 군집 내에 있는 모든 다른 개체들 사이의 평균 거리\r- b(i) : 개체 i로부터 다른 군집 내에 있는 개체들 사이의 평균 거리 중 가장 작은 값 -\u003e 클수록 좋다\r코드\r### K-means Clustering\rset.seed(1234)\rkmc1 =kmeans(zUSArrests,4)\rkmc1\r## K-means clustering with 4 clusters of sizes 13, 13, 8, 16\r## ## Cluster means:\r## Murder Assault UrbanPop Rape\r## 1 0.6950701 1.0394414 0.7226370 1.27693964\r## 2 -0.9615407 -1.1066010 -0.9301069 -0.96676331\r## 3 1.4118898 0.8743346 -0.8145211 0.01927104\r## 4 -0.4894375 -0.3826001 0.5758298 -0.26165379\r## ## Clustering vector:\r## Alabama Alaska Arizona Arkansas California ## 3 1 1 3 1 ## Colorado Connecticut Delaware Florida Georgia ## 1 4 4 1 3 ## Hawaii Idaho Illinois Indiana Iowa ## 4 2 1 4 2 ## Kansas Kentucky Louisiana Maine Maryland ## 4 2 3 2 1 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 4 1 2 3 1 ## Montana Nebraska Nevada New Hampshire New Jersey ## 2 2 1 2 4 ## New Mexico New York North Carolina North Dakota Ohio ## 1 1 3 2 4 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 4 4 4 4 3 ## South Dakota Tennessee Texas Utah Vermont ## 2 3 1 4 2 ## Virginia Washington West Virginia Wisconsin Wyoming ## 4 4 2 2 4 ## ## Within cluster sum of squares by cluster:\r## [1] 19.922437 11.952463 8.316061 16.212213\r## (between_SS / total_SS = 71.2 %)\r## ## Available components:\r## ## [1] \"cluster\" \"centers\" \"totss\" \"withinss\" \"tot.withinss\"\r## [6] \"betweenss\" \"size\" \"iter\" \"ifault\"\rpairs(zUSArrests, col=kmc1$cluster, pch=16)\r## Elbow point\rwss =0\rfor (i in 1:10) wss[i] =sum(kmeans(zUSArrests,center=i)$withinss)\rplot(1:10, wss, type='b', xlab=\"Number of Clusters\", ylab=\"Within group sum of squares\")\r# transformation\rzdungratio =scale(dungratio)\rsummary(zdungratio)\r## fratio lratio sratio ## Min. :-7.48199 Min. :-2.868600 Min. :-5.6639 ## 1st Qu.:-0.43604 1st Qu.:-0.670959 1st Qu.:-0.1740 ## Median : 0.05938 Median : 0.003108 Median : 0.2042 ## Mean : 0.00000 Mean : 0.000000 Mean : 0.0000 ## 3rd Qu.: 0.57609 3rd Qu.: 0.667111 3rd Qu.: 0.5313 ## Max. : 2.23705 Max. : 2.939617 Max. : 1.4491\r# k 찾기 : 3D plot\r# library(rgl)\r# plot3d(zdungratio[,1], zdungratio[,2], zdungratio[,3], col=\"blue\", size=5) \r\r# k찾기 : hierarchical clustering\rhc_dung=hclust(dist(zdungratio), method=\"average\")\rplot(hc_dung, hang = -1)\rrect.hclust(hc_dung, k=3)\r# initial points\rtmp \u003c-data.frame(zdungratio)\rtmp$cluster \u003c-cutree(hc_dung, k=3)\rcent \u003c-NULL\rfor (k in 1:3){ cent \u003c-rbind(cent, colMeans(tmp[tmp$cluster ==k,])) }\rcent\r## fratio lratio sratio cluster\r## [1,] 0.07672644 0.001058849 0.2266506 1\r## [2,] 0.13034493 0.135477070 -3.2094215 2\r## [3,] -6.07115863 -0.767028418 0.4374348 3\rx1 \u003c-c(0,0,-6) ; x2 \u003c-c(0,0,0) ; x3 \u003c-c(0,-3,0)\rkcenters \u003c-data.frame(x1,x2,x3)\r\r# K-means clustering\rkmean_dung =kmeans(zdungratio,centers=kcenters) # centers = : 인위적인 초기값\rkmean_dung\r## K-means clustering with 3 clusters of sizes 635, 45, 9\r## ## Cluster means:\r## fratio lratio sratio\r## 1 0.07848197 0.001229268 0.2239685\r## 2 0.10676389 0.136059340 -3.2479316\r## 3 -6.07115863 -0.767028418 0.4374348\r## ## Clustering vector:\r## [1] 1 1 1 1 1 1 1 1 1 1 2 1 1 3 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [38] 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1\r## [75] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 3 1 2 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1\r## [112] 1 1 1 1 1 1 1 2 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [149] 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1\r## [186] 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1\r## [223] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1\r## [260] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 3 1 1 1 1 1 1\r## [297] 1 2 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1\r## [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1\r## [371] 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [408] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1\r## [445] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [482] 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2\r## [519] 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [556] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [593] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1\r## [630] 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r## [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\r## ## Within cluster sum of squares by cluster:\r## [1] 1084.96483 107.79811 20.67056\r## (between_SS / total_SS = 41.2 %)\r## ## Available components:\r## ## [1] \"cluster\" \"centers\" \"totss\" \"withinss\" \"tot.withinss\"\r## [6] \"betweenss\" \"size\" \"iter\" \"ifault\"\rpairs(zdungratio, col=kmean_dung$cluster, pch=16)\r\r\r\rK-medioid clustering\rK-means와 비슷하나 중심화 할때 medoid 사용\nmedoid : 변수별 중앙값의 좌표(object whose average dissimilarity to all the objects in the cluster is minimal)\nEuclidean distances가 아니기에 outlier에 robust함\n코드\r### K-medoids clustering\rlibrary(fpc)\rzUSArrests=scale(USArrests)\rkmed =pamk(zUSArrests) # pamk() : k=2~10을 해보고 silhouette을 바탕으로 최적을 찾음\rkmed\r## $pamobject\r## Medoids:\r## ID Murder Assault UrbanPop Rape\r## New Mexico 31 0.8292944 1.3708088 0.3081225 1.1603196\r## Nebraska 27 -0.8008247 -0.8250772 -0.2445636 -0.5052109\r## Clustering vector:\r## Alabama Alaska Arizona Arkansas California ## 1 1 1 2 1 ## Colorado Connecticut Delaware Florida Georgia ## 1 2 2 1 1 ## Hawaii Idaho Illinois Indiana Iowa ## 2 2 1 2 2 ## Kansas Kentucky Louisiana Maine Maryland ## 2 2 1 2 1 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 2 1 2 1 1 ## Montana Nebraska Nevada New Hampshire New Jersey ## 2 2 1 2 2 ## New Mexico New York North Carolina North Dakota Ohio ## 1 1 1 2 2 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 2 2 2 2 1 ## South Dakota Tennessee Texas Utah Vermont ## 2 1 1 2 2 ## Virginia Washington West Virginia Wisconsin Wyoming ## 2 2 2 2 2 ## Objective function:\r## build swap ## 1.441358 1.368969 ## ## Available components:\r## [1] \"medoids\" \"id.med\" \"clustering\" \"objective\" \"isolation\" ## [6] \"clusinfo\" \"silinfo\" \"diss\" \"call\" \"data\" ## ## $nc\r## [1] 2\r## ## $crit\r## [1] 0.0000000 0.4084890 0.3143656 0.3389904 0.3105170 0.2629987 0.2243815\r## [8] 0.2386072 0.2466113 0.2447023\rpairs(USArrests, col=kmed$pamobject$clustering, pch=16)\rpar(mfrow=c(1,2))\rplot(kmed$pamobject)\rexplain ##.## % : PCA의 설명력의 의미, 즉 n차원을 2차원의 그래프로 축소 (PCA 그래프)\naverage silhouette wideth : 0.## -\u003e 평균 silhouette를 의미\n값이 클수록 좋은것임\n\r\rDensity-based clustering\r밀도(densely populated area) 기반 군집 분석\n장점\n\rK를 미리 결정할 필요 없음\n\rnoise, outlier 에 영향 받지 않음\n\r\r단점\n\r밀도에만 의존하다 보니 군집의 해석이 어려울 수 있음\r\r2개의 모수\rEps : size of neighborhood 반지름\n\r반지름 안에 포함되는 point를 중심으로 또다시 반지름 -\u003e 점들이 게속 연결되어 군집이 커짐\r\rMinPts : minimum # of points 즉 최소 데이터 수\n\r\r\rdense point 기준점 : Min Points보다 neighborhood 안에 점이 더 많을때\n\r기준점에 속하는 neighborhood를 하나의 군집으로 분류\n\r어떠한 군집에도 속하지 않은 데이터는 noise, outlier\n\r\r\r코드\r## DBSCAN\rlibrary(fpc)\rdbscan(zUSArrests,eps=0.8)\r## dbscan Pts=50 MinPts=5 eps=0.8\r## 0 1 2\r## border 33 6 4\r## seed 0 5 2\r## total 33 11 6\rMinPtes=5가 기본값\neps는 시행착오를 통해 최적이 무엇인지 확인 필요\nborder : 군집 외각의 수\nseed : 군집 중심의 수\n\r\r\r","description":"","tags":["data mining","regression","logistic","random forest","decision tree"],"title":"데이터마이닝 - 이론정리","uri":"/posts/data_mining/0/"},{"categories":["R"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\r글꼴 설치\rR에 적용\rRmd body\r알아볼 패키지\r\r\r글꼴 설치\r코딩시 주로 사용하는 한글 글꼴\n\rD2 Coding 글꼴\n\r네이버 나눔코딩 글꼴\n\r\r트루타입 글꼴 설치 가능\nD2Coding 선호 : D2CodingLigature는 축약으로 직관적이나, 명확한 코드를 선호하기에 D2Coding를 추천한다.\n\rR에 적용\rinstall.packages(extrafont)\rlibrary(extrafont)\rfont_import(pattern = \"D2\")\r글꼴 적용 후 Tools-global options-Appearence에서 설정 가능\n\rRmd body\r\u003cstyle\u003e\rbody{\rfont-family:나눔바른고딕;\r}\r\u003c/style\u003e\r다음과 형태로 body 변환 가능\n\r알아볼 패키지\rshowtext 예시\n(CRAN에 패키지 미설치로 인해 추후 보완 예정)\ninstall.packages(\"KoNLP\") # CRAN에 없으므로 다른 방벙 알아서\rinstall.packages(\"devtools\")\rdevtools::install_github(\"lchiffon/wordcloud2\")\rlibrary(KoNLP)\rlibrary(wordcloud2)\r\r\rdt \u003c-readLines('./data/blog_text.txt', encoding = 'UTF-8')\r\r# 하다가 포기\rwordcloud2(data=dt,fontFamily = '나눔바른고딕')\r\r","description":"","tags":["R","R studio","글꼴"],"title":"R 글꼴 변경","uri":"/posts/r/3/"},{"categories":["R"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\r초기 설정\rR 설치\rR studio 설치\r추가 설치 : Git bash 및 우분투 등\r\r초기 설정 확인\r업데이트\r\r\rIDE setting for Data Science\n데이터 사이언스쪽으로 공부하고 싶어서 노트북을 새로 구매헀다. 무려 8년만에. 코드를 작성한다면 애플을 샀어야 했는데, 경제적 여건 상 국내 중소기업인 한성컴퓨터 제품을 구매헀다. 매우 쓰레기 였다.\n구매하고 한달도 안되어서 사용중 셧다운 또는 자동 절전모드로 전환되는 현상이 나타났다. 서비스 센터에 입고를 했더니, 증상이 안나온다고 다시 보냈다. 직접 증상이 나오는 것을 촬영하고 보냈더니 포맷한번 하고 보냈다. 내가 보내기 전에 포맷은 수도 없이 많이 했는데. 결국 3번 보내고 나서 SSD교체를 해주었다. 문제는 터치패드 근처에 아이패드를 가져다 되면 노트북이 꺼질 수 있다는 것이다. 신재품에서도 동일하게 나타나므로 주의를 요구한다고 말했다. 내가 컴퓨터 하루이틀 만진것도 아니고, 데스크탑은 직접 부품을 사서 다 조립도 할 수 있는데 고작 그 약한 자성떄문에 꺼지는게 당연하다고… 한성은 쓰레기다. 돈 벌면 바로 애플 살것이다.\n여튼 그래서 포맷을 수도없이 많이 했다. 즉 R과 Rstudio 등등 수도없이 반복해서 설치해보면서 나에게 익숙한 개발환경이 무엇인지 찾았다. 이제 본론으로 가자.\n초기 설정\rR 설치\r먼저 R을 설치한다. 설치시 유의할 점은 아래와 같다.\n\r64-bit만 설치\n\r설치 경로는 모두 영어로, 폴더명에 띄어쓰기는 지양하자\n\r한국어 보다는 영어를 기본언어로 설정하자\r\r\rR studio 설치\r이제 IDE인 R studio를 설치하면 된다. R studio를 설치하고 설정할 환경이 많다. (사실 이부분을 정리하고자 쓴거다.)\nTools - Global Options\n\rGeneral-Basic : Default working directory 변경\n\rGeneral-Basic : Workspace에서 .RData 사용하지 않음(never) 설정 및 시작시 자동 불러오기 미사용\n\rCode-Saving : encoding을 UTF-8로 변경\n\rAppearance : 테마 변경, 눈이 안좋으므로 zoom 125%에 Tommorrow Night Bright를 선호\n\rTerminal : Bash(Windows Subsystem for Linux), 참고로 우분트 등 설치후 가능\n\r\r기타\n\r마크다운 : Preview in Viewr Pane 체크\n\r마크다운 : Chunk Ouptput in Console 체크\n\r\r\r\r추가 설치 : Git bash 및 우분투 등\r참고로 윈도우10 기준 개발환경이다. 에플을 사야하는 이유가 이것이다. 지금부터는 에플이라면 필요없는 설치이기 때문이다.\n\rwindows 기능 켜기/끄기에서 Linux용 Windows 하위 시스템 체크\n\r우분투 설치\n\r파일 관리를 위한 git bash(https://git-scm.com/) 설치\n\rpdf 마트다운을 위한 TeX 설치\n\r\r\r\r초기 설정 확인\r이렇게 설치를 완료했다면 설치 및 설정이 잘되었는지 확인해 보자.\nsessionInfo()\r## R version 4.0.0 (2020-04-24)\r## Platform: x86_64-w64-mingw32/x64 (64-bit)\r## Running under: Windows 10 x64 (build 18363)\r## ## Matrix products: default\r## ## locale:\r## [1] LC_COLLATE=Korean_Korea.949 LC_CTYPE=Korean_Korea.949 ## [3] LC_MONETARY=Korean_Korea.949 LC_NUMERIC=C ## [5] LC_TIME=Korean_Korea.949 ## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached):\r## [1] compiler_4.0.0 magrittr_1.5 bookdown_0.19 tools_4.0.0 ## [5] htmltools_0.4.0 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 blogdown_0.19 knitr_1.28 stringr_1.4.0 ## [13] digest_0.6.25 xfun_0.14 rlang_0.4.6 evaluate_0.14\rgetwd()\r## [1] \"C:/Users/User/Documents/git/blog/content/posts\"\r경고표시나 중간중간 한글을 사용하면 문제가 많다. 그러는 경우 locale을 바꿔야 한다.\nSys.getlocale()\r## [1] \"LC_COLLATE=Korean_Korea.949;LC_CTYPE=Korean_Korea.949;LC_MONETARY=Korean_Korea.949;LC_NUMERIC=C;LC_TIME=Korean_Korea.949\"\rSys.setlocale() 로 원하는 설정을 할 수 있다.\n\r업데이트\r초기 설정 이후 시간이 지나면 R과 R studio 업데이트가 필요 할 수 있다. 먼저 R은 다음의 코드를 이용하면 된다.\n# install.packages('installr')\rinstallr::check.for.updates.R()\rinstallr::install.R()\rR studio의 경우 Help-Check for Update 를 확인해 보면된다.\n마지막으로 설치된 packages를 업데이트 해주면 된다.\nupdate.packages()\r\r","description":"","tags":["R","Rstudio","IDE","Git"],"title":"Data Science를 위한 R 초기 환경","uri":"/posts/r/1/"},{"categories":["R","data mining"],"content":"\r\r\r\r\r\r\r데이터마이닝 연습으로 Random Forest을 사용한 예제이다.\n먼저 전처리에서 사용한 packages이다.\nlibrary(tidyverse)\rlibrary(skimr)\r그 다음 모형을 만들고자 하는 데이터를 불러왔다. 해당 데이터는 카글에서 가지고 온 데이터다.\nraw_data=read_csv('./data/ausraindata.csv',col_types = cols(Evaporation = col_double(),\rSunshine = col_double()) )\r해당 데이터는 오늘의 날씨를 바탕으로 내일의 강수유무를 예측해 보는 데이터 이다. 이 자료는 수많은 Australian weather stations의 자료가 포함되어 있다. 총 24개의 열로 구성되어 있으며 각 열별 데이터의 의미는 아래와 같다.\n\rDate : The date of observation\n\rLocation : weather station의 위치\n\rMinTemp : minimum temperature in degrees Celsius\n\rMaxTemp : The maximum temperature in degrees Celsius\n\rRainfall : 당일 강수량 (mm)\n\rEvaporation : Class A pan evaporation (mm) in the 24 hours to 9am\n\rSunshine : The number of hours of bright sunshine in the day.\n\rWindGustDir : The direction of the strongest wind gust in the 24 hours to midnight\n\rWindGustSpeed : The speed (km/h) of the strongest wind gust in the 24 hours to midnight\n\rWindDir9am : Direction of the wind at 9am\n\rWindDir3pm : Direction of the wind at 3pm\n\rWindSpeed9am : Wind speed (km/hr) averaged over 10 minutes prior to 9am\n\rWindSpeed3pm : Wind speed (km/hr) averaged over 10 minutes prior to 3pm\n\rHumidity9am : Humidity (percent) at 9am\n\rHumidity3pm : Humidity (percent) at 3pm\n\rPressure9am : Atmospheric pressure (hpa) reduced to mean sea level at 9am\n\rPressure3pm : Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n\rCloud9am : Fraction of sky obscured by cloud at 9am. This is measured in “oktas”, which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n\rCloud3pm : Fraction of sky obscured by cloud at 3pm. This is measured in “oktas”, which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n\rTemp9am : Temperature (degrees C) at 9am\n\rTemp3pm : Temperature (degrees C) at 3pm\n\rRainToday : Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n\rRISK_MM : 다음날의 강수량(mm)\n\rRainTomorrow : 우리의 목표변수로 다음날 강수 유무\n\r\r전처리\r분석에 앞서, 실제 데이터 구성을 직접 보면서 전처리를 시행한다.\nglimpse(raw_data)\r## Rows: 142,193\r## Columns: 24\r## $ Date \u003cdate\u003e 2008-12-01, 2008-12-02, 2008-12-03, 2008-12-04, 2008...\r## $ Location \u003cchr\u003e \"Albury\", \"Albury\", \"Albury\", \"Albury\", \"Albury\", \"Al...\r## $ MinTemp \u003cdbl\u003e 13.4, 7.4, 12.9, 9.2, 17.5, 14.6, 14.3, 7.7, 9.7, 13....\r## $ MaxTemp \u003cdbl\u003e 22.9, 25.1, 25.7, 28.0, 32.3, 29.7, 25.0, 26.7, 31.9,...\r## $ Rainfall \u003cdbl\u003e 0.6, 0.0, 0.0, 0.0, 1.0, 0.2, 0.0, 0.0, 0.0, 1.4, 0.0...\r## $ Evaporation \u003cdbl\u003e NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...\r## $ Sunshine \u003cdbl\u003e NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...\r## $ WindGustDir \u003cchr\u003e \"W\", \"WNW\", \"WSW\", \"NE\", \"W\", \"WNW\", \"W\", \"W\", \"NNW\",...\r## $ WindGustSpeed \u003cdbl\u003e 44, 44, 46, 24, 41, 56, 50, 35, 80, 28, 30, 31, 61, 4...\r## $ WindDir9am \u003cchr\u003e \"W\", \"NNW\", \"W\", \"SE\", \"ENE\", \"W\", \"SW\", \"SSE\", \"SE\",...\r## $ WindDir3pm \u003cchr\u003e \"WNW\", \"WSW\", \"WSW\", \"E\", \"NW\", \"W\", \"W\", \"W\", \"NW\", ...\r## $ WindSpeed9am \u003cdbl\u003e 20, 4, 19, 11, 7, 19, 20, 6, 7, 15, 17, 15, 28, 24, N...\r## $ WindSpeed3pm \u003cdbl\u003e 24, 22, 26, 9, 20, 24, 24, 17, 28, 11, 6, 13, 28, 20,...\r## $ Humidity9am \u003cdbl\u003e 71, 44, 38, 45, 82, 55, 49, 48, 42, 58, 48, 89, 76, 6...\r## $ Humidity3pm \u003cdbl\u003e 22, 25, 30, 16, 33, 23, 19, 19, 9, 27, 22, 91, 93, 43...\r## $ Pressure9am \u003cdbl\u003e 1007.7, 1010.6, 1007.6, 1017.6, 1010.8, 1009.2, 1009....\r## $ Pressure3pm \u003cdbl\u003e 1007.1, 1007.8, 1008.7, 1012.8, 1006.0, 1005.4, 1008....\r## $ Cloud9am \u003cdbl\u003e 8, NA, NA, NA, 7, NA, 1, NA, NA, NA, NA, 8, 8, NA, 0,...\r## $ Cloud3pm \u003cdbl\u003e NA, NA, 2, NA, 8, NA, NA, NA, NA, NA, NA, 8, 8, 7, NA...\r## $ Temp9am \u003cdbl\u003e 16.9, 17.2, 21.0, 18.1, 17.8, 20.6, 18.1, 16.3, 18.3,...\r## $ Temp3pm \u003cdbl\u003e 21.8, 24.3, 23.2, 26.5, 29.7, 28.9, 24.6, 25.5, 30.2,...\r## $ RainToday \u003cchr\u003e \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",...\r## $ RISK_MM \u003cdbl\u003e 0.0, 0.0, 0.0, 1.0, 0.2, 0.0, 0.0, 0.0, 1.4, 0.0, 2.2...\r## $ RainTomorrow \u003cchr\u003e \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\"...\r먼저 이 데이터는 2008-12-01 부터 2017-06-24 까지의 데이터이다. 해당 날짜의 모든 데이터를 가지고 있는 시계열 자료로 볼 수 있으나, 뉴럴네트워크를 사용하기에 해당 Date 변수는 사용하지 않는다.\n또한 우리는 분류를 해야하므로 내일 강수량이 아닌 강수유무를 목적변수로 한다. 따라서 내일 강수량인 RISK_MM도 제외한다.\nraw_data %\u003e% filter(is.na(Evaporation)) %\u003e% count() / nrow(raw_data)\r## n\r## 1 0.4278903\rraw_data %\u003e% filter(is.na(Sunshine)) %\u003e% count() / nrow(raw_data)\r## n\r## 1 0.4769292\rraw_data %\u003e% filter(is.na(Cloud9am)) %\u003e% count() / nrow(raw_data)\r## n\r## 1 0.3773533\rraw_data %\u003e% filter(is.na(Cloud3pm)) %\u003e% count() / nrow(raw_data)\r## n\r## 1 0.4015247\r또한 위에서 볼 수있듯이 4개의 열들은 결측치의 값이 높은 것을 알 수 있다. 해당 행을 `na.omit을 이용해 삭제하였다.\nnrow( raw_data %\u003e% distinct(Location) )\r## [1] 49\rnrow( raw_data %\u003e% distinct(WindGustDir) )\r## [1] 17\rnrow( raw_data %\u003e% distinct(WindDir9am) )\r## [1] 17\rnrow( raw_data %\u003e% distinct(WindDir3pm) )\r## [1] 17\r또한 49개로 그 범주의 개수가 다양한 Location과 16개(NA 1개포함) 방향변수들도 제거하였다.\n앞서 언급한 열들을 제거하고, na인 행을 제거하고, 비가 온 경우(‘Yes’)를 1로 변환하였다. factor형으로 만들었다.\ntemp_data = raw_data %\u003e%\rselect(\r-Date, -RISK_MM,\r# -Evaporation, -Sunshine, -Cloud9am, -Cloud3pm,\r-Location, -WindGustDir,-WindDir9am, -WindDir3pm\r) %\u003e%\rna.omit() %\u003e%\rmutate(\rRainToday = as.numeric(RainToday=='Yes'),\rRainTomorrow = as.factor(as.numeric(RainTomorrow=='Yes'))\r)\r총 58090개의 관측지가 있다. 그 결과를 skimr 패키지를 이용해 간략히 보였다.\nskim(temp_data) # skimr 패키지\r\rTable 1: Data summary\r\rName\rtemp_data\r\rNumber of rows\r58090\r\rNumber of columns\r18\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r1\r\rnumeric\r17\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rRainTomorrow\r0\r1\rFALSE\r2\r0: 45361, 1: 12729\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rMinTemp\r0\r1\r13.34\r6.47\r-6.7\r8.4\r13.1\r18.3\r31.4\r▁▅▇▆▁\r\rMaxTemp\r0\r1\r24.13\r6.97\r4.1\r18.6\r23.8\r29.6\r48.1\r▁▇▇▅▁\r\rRainfall\r0\r1\r2.12\r6.99\r0.0\r0.0\r0.0\r0.6\r206.2\r▇▁▁▁▁\r\rEvaporation\r0\r1\r5.45\r3.69\r0.0\r2.8\r4.8\r7.4\r81.2\r▇▁▁▁▁\r\rSunshine\r0\r1\r7.70\r3.77\r0.0\r5.0\r8.6\r10.7\r14.5\r▃▃▅▇▃\r\rWindGustSpeed\r0\r1\r40.56\r13.38\r9.0\r31.0\r39.0\r48.0\r124.0\r▃▇▂▁▁\r\rWindSpeed9am\r0\r1\r15.24\r8.58\r0.0\r9.0\r15.0\r20.0\r67.0\r▇▆▂▁▁\r\rWindSpeed3pm\r0\r1\r19.58\r8.56\r0.0\r13.0\r19.0\r24.0\r76.0\r▆▇▂▁▁\r\rHumidity9am\r0\r1\r66.22\r18.63\r0.0\r55.0\r67.0\r80.0\r100.0\r▁▂▅▇▅\r\rHumidity3pm\r0\r1\r49.70\r20.22\r0.0\r36.0\r51.0\r63.0\r100.0\r▂▅▇▅▂\r\rPressure9am\r0\r1\r1017.33\r6.94\r980.5\r1012.7\r1017.3\r1022.0\r1040.4\r▁▁▇▇▁\r\rPressure3pm\r0\r1\r1014.88\r6.90\r977.1\r1010.1\r1014.8\r1019.5\r1038.9\r▁▁▇▇▁\r\rCloud9am\r0\r1\r4.25\r2.80\r0.0\r1.0\r5.0\r7.0\r8.0\r▆▃▁▃▇\r\rCloud3pm\r0\r1\r4.33\r2.65\r0.0\r2.0\r5.0\r7.0\r9.0\r▆▃▃▇▂\r\rTemp9am\r0\r1\r18.09\r6.60\r-0.9\r12.9\r17.7\r23.2\r39.4\r▁▇▇▅▁\r\rTemp3pm\r0\r1\r22.63\r6.84\r3.7\r17.3\r22.3\r27.8\r46.1\r▁▇▇▃▁\r\rRainToday\r0\r1\r0.22\r0.41\r0.0\r0.0\r0.0\r0.0\r1.0\r▇▁▁▁▂\r\r\r\r이상 전처리를 완료하였다.\n\r1. Training 데이터와 Test 데이터를 50:50의 비율로 분할하시오.\rset.seed(1234)\rsp_n = sample(1:nrow(temp_data), round(nrow(temp_data)/2)) train = temp_data[sp_n,]\rtest = temp_data[-sp_n,]\rnrow(train);nrow(test)\r## [1] 29045\r## [1] 29045\r\r2. R 프로그램의 ‘randomForest’ 명령어를 사용하여 랜덤포레스트 분석을 수행하고자 한다. 단, hyper-parameter는 아래와 같이 조정한다.\r\rA. ntree=100 을 사용하고,\n\rB. mtry = (0.1p, 0.2p, … , 1.0*p)을 내림하여 사용한다.\n\rC. nodesize = (1, 0.01n, 0.02n, … , 0.10*n) 을 반올림하여 사용한다.\n\rD. 그외 parameter 값들은 default 값을 사용한다.\n\r\r먼저 17개의 변수로 RainTomorrow를 예측하므로 p는 17이 된다. 따라서 B의 조건에 따른 mtry를 구할 수 있다. 또한 train 데이터의 수는 29045개 이므로 n은 29045가 된다. 따라서 C조건에 따른 nodesize를 구할 수 있다. 각각은 다음과 같다.\n( para_mtry = floor(17 * 1:10 / 10) )\r## [1] 1 3 5 6 8 10 11 13 15 17\r( para_ndsz = c(1, round( nrow(train) * 1:10 / 100 ) ) )\r## [1] 1 290 581 871 1162 1452 1743 2033 2324 2614 2904\r이제 코드를 돌리면 아래와 같다.\nlibrary(randomForest)\r## randomForest 4.6-14\r## Type rfNews() to see new features/changes/bug fixes.\r## ## Attaching package: 'randomForest'\r## The following object is masked from 'package:dplyr':\r## ## combine\r## The following object is masked from 'package:ggplot2':\r## ## margin\rlibrary(pROC)\r## Type 'citation(\"pROC\")' for a citation.\r## ## Attaching package: 'pROC'\r## The following objects are masked from 'package:stats':\r## ## cov, smooth, var\rfor(mt in para_mtry){\rfor(nd in para_ndsz){\rprint(paste('rf',mt,nd,sep='_'))\rset.seed(2010)\rassign(\rpaste('rf',mt,nd,sep='_'),\rrandomForest(RainTomorrow ~ ., data = train, ntree=100, mtry=mt, nodesize=nd)\r)\r}\r}\r## [1] \"rf_1_1\"\r## [1] \"rf_1_290\"\r## [1] \"rf_1_581\"\r## [1] \"rf_1_871\"\r## [1] \"rf_1_1162\"\r## [1] \"rf_1_1452\"\r## [1] \"rf_1_1743\"\r## [1] \"rf_1_2033\"\r## [1] \"rf_1_2324\"\r## [1] \"rf_1_2614\"\r## [1] \"rf_1_2904\"\r## [1] \"rf_3_1\"\r## [1] \"rf_3_290\"\r## [1] \"rf_3_581\"\r## [1] \"rf_3_871\"\r## [1] \"rf_3_1162\"\r## [1] \"rf_3_1452\"\r## [1] \"rf_3_1743\"\r## [1] \"rf_3_2033\"\r## [1] \"rf_3_2324\"\r## [1] \"rf_3_2614\"\r## [1] \"rf_3_2904\"\r## [1] \"rf_5_1\"\r## [1] \"rf_5_290\"\r## [1] \"rf_5_581\"\r## [1] \"rf_5_871\"\r## [1] \"rf_5_1162\"\r## [1] \"rf_5_1452\"\r## [1] \"rf_5_1743\"\r## [1] \"rf_5_2033\"\r## [1] \"rf_5_2324\"\r## [1] \"rf_5_2614\"\r## [1] \"rf_5_2904\"\r## [1] \"rf_6_1\"\r## [1] \"rf_6_290\"\r## [1] \"rf_6_581\"\r## [1] \"rf_6_871\"\r## [1] \"rf_6_1162\"\r## [1] \"rf_6_1452\"\r## [1] \"rf_6_1743\"\r## [1] \"rf_6_2033\"\r## [1] \"rf_6_2324\"\r## [1] \"rf_6_2614\"\r## [1] \"rf_6_2904\"\r## [1] \"rf_8_1\"\r## [1] \"rf_8_290\"\r## [1] \"rf_8_581\"\r## [1] \"rf_8_871\"\r## [1] \"rf_8_1162\"\r## [1] \"rf_8_1452\"\r## [1] \"rf_8_1743\"\r## [1] \"rf_8_2033\"\r## [1] \"rf_8_2324\"\r## [1] \"rf_8_2614\"\r## [1] \"rf_8_2904\"\r## [1] \"rf_10_1\"\r## [1] \"rf_10_290\"\r## [1] \"rf_10_581\"\r## [1] \"rf_10_871\"\r## [1] \"rf_10_1162\"\r## [1] \"rf_10_1452\"\r## [1] \"rf_10_1743\"\r## [1] \"rf_10_2033\"\r## [1] \"rf_10_2324\"\r## [1] \"rf_10_2614\"\r## [1] \"rf_10_2904\"\r## [1] \"rf_11_1\"\r## [1] \"rf_11_290\"\r## [1] \"rf_11_581\"\r## [1] \"rf_11_871\"\r## [1] \"rf_11_1162\"\r## [1] \"rf_11_1452\"\r## [1] \"rf_11_1743\"\r## [1] \"rf_11_2033\"\r## [1] \"rf_11_2324\"\r## [1] \"rf_11_2614\"\r## [1] \"rf_11_2904\"\r## [1] \"rf_13_1\"\r## [1] \"rf_13_290\"\r## [1] \"rf_13_581\"\r## [1] \"rf_13_871\"\r## [1] \"rf_13_1162\"\r## [1] \"rf_13_1452\"\r## [1] \"rf_13_1743\"\r## [1] \"rf_13_2033\"\r## [1] \"rf_13_2324\"\r## [1] \"rf_13_2614\"\r## [1] \"rf_13_2904\"\r## [1] \"rf_15_1\"\r## [1] \"rf_15_290\"\r## [1] \"rf_15_581\"\r## [1] \"rf_15_871\"\r## [1] \"rf_15_1162\"\r## [1] \"rf_15_1452\"\r## [1] \"rf_15_1743\"\r## [1] \"rf_15_2033\"\r## [1] \"rf_15_2324\"\r## [1] \"rf_15_2614\"\r## [1] \"rf_15_2904\"\r## [1] \"rf_17_1\"\r## [1] \"rf_17_290\"\r## [1] \"rf_17_581\"\r## [1] \"rf_17_871\"\r## [1] \"rf_17_1162\"\r## [1] \"rf_17_1452\"\r## [1] \"rf_17_1743\"\r## [1] \"rf_17_2033\"\r## [1] \"rf_17_2324\"\r## [1] \"rf_17_2614\"\r## [1] \"rf_17_2904\"\r각각의 parameter별 모형은 **rf_(mtry값)_(nodesize값)** 라는 객체에 저장되었다. 즉 mtry가 1이고 nodesize가 290인 모형은 rf_1_290라는 객체 저장되어 있다.\n\r3. 위 2번의 조건에 맞는 랜덤포레스트를 training 데이터를 이용하여 생성하고, test 데이터를 이용하여 예측 정확도를 계산하고자 한다. 이때 예측정확도는 AUROC 값을 사용한다.\r앞서 random forest를 생성하였다. auroc를 test 데이터를 이용해 계산하면 아래와 같다. (message=F)\nfor( md in ls(pattern = \"^rf_\") ){\rassign(\rpaste('pred',md ,sep='_'),\rpredict( get(md), newdata=test , type=\"prob\")\r)\rassign(\rpaste('roc',md ,sep='_'),\rroc(test$RainTomorrow ~ get(paste('pred',md ,sep='_'))[,2])\r)\r}\rtest 데이터를 이용하여 각각의 예측 정확도auroc값을 계산한 결과는 아래와 같다.\nauc_matrix = matrix(nrow=10,ncol=11)\rfor( md in ls(pattern = \"^roc_\") ){\rtemp_mt = strsplit(md,split='_')[[1]][3]\rtemp_ns = strsplit(md,split='_')[[1]][4]\rcat('mtry값:',temp_mt,'\\t',\r'nodesize값:',temp_ns,'\\n',\rsep =\"\")\rtemp_auc = get(md)$auc print(temp_auc)\rcat('\\n')\rauc_matrix[which(para_mtry ==temp_mt),which(para_ndsz ==temp_ns)] = temp_auc\r}\r## mtry값:1 nodesize값:1\r## Area under the curve: 0.8798\r## ## mtry값:1 nodesize값:1162\r## Area under the curve: 0.853\r## ## mtry값:1 nodesize값:1452\r## Area under the curve: 0.8509\r## ## mtry값:1 nodesize값:1743\r## Area under the curve: 0.8466\r## ## mtry값:1 nodesize값:2033\r## Area under the curve: 0.849\r## ## mtry값:1 nodesize값:2324\r## Area under the curve: 0.8509\r## ## mtry값:1 nodesize값:2614\r## Area under the curve: 0.8466\r## ## mtry값:1 nodesize값:290\r## Area under the curve: 0.8572\r## ## mtry값:1 nodesize값:2904\r## Area under the curve: 0.8424\r## ## mtry값:1 nodesize값:581\r## Area under the curve: 0.8548\r## ## mtry값:1 nodesize값:871\r## Area under the curve: 0.8542\r## ## mtry값:10 nodesize값:1\r## Area under the curve: 0.8886\r## ## mtry값:10 nodesize값:1162\r## Area under the curve: 0.8393\r## ## mtry값:10 nodesize값:1452\r## Area under the curve: 0.8436\r## ## mtry값:10 nodesize값:1743\r## Area under the curve: 0.8351\r## ## mtry값:10 nodesize값:2033\r## Area under the curve: 0.8312\r## ## mtry값:10 nodesize값:2324\r## Area under the curve: 0.8316\r## ## mtry값:10 nodesize값:2614\r## Area under the curve: 0.8313\r## ## mtry값:10 nodesize값:290\r## Area under the curve: 0.859\r## ## mtry값:10 nodesize값:2904\r## Area under the curve: 0.8306\r## ## mtry값:10 nodesize값:581\r## Area under the curve: 0.8465\r## ## mtry값:10 nodesize값:871\r## Area under the curve: 0.8466\r## ## mtry값:11 nodesize값:1\r## Area under the curve: 0.888\r## ## mtry값:11 nodesize값:1162\r## Area under the curve: 0.8392\r## ## mtry값:11 nodesize값:1452\r## Area under the curve: 0.8365\r## ## mtry값:11 nodesize값:1743\r## Area under the curve: 0.8294\r## ## mtry값:11 nodesize값:2033\r## Area under the curve: 0.8265\r## ## mtry값:11 nodesize값:2324\r## Area under the curve: 0.8334\r## ## mtry값:11 nodesize값:2614\r## Area under the curve: 0.8274\r## ## mtry값:11 nodesize값:290\r## Area under the curve: 0.8599\r## ## mtry값:11 nodesize값:2904\r## Area under the curve: 0.8289\r## ## mtry값:11 nodesize값:581\r## Area under the curve: 0.8479\r## ## mtry값:11 nodesize값:871\r## Area under the curve: 0.8373\r## ## mtry값:13 nodesize값:1\r## Area under the curve: 0.887\r## ## mtry값:13 nodesize값:1162\r## Area under the curve: 0.8391\r## ## mtry값:13 nodesize값:1452\r## Area under the curve: 0.831\r## ## mtry값:13 nodesize값:1743\r## Area under the curve: 0.8314\r## ## mtry값:13 nodesize값:2033\r## Area under the curve: 0.8231\r## ## mtry값:13 nodesize값:2324\r## Area under the curve: 0.823\r## ## mtry값:13 nodesize값:2614\r## Area under the curve: 0.8182\r## ## mtry값:13 nodesize값:290\r## Area under the curve: 0.8557\r## ## mtry값:13 nodesize값:2904\r## Area under the curve: 0.8207\r## ## mtry값:13 nodesize값:581\r## Area under the curve: 0.8423\r## ## mtry값:13 nodesize값:871\r## Area under the curve: 0.8393\r## ## mtry값:15 nodesize값:1\r## Area under the curve: 0.8856\r## ## mtry값:15 nodesize값:1162\r## Area under the curve: 0.8333\r## ## mtry값:15 nodesize값:1452\r## Area under the curve: 0.8302\r## ## mtry값:15 nodesize값:1743\r## Area under the curve: 0.8219\r## ## mtry값:15 nodesize값:2033\r## Area under the curve: 0.8201\r## ## mtry값:15 nodesize값:2324\r## Area under the curve: 0.8177\r## ## mtry값:15 nodesize값:2614\r## Area under the curve: 0.8135\r## ## mtry값:15 nodesize값:290\r## Area under the curve: 0.8576\r## ## mtry값:15 nodesize값:2904\r## Area under the curve: 0.8134\r## ## mtry값:15 nodesize값:581\r## Area under the curve: 0.8329\r## ## mtry값:15 nodesize값:871\r## Area under the curve: 0.8343\r## ## mtry값:17 nodesize값:1\r## Area under the curve: 0.8852\r## ## mtry값:17 nodesize값:1162\r## Area under the curve: 0.8248\r## ## mtry값:17 nodesize값:1452\r## Area under the curve: 0.8245\r## ## mtry값:17 nodesize값:1743\r## Area under the curve: 0.8235\r## ## mtry값:17 nodesize값:2033\r## Area under the curve: 0.815\r## ## mtry값:17 nodesize값:2324\r## Area under the curve: 0.8085\r## ## mtry값:17 nodesize값:2614\r## Area under the curve: 0.8077\r## ## mtry값:17 nodesize값:290\r## Area under the curve: 0.8545\r## ## mtry값:17 nodesize값:2904\r## Area under the curve: 0.7965\r## ## mtry값:17 nodesize값:581\r## Area under the curve: 0.8312\r## ## mtry값:17 nodesize값:871\r## Area under the curve: 0.827\r## ## mtry값:3 nodesize값:1\r## Area under the curve: 0.89\r## ## mtry값:3 nodesize값:1162\r## Area under the curve: 0.8507\r## ## mtry값:3 nodesize값:1452\r## Area under the curve: 0.8467\r## ## mtry값:3 nodesize값:1743\r## Area under the curve: 0.8435\r## ## mtry값:3 nodesize값:2033\r## Area under the curve: 0.8476\r## ## mtry값:3 nodesize값:2324\r## Area under the curve: 0.8467\r## ## mtry값:3 nodesize값:2614\r## Area under the curve: 0.8396\r## ## mtry값:3 nodesize값:290\r## Area under the curve: 0.8589\r## ## mtry값:3 nodesize값:2904\r## Area under the curve: 0.8432\r## ## mtry값:3 nodesize값:581\r## Area under the curve: 0.8547\r## ## mtry값:3 nodesize값:871\r## Area under the curve: 0.8531\r## ## mtry값:5 nodesize값:1\r## Area under the curve: 0.8902\r## ## mtry값:5 nodesize값:1162\r## Area under the curve: 0.842\r## ## mtry값:5 nodesize값:1452\r## Area under the curve: 0.8445\r## ## mtry값:5 nodesize값:1743\r## Area under the curve: 0.8416\r## ## mtry값:5 nodesize값:2033\r## Area under the curve: 0.8336\r## ## mtry값:5 nodesize값:2324\r## Area under the curve: 0.8358\r## ## mtry값:5 nodesize값:2614\r## Area under the curve: 0.8394\r## ## mtry값:5 nodesize값:290\r## Area under the curve: 0.8624\r## ## mtry값:5 nodesize값:2904\r## Area under the curve: 0.837\r## ## mtry값:5 nodesize값:581\r## Area under the curve: 0.8525\r## ## mtry값:5 nodesize값:871\r## Area under the curve: 0.8487\r## ## mtry값:6 nodesize값:1\r## Area under the curve: 0.8896\r## ## mtry값:6 nodesize값:1162\r## Area under the curve: 0.8495\r## ## mtry값:6 nodesize값:1452\r## Area under the curve: 0.8422\r## ## mtry값:6 nodesize값:1743\r## Area under the curve: 0.8427\r## ## mtry값:6 nodesize값:2033\r## Area under the curve: 0.8414\r## ## mtry값:6 nodesize값:2324\r## Area under the curve: 0.8394\r## ## mtry값:6 nodesize값:2614\r## Area under the curve: 0.8389\r## ## mtry값:6 nodesize값:290\r## Area under the curve: 0.8598\r## ## mtry값:6 nodesize값:2904\r## Area under the curve: 0.8303\r## ## mtry값:6 nodesize값:581\r## Area under the curve: 0.8477\r## ## mtry값:6 nodesize값:871\r## Area under the curve: 0.8476\r## ## mtry값:8 nodesize값:1\r## Area under the curve: 0.8891\r## ## mtry값:8 nodesize값:1162\r## Area under the curve: 0.844\r## ## mtry값:8 nodesize값:1452\r## Area under the curve: 0.8441\r## ## mtry값:8 nodesize값:1743\r## Area under the curve: 0.8379\r## ## mtry값:8 nodesize값:2033\r## Area under the curve: 0.8389\r## ## mtry값:8 nodesize값:2324\r## Area under the curve: 0.8368\r## ## mtry값:8 nodesize값:2614\r## Area under the curve: 0.8314\r## ## mtry값:8 nodesize값:290\r## Area under the curve: 0.8596\r## ## mtry값:8 nodesize값:2904\r## Area under the curve: 0.8304\r## ## mtry값:8 nodesize값:581\r## Area under the curve: 0.8494\r## ## mtry값:8 nodesize값:871\r## Area under the curve: 0.8464\r행렬로도 보여주면 아래와 같다.\nrownames(auc_matrix) = para_mtry\rcolnames(auc_matrix) = para_ndsz\rauc_matrix\r## 1 290 581 871 1162 1452 1743\r## 1 0.8797539 0.8572112 0.8548120 0.8541534 0.8530406 0.8508504 0.8465923\r## 3 0.8899524 0.8589102 0.8546948 0.8531171 0.8507239 0.8466689 0.8434589\r## 5 0.8901695 0.8624353 0.8525360 0.8486703 0.8419568 0.8445128 0.8416349\r## 6 0.8895977 0.8597663 0.8477312 0.8476448 0.8495143 0.8421653 0.8426893\r## 8 0.8890539 0.8596478 0.8494105 0.8464140 0.8439507 0.8441229 0.8379413\r## 10 0.8885989 0.8589943 0.8464877 0.8465859 0.8392590 0.8435724 0.8351362\r## 11 0.8880276 0.8599148 0.8479312 0.8372760 0.8392385 0.8365081 0.8294359\r## 13 0.8869692 0.8557121 0.8423401 0.8392755 0.8390583 0.8309899 0.8313972\r## 15 0.8855846 0.8575909 0.8329103 0.8342663 0.8333442 0.8302059 0.8218959\r## 17 0.8851665 0.8544948 0.8311615 0.8269814 0.8248338 0.8244663 0.8234599\r## 2033 2324 2614 2904\r## 1 0.8489772 0.8509397 0.8465913 0.8424482\r## 3 0.8475778 0.8466515 0.8395814 0.8431646\r## 5 0.8336063 0.8358320 0.8394180 0.8369673\r## 6 0.8413764 0.8393648 0.8389086 0.8302772\r## 8 0.8388660 0.8367690 0.8313858 0.8303713\r## 10 0.8311746 0.8315563 0.8313170 0.8306044\r## 11 0.8264946 0.8333505 0.8274361 0.8288907\r## 13 0.8231113 0.8229738 0.8182062 0.8207216\r## 15 0.8200801 0.8176567 0.8134933 0.8133673\r## 17 0.8149581 0.8084856 0.8076712 0.7964592\r\r4. 3번의 결과, 총 110개의 AUROC 값을 구할 수 있다. 이를 mtry 값과 nodesize 값의 조합에 따라 AUROC 값으로 3차원 포물선 그래프를 생성하시오. (3D surface plot)\r해당 auroc를 이용하여 3d surface plot을 그리면 아래와 같다.\nlibrary(plotly)\r## ## Attaching package: 'plotly'\r## The following object is masked from 'package:ggplot2':\r## ## last_plot\r## The following object is masked from 'package:stats':\r## ## filter\r## The following object is masked from 'package:graphics':\r## ## layout\rplot_ly(z = auc_matrix,x=para_mtry,y=para_ndsz) %\u003e%\radd_surface() %\u003e%\rlayout(scene = list(\rxaxis = list(title = 'mtry',tickvals=para_mtry),\ryaxis = list(title = 'nodesize',tickvals=para_ndsz),\rzaxis = list(title = 'auc'))\r)\r\r{\"x\":{\"visdat\":{\"12d85a685ec2\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"12d85a685ec2\",\"attrs\":{\"12d85a685ec2\":{\"z\":[[0.879753909191365,0.857211175123725,0.854811972002102,0.854153366076446,0.853040569816661,0.850850435213139,0.8465923363023,0.848977171104065,0.85093973905314,0.846591314172719,0.842448227209508],[0.889952446264445,0.858910185067677,0.854694788459305,0.853117105640692,0.850723894194487,0.846668909982982,0.843458934405753,0.847577824085869,0.846651513131034,0.839581429057744,0.843164571411155],[0.890169523185017,0.862435269084319,0.852536040461079,0.848670332622165,0.841956789372187,0.844512815391848,0.84163488394314,0.833606324527615,0.835831958345349,0.839417960596678,0.836967272429525],[0.889597657171627,0.859766323557383,0.847731184821085,0.847644789060193,0.84951433633443,0.842165327897173,0.842689349986571,0.841376360872615,0.839364847715151,0.838908599355807,0.830277172771134],[0.889053873910343,0.859647839122393,0.849410492098883,0.846413993618855,0.843950716394448,0.844122909092841,0.837941293089318,0.838865972766641,0.836768979290939,0.831385784150213,0.830371274081287],[0.888598898911082,0.858994257806795,0.846487690194052,0.846585924762205,0.83925898331104,0.84357238046463,0.835136180629803,0.831174609425703,0.831556287065361,0.831316984849066,0.830604440078586],[0.888027628279568,0.859914766369467,0.847931205599569,0.837275996860954,0.839238516628837,0.836508133198641,0.829435942918943,0.826494625670199,0.833350530577485,0.827436120583648,0.828890655716155],[0.886969249235017,0.855712079271332,0.842340132704494,0.839275478486389,0.839058267346781,0.830989855202164,0.831397240949449,0.823111256306281,0.822973819455133,0.818206173461681,0.820721624034365],[0.885584556182319,0.857590863568442,0.832910302464593,0.834266265760601,0.833344236048857,0.830205861164958,0.821895937352226,0.820080125873503,0.817656653196995,0.813493268185754,0.813367301899896],[0.885166481093363,0.85449475735628,0.831161473167763,0.826981365841266,0.824833819970259,0.824466293835094,0.823459909180104,0.814958114093421,0.808485552518443,0.807671235303687,0.796459206849747]],\"x\":[1,3,5,6,8,10,11,13,15,17],\"y\":[1,290,581,871,1162,1452,1743,2033,2324,2614,2904],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"surface\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"mtry\",\"tickvals\":[1,3,5,6,8,10,11,13,15,17]},\"yaxis\":{\"title\":\"nodesize\",\"tickvals\":[1,290,581,871,1162,1452,1743,2033,2324,2614,2904]},\"zaxis\":{\"title\":\"auc\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666665\",\"rgba(70,19,97,1)\"],[\"0.0833333333333329\",\"rgba(72,32,111,1)\"],[\"0.125000000000001\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333334\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666666\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.374999999999999\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333334\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625000000000001\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.874999999999999\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"z\":[[0.879753909191365,0.857211175123725,0.854811972002102,0.854153366076446,0.853040569816661,0.850850435213139,0.8465923363023,0.848977171104065,0.85093973905314,0.846591314172719,0.842448227209508],[0.889952446264445,0.858910185067677,0.854694788459305,0.853117105640692,0.850723894194487,0.846668909982982,0.843458934405753,0.847577824085869,0.846651513131034,0.839581429057744,0.843164571411155],[0.890169523185017,0.862435269084319,0.852536040461079,0.848670332622165,0.841956789372187,0.844512815391848,0.84163488394314,0.833606324527615,0.835831958345349,0.839417960596678,0.836967272429525],[0.889597657171627,0.859766323557383,0.847731184821085,0.847644789060193,0.84951433633443,0.842165327897173,0.842689349986571,0.841376360872615,0.839364847715151,0.838908599355807,0.830277172771134],[0.889053873910343,0.859647839122393,0.849410492098883,0.846413993618855,0.843950716394448,0.844122909092841,0.837941293089318,0.838865972766641,0.836768979290939,0.831385784150213,0.830371274081287],[0.888598898911082,0.858994257806795,0.846487690194052,0.846585924762205,0.83925898331104,0.84357238046463,0.835136180629803,0.831174609425703,0.831556287065361,0.831316984849066,0.830604440078586],[0.888027628279568,0.859914766369467,0.847931205599569,0.837275996860954,0.839238516628837,0.836508133198641,0.829435942918943,0.826494625670199,0.833350530577485,0.827436120583648,0.828890655716155],[0.886969249235017,0.855712079271332,0.842340132704494,0.839275478486389,0.839058267346781,0.830989855202164,0.831397240949449,0.823111256306281,0.822973819455133,0.818206173461681,0.820721624034365],[0.885584556182319,0.857590863568442,0.832910302464593,0.834266265760601,0.833344236048857,0.830205861164958,0.821895937352226,0.820080125873503,0.817656653196995,0.813493268185754,0.813367301899896],[0.885166481093363,0.85449475735628,0.831161473167763,0.826981365841266,0.824833819970259,0.824466293835094,0.823459909180104,0.814958114093421,0.808485552518443,0.807671235303687,0.796459206849747]],\"x\":[1,3,5,6,8,10,11,13,15,17],\"y\":[1,290,581,871,1162,1452,1743,2033,2324,2614,2904],\"type\":\"surface\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\r5. 4번의 결과에서 예측정확도가 가장 높은 최적의 hyper-parameter 조합은 무엇인지 밝히시오.\rauc_matrix == max(auc_matrix)\r## 1 290 581 871 1162 1452 1743 2033 2324 2614 2904\r## 1 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 3 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 5 TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 6 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 8 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 10 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 11 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 13 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 15 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 17 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r즉 mtry가 3이고, node size 가 1일 떄 가장 예측정확도가 높은 조합이다.\n\r","description":"","tags":["data mining","random forest","decision tree"],"title":"데이터마이닝 - Random Forest","uri":"/posts/data_mining/3/"},{"categories":null,"content":" 해당 블로그는 blogdown package를 활용하여 R로 제작되었습니다. 2020학년도 1학기 데이터사이언스 입문 수업에서 배운 내용을 바탕으로 데이터 사이언스를 위한 블로그입니다.\n작성자에 대한 정보가 궁금하면 다음의 resume를 참고해 주시기 바랍니다.\n","description":"","tags":null,"title":"About me","uri":"/about/"},{"categories":["R","data mining"],"content":"\r\r\r\r\r\r\r데이터마이닝 연습으로 Decision Tree 모델을 사용한 예제이다.\n먼저 사용한 페키지이다.\nlibrary(tidyverse)\r## -- Attaching packages --------------------------------------------------------------- tidyverse 1.3.0 --\r## √ ggplot2 3.3.1 √ purrr 0.3.4\r## √ tibble 3.0.1 √ dplyr 1.0.0\r## √ tidyr 1.1.0 √ stringr 1.4.0\r## √ readr 1.3.1 √ forcats 0.5.0\r## -- Conflicts ------------------------------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(rpart)\rlibrary(rpart.plot)\rlibrary(pROC)\r## Type 'citation(\"pROC\")' for a citation.\r## ## Attaching package: 'pROC'\r## The following objects are masked from 'package:stats':\r## ## cov, smooth, var\rlibrary(plotly)\r## ## Attaching package: 'plotly'\r## The following object is masked from 'package:ggplot2':\r## ## last_plot\r## The following object is masked from 'package:stats':\r## ## filter\r## The following object is masked from 'package:graphics':\r## ## layout\r다음으로 우리가 사용할 데이터이다. 데이터는 https://www.openml.org/d/29 에서 가져왔다.\nraw_hw_tb = read_csv(\"data/dataset_29_credit-a.csv\",na=\"?\")\r## Parsed with column specification:\r## cols(\r## A1 = col_character(),\r## A2 = col_double(),\r## A3 = col_double(),\r## A4 = col_character(),\r## A5 = col_character(),\r## A6 = col_character(),\r## A7 = col_character(),\r## A8 = col_double(),\r## A9 = col_logical(),\r## A10 = col_logical(),\r## A11 = col_character(),\r## A12 = col_logical(),\r## A13 = col_character(),\r## A14 = col_character(),\r## A15 = col_double(),\r## class = col_character()\r## )\r해당자료는 Credit Approval로 credit card applications와 관련된 자료이다. Ross Quinlan의 자료로, 1987년 UCI에 공개된 자료이다. 모든 자료는 보안을 위해 다른 단어로 변경되었다. 정확히 각각의 변수가 의미하는 바는 알 수 없느나 홈페이지를 통해 각 변수들의 특성을 파악할 수 있다. 먼저 A1, A4, A5, A6, A7, A9, A10, A12, A13는 명목형 자료이다. 대부분은 2개의 범주로 구성되어 있으나, A6과 같은경우 15개의 범주로 구성되어 있다. 반면 A2, A3, A8, A11, A14, A15는 숫자형 범주이다. read_csv에서 보면 그 특성과 다르게 가져왔기에 이에 맞게 변수를 변환하였다. 또한 중간중간 NA가 있으나 나무모형을 사용하기에 그대로 사용하였다.\nhw_tb = raw_hw_tb %\u003e% mutate(\rA1=as.factor(A1), A4=as.factor(A4), A5=as.factor(A5), A6=as.factor(A6), A7=as.factor(A7),\rA9=as.factor(A9), A10=as.factor(A10), A12=as.factor(A12), A13=as.factor(A13),\rA2=as.numeric(A2), A3=as.numeric(A3), A8=as.numeric(A8), A11=as.numeric(A11),\rA14=as.numeric(A14), A15=as.numeric(A15),\rclass=as.factor(class)\r)\r자료의 구조와 총 개수를 보면 다음과 같다.\nstr(hw_tb)\r## tibble [690 x 16] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r## $ A1 : Factor w/ 2 levels \"a\",\"b\": 2 1 1 2 2 2 2 1 2 2 ...\r## $ A2 : num [1:690] 30.8 58.7 24.5 27.8 20.2 ...\r## $ A3 : num [1:690] 0 4.46 0.5 1.54 5.62 ...\r## $ A4 : Factor w/ 3 levels \"l\",\"u\",\"y\": 2 2 2 2 2 2 2 2 3 3 ...\r## $ A5 : Factor w/ 3 levels \"g\",\"gg\",\"p\": 1 1 1 1 1 1 1 1 3 3 ...\r## $ A6 : Factor w/ 14 levels \"aa\",\"c\",\"cc\",..: 13 11 11 13 13 10 12 3 9 13 ...\r## $ A7 : Factor w/ 9 levels \"bb\",\"dd\",\"ff\",..: 8 4 4 8 8 8 4 8 4 8 ...\r## $ A8 : num [1:690] 1.25 3.04 1.5 3.75 1.71 ...\r## $ A9 : Factor w/ 2 levels \"FALSE\",\"TRUE\": 2 2 2 2 2 2 2 2 2 2 ...\r## $ A10 : Factor w/ 2 levels \"FALSE\",\"TRUE\": 2 2 1 2 1 1 1 1 1 1 ...\r## $ A11 : num [1:690] 1 6 0 5 0 0 0 0 0 0 ...\r## $ A12 : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 2 1 2 2 1 1 2 ...\r## $ A13 : Factor w/ 3 levels \"g\",\"p\",\"s\": 1 1 1 1 3 1 1 1 1 1 ...\r## $ A14 : num [1:690] 202 43 280 100 120 360 164 80 180 52 ...\r## $ A15 : num [1:690] 0 560 824 3 0 ...\r## $ class: Factor w/ 2 levels \"-\",\"+\": 2 2 2 2 2 2 2 2 2 2 ...\r## - attr(*, \"spec\")=\r## .. cols(\r## .. A1 = col_character(),\r## .. A2 = col_double(),\r## .. A3 = col_double(),\r## .. A4 = col_character(),\r## .. A5 = col_character(),\r## .. A6 = col_character(),\r## .. A7 = col_character(),\r## .. A8 = col_double(),\r## .. A9 = col_logical(),\r## .. A10 = col_logical(),\r## .. A11 = col_character(),\r## .. A12 = col_logical(),\r## .. A13 = col_character(),\r## .. A14 = col_character(),\r## .. A15 = col_double(),\r## .. class = col_character()\r## .. )\rnrow(hw_tb)\r## [1] 690\r#1. Training 데이터와 Test 데이터를 50:50의 비율로 분할하시오.\nset.seed(1234)\rn = sample(1:nrow(hw_tb), round(nrow(hw_tb)/2)) train = hw_tb[n,] test = hw_tb[-n,]\rc(nrow(train), nrow(test))\r## [1] 345 345\r#2. R 프로그램의 ‘rpart’ 명령어를 사용하여 의사결정나무를 수행하고자 한다. 단, hyper-parameter는 아래와 같이 조정한다.\n\rA. minsplit = 1 ~ 46 (5의 간격으로)\n\rB. cp = 0.001 ~ 0.01 (0.001의 간격으로)\n\rC. xval = 0 으로 고정 (pruning 없음)\n\rD. 그외 parameter 값들은 default 값을 사용\n\r\r3. 위 2번의 조건에 맞는 의사결정나무를 training 데이터를 이용하여 생성하고, test 데이터를 이용하여 예측 정확도를 계산하고자 한다. 이때 예측정확도는 AUROC 값을 사용한다. 그 결과, 총 110개의 AUROC 값을 구할 수 있다. 이를 minsplit과 cp 값의 조합에 따라 AUROC 값으로 3차원 포물선 그래프(3D surface plot)를 생성하시오.\rauc = matrix(NA,nrow=10,ncol=10)\rfor(i1 in 1:10){\rfor(i2 in 1:10){\rtree_control = rpart.control(minsplit = 5*i1-4, cp =0.001*i2, xval=0)\rtemp_tree = assign(paste('tree',5*i1-4,i2,sep='_'), rpart(class ~ . , data=train, method='class', control=tree_control) )\rtemp_prob = assign(paste('prob',5*i1-4,i2,sep='_'), predict(temp_tree, newdata=test, type=\"prob\") )\rtemp_roc = assign(paste('roc',5*i1-4,i2,sep='_'), roc(test$class ~ temp_prob[,2]) )\rauc[i1,i2]=temp_roc$auc\r}\r}\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r## Setting levels: control = -, case = +\r## Setting direction: controls \u003c cases\r코드 실행 결과는 아래와 같은 객체를 통해 확인할 수 있다. 자세한 내용은 생략한다.\n\r각각의 모형 : tree_(minsplit값)_(cp값*1000)\n\r각각의 예측 : prob_(minsplit값)_(cp값*1000)\n\r각각의 roc(4번) : roc_(minsplit값)_(cp값*1000)\n\r\r해당 auroc를 이용하여 3d surface plot을 그리면 아래와 같다.\ncp = 0.001*1:10\rminsplit = 1:10*5-4\rrownames(auc)=cp\rcolnames(auc)=minsplit\rplot_ly(z = auc,x=cp,y=minsplit) %\u003e%\radd_surface() %\u003e%\rlayout(scene = list(\rxaxis = list(title = 'cp',tickvals=cp),\ryaxis = list(title = 'minsplit',tickvals=minsplit),\rzaxis = list(title = 'auc'))\r)\r\r{\"x\":{\"visdat\":{\"1eac4dcb311e\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"1eac4dcb311e\",\"attrs\":{\"1eac4dcb311e\":{\"z\":[[0.834699453551913,0.834699453551913,0.834429602644539,0.805538689873845,0.873372461714903,0.873372461714903,0.904118599473791,0.904118599473791,0.904118599473791,0.904118599473791],[0.88477366255144,0.88477366255144,0.88477366255144,0.88477366255144,0.88477366255144,0.88477366255144,0.897473520879714,0.897473520879714,0.897473520879714,0.897473520879714],[0.90627740673278,0.90627740673278,0.90627740673278,0.90627740673278,0.90627740673278,0.90627740673278,0.899986507454631,0.899986507454631,0.899986507454631,0.899986507454631],[0.920343385279633,0.920343385279633,0.920343385279633,0.920343385279633,0.920343385279633,0.920343385279633,0.916261890305606,0.916261890305606,0.916261890305606,0.916261890305606],[0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003],[0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228]],\"x\":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01],\"y\":[1,6,11,16,21,26,31,36,41,46],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"surface\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"cp\",\"tickvals\":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01]},\"yaxis\":{\"title\":\"minsplit\",\"tickvals\":[1,6,11,16,21,26,31,36,41,46]},\"zaxis\":{\"title\":\"auc\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666664\",\"rgba(70,19,97,1)\"],[\"0.0833333333333337\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666666\",\"rgba(199,225,42,1)\"],[\"0.958333333333334\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"z\":[[0.834699453551913,0.834699453551913,0.834429602644539,0.805538689873845,0.873372461714903,0.873372461714903,0.904118599473791,0.904118599473791,0.904118599473791,0.904118599473791],[0.88477366255144,0.88477366255144,0.88477366255144,0.88477366255144,0.88477366255144,0.88477366255144,0.897473520879714,0.897473520879714,0.897473520879714,0.897473520879714],[0.90627740673278,0.90627740673278,0.90627740673278,0.90627740673278,0.90627740673278,0.90627740673278,0.899986507454631,0.899986507454631,0.899986507454631,0.899986507454631],[0.920343385279633,0.920343385279633,0.920343385279633,0.920343385279633,0.920343385279633,0.920343385279633,0.916261890305606,0.916261890305606,0.916261890305606,0.916261890305606],[0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003,0.915013829859003],[0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654,0.916025770761654],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228],[0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228,0.91435606827228]],\"x\":[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01],\"y\":[1,6,11,16,21,26,31,36,41,46],\"type\":\"surface\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r그래프는 ploty를 사용하였기에 html에서 종합적으로 볼 수 있다.\n#4. 위의 결과에서 예측정확도가 가장 높은 최적의 hyper-parameter 조합은 무엇인지 밝히시오.\n먼저 auc 행렬을 이용해 최댓값을 찾으면 다음의 위치이다.\nauc == max(auc)\r## 1 6 11 16 21 26 31 36 41 46\r## 0.001 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.002 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.003 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.004 TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE\r## 0.005 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.006 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.007 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.008 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.009 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r## 0.01 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r즉 cp가 0.005이고 minsplit이 1, 6, 11, 16, 21일때 가장 좋은 auroc를 가진다. 즉 해당 조합이 최적의 hyper-parameter이다.\n\r","description":"","tags":["data mining","decision tree"],"title":"데이터마이닝 - Decision Tree","uri":"/posts/data_mining/2/"},{"categories":["R","data mining"],"content":"\r데이터마이닝 연습으로 로지스틱 회귀분석을 사용한 예제이다.\n자료 불러오기\rlibrary(readxl)\rairbnb \u003c- read_excel(\"data/airbnb.xlsx\")\rstr(airbnb)\r## tibble [74,111 x 29] (S3: tbl_df/tbl/data.frame)\r## $ id : num [1:74111] 6901257 6304928 7919400 13418779 3808709 ...\r## $ log_price : num [1:74111] 5.01 5.13 4.98 6.62 4.74 ...\r## $ property_type : chr [1:74111] \"Apartment\" \"Apartment\" \"Apartment\" \"House\" ...\r## $ room_type : chr [1:74111] \"Entire home/apt\" \"Entire home/apt\" \"Entire home/apt\" \"Entire home/apt\" ...\r## $ amenities : chr [1:74111] \"{\\\"Wireless Internet\\\",\\\"Air conditioning\\\",Kitchen,Heating,\\\"Family/kid friendly\\\",Essentials,\\\"Hair dryer\\\",I\"| __truncated__ \"{\\\"Wireless Internet\\\",\\\"Air conditioning\\\",Kitchen,Heating,\\\"Family/kid friendly\\\",Washer,Dryer,\\\"Smoke detect\"| __truncated__ \"{TV,\\\"Cable TV\\\",\\\"Wireless Internet\\\",\\\"Air conditioning\\\",Kitchen,Breakfast,\\\"Buzzer/wireless intercom\\\",Heat\"| __truncated__ \"{TV,\\\"Cable TV\\\",Internet,\\\"Wireless Internet\\\",Kitchen,\\\"Indoor fireplace\\\",\\\"Buzzer/wireless intercom\\\",Heati\"| __truncated__ ...\r## $ accommodates : num [1:74111] 3 7 5 4 2 2 3 2 2 2 ...\r## $ bathrooms : num [1:74111] 1 1 1 1 1 1 1 1 1 1 ...\r## $ bed_type : chr [1:74111] \"Real Bed\" \"Real Bed\" \"Real Bed\" \"Real Bed\" ...\r## $ cancellation_policy : chr [1:74111] \"strict\" \"strict\" \"moderate\" \"flexible\" ...\r## $ cleaning_fee : logi [1:74111] TRUE TRUE TRUE TRUE TRUE TRUE ...\r## $ city : chr [1:74111] \"NYC\" \"NYC\" \"NYC\" \"SF\" ...\r## $ description : chr [1:74111] \"Beautiful, sunlit brownstone 1-bedroom in the loveliest neighborhood in Brooklyn. Blocks from the promenade and\"| __truncated__ \"Enjoy travelling during your stay in Manhattan. My place is centrally located near Times Square and Central Par\"| __truncated__ \"The Oasis comes complete with a full backyard with outdoor furniture to make the most of this summer vacation!!\"| __truncated__ \"This light-filled home-away-from-home is super clean and comes with all of the modern amenities travelers could\"| __truncated__ ...\r## $ first_review : POSIXct[1:74111], format: \"2018-06-16\" \"2005-08-17\" ...\r## $ host_has_profile_pic : chr [1:74111] \"t\" \"t\" \"t\" \"t\" ...\r## $ host_identity_verified: chr [1:74111] \"t\" \"f\" \"t\" \"t\" ...\r## $ host_response_rate : num [1:74111] NA 1 1 NA 1 1 1 1 1 1 ...\r## $ host_since : POSIXct[1:74111], format: \"2026-03-12\" \"2019-06-17\" ...\r## $ instant_bookable : chr [1:74111] \"f\" \"t\" \"t\" \"f\" ...\r## $ last_review : POSIXct[1:74111], format: \"2018-07-16\" \"2023-09-17\" ...\r## $ latitude : num [1:74111] 40.7 40.8 40.8 37.8 38.9 ...\r## $ longitude : num [1:74111] -74 -74 -73.9 -122.4 -77 ...\r## $ name : chr [1:74111] \"Beautiful brownstone 1-bedroom\" \"Superb 3BR Apt Located Near Times Square\" \"The Garden Oasis\" \"Beautiful Flat in the Heart of SF!\" ...\r## $ neighbourhood : chr [1:74111] \"Brooklyn Heights\" \"Hell's Kitchen\" \"Harlem\" \"Lower Haight\" ...\r## $ number_of_reviews : num [1:74111] 2 6 10 0 4 3 15 9 159 2 ...\r## $ review_scores_rating : num [1:74111] 100 93 92 NA 40 100 97 93 99 90 ...\r## $ thumbnail_url : chr [1:74111] \"https://a0.muscache.com/im/pictures/6d7cbbf7-c034-459c-bc82-6522c957627c.jpg?aki_policy=small\" \"https://a0.muscache.com/im/pictures/348a55fe-4b65-452a-b48a-bfecb3b58a66.jpg?aki_policy=small\" \"https://a0.muscache.com/im/pictures/6fae5362-9e3a-4fa9-aa54-bbd5ea26538d.jpg?aki_policy=small\" \"https://a0.muscache.com/im/pictures/72208dad-9c86-41ea-a735-43d933111063.jpg?aki_policy=small\" ...\r## $ zipcode : chr [1:74111] \"11201\" \"10019\" \"10027\" \"94117\" ...\r## $ bedrooms : num [1:74111] 1 3 1 2 0 1 1 1 1 1 ...\r## $ beds : num [1:74111] 1 3 3 2 1 1 1 1 1 1 ...\r\r데이터 전처리\r“property_type”은 ‘House’, ‘Apartment’, ’Other’ 등의 3범주로 변환하시오.\rattach(airbnb)\rairbnb$property_type[!(property_type=='House'| property_type=='Apartment')] = \"Other\"\rairbnb$property_type \u003c- as.factor(airbnb$property_type)\rlevels(airbnb$property_type)\r## [1] \"Apartment\" \"House\" \"Other\"\r\r“bed_type”은 ‘Bed’, ‘Other’ 등의 2범주로 변환하시오.\rReal Bed와 Airbed 는 Bed로, 나머지는 Other로 변환하였다.\nairbnb$bed_type[(bed_type=='Airbed'| bed_type=='Real Bed')] = \"Bed\"\rairbnb$bed_type[!(bed_type=='Airbed'| bed_type=='Real Bed')] = \"Other\"\rairbnb$bed_type \u003c- as.factor(airbnb$bed_type)\rlevels(airbnb$bed_type)\r## [1] \"Bed\" \"Other\"\rdetach(airbnb)\r\r“number_of_reviews”가 11개 이상인 데이터만 추출해서 분석에 사용하시오.\rairbnb \u003c- airbnb[airbnb$number_of_reviews\u003e=11 , 1:ncol(airbnb)]\r\r‘가격비(price_ratio)’ 변수를 생성하시오.\rairbnb$log_price \u003c- exp(airbnb$log_price)\rmean_price \u003c- aggregate(log_price ~ city, airbnb, mean)\rnames(mean_price) \u003c-c (\"city\",\"mean_price\")\rf_airbnb \u003c- merge(x =airbnb,y=mean_price,by=\"city\",all.x=T)\rf_airbnb$price_ratio \u003c- f_airbnb$log_price/f_airbnb$mean_price*100\r\r\r데이터 분석\r1. “가격비(price_ratio)” 변수의 평균과 표준편차를 답하시오.\rmean(f_airbnb$price_ratio)\r## [1] 100\rsd(f_airbnb$price_ratio)\r## [1] 83.72338\r\r2. “가격비(price_ratio)”를 종속변수로 하여 선형회귀분석을 수행하시오.\r먼저 시행에 앞서 불필요한 변수를 제거한다. 제거한 것들은 아래와 같다. amenities나 description의 경우 길이로 변환하여 분석하였으나 큰 의미가 없어 제거하고 분석하였다. (y변수 계산에 필요했던 것들 및 url이나 설명같이 의미없는 것들을 제외한다.)\n\rdescription\n\ramenities\n\rname\n\rneighbourhood\n\rthumbnail_url\n\rid\n\rlog_price\n\rmean_price\n\r\rf_df \u003c- subset(f_airbnb, select = -c(description, amenities, name, neighbourhood, thumbnail_url, id, log_price, mean_price))\rcharacter를 factor로 변환한다. 순서가 있는 cancellation_policy의 경우 순서를 준다.\nfind_char_col\u003c-NA\rfor (i in 1:ncol(f_df)){\rfind_char_col[i] \u003c- is.character(f_df[,i])\r}\rchar_col \u003c- colnames(f_df)[find_char_col]\rf_df[char_col] \u003c- lapply(f_df[char_col] , factor)\rf_df$cleaning_fee \u003c- as.factor(f_df$cleaning_fee)\rcancellation_policy에서도 “super_strict_60”는 2개뿐이므로“super_strict_30”에 포함시켰다. (즉 super_strict_30는 super_strict_30+를 의미한다.)\nwhich(f_df$cancellation_policy == \"super_strict_60\")\r## [1] 6558 13642\rf_df$cancellation_policy[which(f_df$cancellation_policy == \"super_strict_60\")] \u003c- \"super_strict_30\"\rf_df$cancellation_policy \u003c- factor(f_df$cancellation_policy,levels=c(\"flexible\", \"moderate\", \"strict\", \"super_strict_30\"), order=T)\r먼저 각각의 plot을 그려본다. (결과는 생략 eval=FALSE)\nfor (i in 1:(ncol(f_df)-1)) {\rplot(f_df$price_ratio~f_df[[i]],xlab=colnames(f_df)[i])\r}\r대략적으로 눈으로 보기에 차이가 나타나는 것은 다음과 같다. (진한 것은 더욱 두드러 진 것)\n\rroom_type\raccommodates\rbathrooms\rbed_type\rcancellation_policy\rcleaning_fee\rhost_has_profile_pic\rhost_identity_verified\rhost_response_rate\rinstant_bookable\rlatitude\rlongitude\rnumber_of_reviews\rreview_scores_rating\rbedrooms\rbeds\r\r그런데 latitude와 longitude는 사실상 zipcode에 그 데이터에 의미가 어느정도 포함된다고 판단하여 제거하였다.\n또한 zipcode는 너무 자세하므로 분석의 편의상 앞의 2자리만 사용하였다. (city는 이 zipcode에 포함되므로 생략하였다.)\n그리고 host_has_profile_pic의 경우 f는 28개뿐으로 매우 적으므로 변수를 사용하지 않는다.\nsummary( lm( price_ratio ~ latitude, data=f_df))\r## ## Call:\r## lm(formula = price_ratio ~ latitude, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -92.71 -47.41 -22.56 19.19 1323.76 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 9.996e+01 6.061e+00 16.492 \u003c2e-16 ***\r## latitude 9.344e-04 1.574e-01 0.006 0.995 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 83.72 on 29014 degrees of freedom\r## Multiple R-squared: 1.215e-09, Adjusted R-squared: -3.446e-05 ## F-statistic: 3.525e-05 on 1 and 29014 DF, p-value: 0.9953\rsummary( lm( price_ratio ~ longitude, data=f_df))\r## ## Call:\r## lm(formula = price_ratio ~ longitude, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -92.67 -47.36 -22.63 19.12 1323.69 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 99.765008 2.164951 46.082 \u003c2e-16 ***\r## longitude -0.002513 0.022547 -0.111 0.911 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 83.72 on 29014 degrees of freedom\r## Multiple R-squared: 4.281e-07, Adjusted R-squared: -3.404e-05 ## F-statistic: 0.01242 on 1 and 29014 DF, p-value: 0.9113\rf_df$zipcode \u003c- as.factor(substr(f_airbnb$zipcode,1,2))\rtable(f_df$host_has_profile_pic)\r## ## f t ## 28 28924\r그래서 이를 제외한 다음을 분석변수로 한다.\n\rroom_type\rproperty_type\raccommodates\rbathrooms\rbed_type\rcancellation_policy\rcleaning_fee\rhost_identity_verified\rhost_response_rate\rinstant_bookable\rfirst_review\rnumber_of_reviews\rhost_since\rreview_scores_rating\rlast_review\rbedrooms\n\rbeds\rzipcode\r\r정확히 하기 위해 trellis plot을 그려보았다.\nlibrary(lattice)\rmypanel \u003c- function(x, y) {\rpanel.xyplot(x, y)\rpanel.loess(x, y, col=\"red\", lwd=2, lty=2) }\rxyplot(price_ratio ~ room_type, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ accommodates, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ bathrooms, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ bed_type, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ cancellation_policy, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ cleaning_fee, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ host_identity_verified , data=f_df,panel=mypanel)\rxyplot(price_ratio ~ host_response_rate, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ instant_bookable , data=f_df,panel=mypanel)\rxyplot(price_ratio ~ number_of_reviews, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ review_scores_rating, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ bedrooms, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ beds, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ zipcode, data=f_df,panel=mypanel)\rxyplot(price_ratio ~ last_review , data=f_df,panel=mypanel)\rxyplot(price_ratio ~ host_since , data=f_df,panel=mypanel)\rxyplot(price_ratio ~ first_review , data=f_df,panel=mypanel)\rxyplot(price_ratio ~ property_type , data=f_df,panel=mypanel)\r이제 18개의 값들을 이용해 선형회귀분석을 실시한다. 그전에 결측치NA가 포함된 행은 제거하고 실시한다.\nf_df = na.omit(f_df)\rlm1 = lm(price_ratio ~ room_type + property_type + accommodates + bathrooms + bed_type + cancellation_policy + cleaning_fee + host_identity_verified + host_response_rate + instant_bookable + first_review + number_of_reviews + host_since + review_scores_rating + last_review + bedrooms + beds + zipcode , data = f_df)\rsummary(lm1)\r## ## Call:\r## lm(formula = price_ratio ~ room_type + property_type + accommodates + ## bathrooms + bed_type + cancellation_policy + cleaning_fee + ## host_identity_verified + host_response_rate + instant_bookable + ## first_review + number_of_reviews + host_since + review_scores_rating + ## last_review + bedrooms + beds + zipcode, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -310.42 -25.81 -2.43 18.11 995.55 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) -9.898e+01 8.209e+00 -12.058 \u003c 2e-16 ***\r## room_typePrivate room -4.603e+01 8.669e-01 -53.089 \u003c 2e-16 ***\r## room_typeShared room -7.641e+01 2.538e+00 -30.105 \u003c 2e-16 ***\r## property_typeHouse 4.183e+00 9.150e-01 4.572 4.85e-06 ***\r## property_typeOther 6.771e+00 1.112e+00 6.087 1.17e-09 ***\r## accommodates 8.450e+00 3.292e-01 25.671 \u003c 2e-16 ***\r## bathrooms 3.515e+01 7.626e-01 46.087 \u003c 2e-16 ***\r## bed_typeOther 1.908e+00 2.279e+00 0.837 0.402372 ## cancellation_policy.L 5.800e+01 6.962e+00 8.330 \u003c 2e-16 ***\r## cancellation_policy.Q 3.702e+01 5.201e+00 7.117 1.13e-12 ***\r## cancellation_policy.C 1.434e+01 2.375e+00 6.039 1.57e-09 ***\r## cleaning_feeTRUE -3.377e+00 1.006e+00 -3.357 0.000789 ***\r## host_identity_verifiedt 2.158e+00 8.533e-01 2.529 0.011431 * ## host_response_rate -1.935e+01 3.508e+00 -5.516 3.50e-08 ***\r## instant_bookablet -4.545e+00 7.645e-01 -5.944 2.81e-09 ***\r## first_review -3.548e-11 5.496e-10 -0.065 0.948532 ## number_of_reviews -4.118e-02 7.218e-03 -5.705 1.17e-08 ***\r## host_since 1.791e-10 5.406e-10 0.331 0.740412 ## review_scores_rating 2.022e+00 7.722e-02 26.187 \u003c 2e-16 ***\r## last_review 7.622e-10 4.587e-10 1.662 0.096593 . ## bedrooms 2.588e+01 6.572e-01 39.373 \u003c 2e-16 ***\r## beds -3.614e+00 4.786e-01 -7.551 4.46e-14 ***\r## zipcode11 -4.252e+01 1.112e+00 -38.245 \u003c 2e-16 ***\r## zipcode1m -3.478e+01 5.632e+01 -0.617 0.536912 ## zipcode20 -4.165e+01 1.547e+00 -26.924 \u003c 2e-16 ***\r## zipcode21 -3.428e+01 1.739e+00 -19.714 \u003c 2e-16 ***\r## zipcode22 -1.917e+01 8.438e+00 -2.272 0.023073 * ## zipcode24 -4.208e+01 5.633e+01 -0.747 0.454991 ## zipcode60 -4.246e+01 1.562e+00 -27.184 \u003c 2e-16 ***\r## zipcode90 -3.486e+01 1.107e+00 -31.483 \u003c 2e-16 ***\r## zipcode91 -5.649e+01 1.742e+00 -32.428 \u003c 2e-16 ***\r## zipcode92 -6.254e+01 5.634e+01 -1.110 0.266986 ## zipcode93 -7.250e+01 1.567e+01 -4.628 3.71e-06 ***\r## zipcode94 -3.287e+01 1.418e+00 -23.189 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 56.31 on 27002 degrees of freedom\r## Multiple R-squared: 0.5462, Adjusted R-squared: 0.5456 ## F-statistic: 984.8 on 33 and 27002 DF, p-value: \u003c 2.2e-16\r위를 보면 유의미하지 않은 p-value가 많다. 이제 stepwise를 해서 제거한다.\nlm2 = step(lm1, direction = 'both')\r## Start: AIC=217990.5\r## price_ratio ~ room_type + property_type + accommodates + bathrooms + ## bed_type + cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + first_review + number_of_reviews + ## host_since + review_scores_rating + last_review + bedrooms + ## beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## - first_review 1 13 85615864 217989\r## - host_since 1 348 85616199 217989\r## - bed_type 1 2223 85618074 217989\r## \u003cnone\u003e 85615851 217990\r## - last_review 1 8755 85624606 217991\r## - host_identity_verified 1 20286 85636137 217995\r## - cleaning_fee 1 35736 85651586 218000\r## - host_response_rate 1 96468 85712319 218019\r## - number_of_reviews 1 103211 85719062 218021\r## - instant_bookable 1 112043 85727894 218024\r## - property_type 2 144262 85760113 218032\r## - beds 1 180785 85796636 218046\r## - cancellation_policy 3 503380 86119231 218143\r## - accommodates 1 2089553 87705404 218640\r## - review_scores_rating 1 2174363 87790214 218667\r## - bedrooms 1 4915304 90531154 219498\r## - zipcode 12 6500593 92116444 219945\r## - bathrooms 1 6734757 92350607 220036\r## - room_type 2 10116125 95731976 221006\r## ## Step: AIC=217988.5\r## price_ratio ~ room_type + property_type + accommodates + bathrooms + ## bed_type + cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## host_since + review_scores_rating + last_review + bedrooms + ## beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## - host_since 1 349 85616213 217987\r## - bed_type 1 2224 85618088 217987\r## \u003cnone\u003e 85615864 217989\r## - last_review 1 8763 85624627 217989\r## + first_review 1 13 85615851 217990\r## - host_identity_verified 1 20283 85636147 217993\r## - cleaning_fee 1 35747 85651611 217998\r## - host_response_rate 1 96497 85712361 218017\r## - number_of_reviews 1 103228 85719092 218019\r## - instant_bookable 1 112052 85727916 218022\r## - property_type 2 144250 85760114 218030\r## - beds 1 180804 85796668 218044\r## - cancellation_policy 3 503371 86119235 218141\r## - accommodates 1 2089584 87705448 218638\r## - review_scores_rating 1 2174743 87790607 218665\r## - bedrooms 1 4915310 90531174 219496\r## - zipcode 12 6500955 92116819 219943\r## - bathrooms 1 6734768 92350632 220034\r## - room_type 2 10116180 95732044 221004\r## ## Step: AIC=217986.6\r## price_ratio ~ room_type + property_type + accommodates + bathrooms + ## bed_type + cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + last_review + bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## - bed_type 1 2238 85618451 217985\r## \u003cnone\u003e 85616213 217987\r## - last_review 1 8739 85624953 217987\r## + host_since 1 349 85615864 217989\r## + first_review 1 14 85616199 217989\r## - host_identity_verified 1 20254 85636467 217991\r## - cleaning_fee 1 35785 85651998 217996\r## - host_response_rate 1 96522 85712735 218015\r## - number_of_reviews 1 103088 85719301 218017\r## - instant_bookable 1 112180 85728393 218020\r## - property_type 2 144324 85760538 218028\r## - beds 1 180830 85797044 218042\r## - cancellation_policy 3 503161 86119374 218139\r## - accommodates 1 2089343 87705556 218636\r## - review_scores_rating 1 2174719 87790932 218663\r## - bedrooms 1 4916145 90532358 219494\r## - zipcode 12 6500763 92116976 219941\r## - bathrooms 1 6734710 92350923 220032\r## - room_type 2 10120958 95737171 221003\r## ## Step: AIC=217985.3\r## price_ratio ~ room_type + property_type + accommodates + bathrooms + ## cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + last_review + bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## \u003cnone\u003e 85618451 217985\r## - last_review 1 8687 85627138 217986\r## + bed_type 1 2238 85616213 217987\r## + host_since 1 363 85618088 217987\r## + first_review 1 15 85618435 217987\r## - host_identity_verified 1 20365 85638815 217990\r## - cleaning_fee 1 35751 85654202 217995\r## - host_response_rate 1 96483 85714934 218014\r## - number_of_reviews 1 102833 85721284 218016\r## - instant_bookable 1 113036 85731487 218019\r## - property_type 2 144046 85762497 218027\r## - beds 1 182147 85800598 218041\r## - cancellation_policy 3 501866 86120317 218137\r## - accommodates 1 2090219 87708670 218635\r## - review_scores_rating 1 2175653 87794104 218662\r## - bedrooms 1 4917159 90535610 219493\r## - zipcode 12 6501272 92119723 219940\r## - bathrooms 1 6732473 92350924 220030\r## - room_type 2 10195031 95813482 221023\rsummary(lm2)\r## ## Call:\r## lm(formula = price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + last_review + bedrooms + beds + zipcode, ## data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -310.29 -25.79 -2.43 18.08 995.53 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) -9.878e+01 8.141e+00 -12.134 \u003c 2e-16 ***\r## room_typePrivate room -4.600e+01 8.659e-01 -53.122 \u003c 2e-16 ***\r## room_typeShared room -7.604e+01 2.499e+00 -30.431 \u003c 2e-16 ***\r## property_typeHouse 4.171e+00 9.148e-01 4.560 5.13e-06 ***\r## property_typeOther 6.772e+00 1.112e+00 6.088 1.16e-09 ***\r## accommodates 8.451e+00 3.291e-01 25.676 \u003c 2e-16 ***\r## bathrooms 3.513e+01 7.624e-01 46.081 \u003c 2e-16 ***\r## cancellation_policy.L 5.795e+01 6.961e+00 8.324 \u003c 2e-16 ***\r## cancellation_policy.Q 3.698e+01 5.200e+00 7.112 1.17e-12 ***\r## cancellation_policy.C 1.434e+01 2.374e+00 6.039 1.57e-09 ***\r## cleaning_feeTRUE -3.377e+00 1.006e+00 -3.358 0.000786 ***\r## host_identity_verifiedt 2.162e+00 8.532e-01 2.534 0.011269 * ## host_response_rate -1.935e+01 3.507e+00 -5.516 3.49e-08 ***\r## instant_bookablet -4.563e+00 7.642e-01 -5.971 2.39e-09 ***\r## number_of_reviews -4.110e-02 7.216e-03 -5.695 1.25e-08 ***\r## review_scores_rating 2.023e+00 7.721e-02 26.196 \u003c 2e-16 ***\r## last_review 7.592e-10 4.587e-10 1.655 0.097875 . ## bedrooms 2.588e+01 6.572e-01 39.382 \u003c 2e-16 ***\r## beds -3.626e+00 4.784e-01 -7.580 3.58e-14 ***\r## zipcode11 -4.252e+01 1.112e+00 -38.246 \u003c 2e-16 ***\r## zipcode1m -3.483e+01 5.632e+01 -0.618 0.536327 ## zipcode20 -4.165e+01 1.547e+00 -26.930 \u003c 2e-16 ***\r## zipcode21 -3.428e+01 1.739e+00 -19.714 \u003c 2e-16 ***\r## zipcode22 -1.922e+01 8.438e+00 -2.278 0.022721 * ## zipcode24 -4.215e+01 5.632e+01 -0.748 0.454229 ## zipcode60 -4.248e+01 1.562e+00 -27.202 \u003c 2e-16 ***\r## zipcode90 -3.486e+01 1.107e+00 -31.487 \u003c 2e-16 ***\r## zipcode91 -5.649e+01 1.742e+00 -32.430 \u003c 2e-16 ***\r## zipcode92 -6.251e+01 5.634e+01 -1.110 0.267158 ## zipcode93 -7.254e+01 1.566e+01 -4.631 3.66e-06 ***\r## zipcode94 -3.287e+01 1.417e+00 -23.192 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 56.31 on 27005 degrees of freedom\r## Multiple R-squared: 0.5462, Adjusted R-squared: 0.5457 ## F-statistic: 1083 on 30 and 27005 DF, p-value: \u003c 2.2e-16\r이후 분석에서 p-value가 유의미하지 않은 last_review 를 제거하고 실시한다.\n\r\r회귀진단\rplot(lm2,which=1)\rplot(lm2,which=2)\r잔차에서 메가폰 형태가 나타남을 알수 있다. 또한 QQ플랏도 문제가 있음을 알 수 있다. Y변수의 변환의 필요성이 있으므로 Y’=logY로 변환하였다.\nf_df$log_price_ratio \u003c- log(f_df$price_ratio)\rlm3 = lm(log_price_ratio ~ room_type + property_type + accommodates + bathrooms + cancellation_policy + cleaning_fee + host_identity_verified + host_response_rate + instant_bookable + number_of_reviews + review_scores_rating + bedrooms + beds + zipcode, data = f_df)\rlm4 = step(lm3, direction = 'both')\r## Start: AIC=-54314.95\r## log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## - cleaning_fee 1 0.00 3618.2 -54317\r## - number_of_reviews 1 0.08 3618.2 -54316\r## \u003cnone\u003e 3618.2 -54315\r## - host_identity_verified 1 1.21 3619.4 -54308\r## - host_response_rate 1 4.82 3623.0 -54281\r## - instant_bookable 1 7.14 3625.3 -54264\r## - property_type 2 7.92 3626.1 -54260\r## - cancellation_policy 3 21.45 3639.6 -54161\r## - beds 1 21.10 3639.3 -54160\r## - bathrooms 1 79.59 3697.7 -53729\r## - accommodates 1 131.11 3749.3 -53355\r## - review_scores_rating 1 188.21 3806.4 -52946\r## - bedrooms 1 216.57 3834.7 -52745\r## - zipcode 12 462.42 4080.6 -51087\r## - room_type 2 1779.04 5397.2 -43507\r## ## Step: AIC=-54316.91\r## log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## - number_of_reviews 1 0.08 3618.2 -54318\r## \u003cnone\u003e 3618.2 -54317\r## + cleaning_fee 1 0.00 3618.2 -54315\r## - host_identity_verified 1 1.21 3619.4 -54310\r## - host_response_rate 1 4.84 3623.0 -54283\r## - instant_bookable 1 7.16 3625.3 -54265\r## - property_type 2 7.92 3626.1 -54262\r## - beds 1 21.11 3639.3 -54162\r## - cancellation_policy 3 21.72 3639.9 -54161\r## - bathrooms 1 79.62 3697.8 -53730\r## - accommodates 1 131.42 3749.6 -53354\r## - review_scores_rating 1 188.30 3806.5 -52947\r## - bedrooms 1 216.56 3834.7 -52747\r## - zipcode 12 462.51 4080.7 -51089\r## - room_type 2 1827.08 5445.2 -43269\r## ## Step: AIC=-54318.33\r## log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + review_scores_rating + ## bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## \u003cnone\u003e 3618.2 -54318\r## + number_of_reviews 1 0.08 3618.2 -54317\r## + cleaning_fee 1 0.00 3618.2 -54316\r## - host_identity_verified 1 1.16 3619.4 -54312\r## - host_response_rate 1 4.95 3623.2 -54283\r## - instant_bookable 1 7.27 3625.5 -54266\r## - property_type 2 7.97 3626.2 -54263\r## - cancellation_policy 3 21.66 3639.9 -54163\r## - beds 1 21.15 3639.4 -54163\r## - bathrooms 1 79.88 3698.1 -53730\r## - accommodates 1 131.34 3749.6 -53356\r## - review_scores_rating 1 188.57 3806.8 -52947\r## - bedrooms 1 218.14 3836.4 -52738\r## - zipcode 12 462.64 4080.9 -51089\r## - room_type 2 1828.54 5446.8 -43264\r그 결과는 아래와 같다.\nsummary(lm4)\r## ## Call:\r## lm(formula = log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + review_scores_rating + ## bedrooms + beds + zipcode, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.51557 -0.22551 -0.00287 0.22588 2.52586 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 2.8112298 0.0525988 53.447 \u003c 2e-16 ***\r## room_typePrivate room -0.5855993 0.0055699 -105.136 \u003c 2e-16 ***\r## room_typeShared room -1.1375240 0.0161312 -70.517 \u003c 2e-16 ***\r## property_typeHouse -0.0183360 0.0059407 -3.087 0.00203 ** ## property_typeOther 0.0422432 0.0072288 5.844 5.16e-09 ***\r## accommodates 0.0668736 0.0021358 31.311 \u003c 2e-16 ***\r## bathrooms 0.1209281 0.0049524 24.418 \u003c 2e-16 ***\r## cancellation_policy.L 0.2933262 0.0452494 6.482 9.18e-11 ***\r## cancellation_policy.Q 0.1616865 0.0337823 4.786 1.71e-06 ***\r## cancellation_policy.C 0.0664165 0.0154332 4.303 1.69e-05 ***\r## host_identity_verifiedt 0.0162382 0.0055087 2.948 0.00320 ** ## host_response_rate -0.1381528 0.0227264 -6.079 1.23e-09 ***\r## instant_bookablet -0.0365163 0.0049570 -7.367 1.80e-13 ***\r## review_scores_rating 0.0188152 0.0005015 37.518 \u003c 2e-16 ***\r## bedrooms 0.1719812 0.0042620 40.352 \u003c 2e-16 ***\r## beds -0.0390361 0.0031066 -12.566 \u003c 2e-16 ***\r## zipcode11 -0.3585074 0.0072259 -49.614 \u003c 2e-16 ***\r## zipcode1m -0.3950436 0.3661121 -1.079 0.28059 ## zipcode20 -0.2976181 0.0100374 -29.651 \u003c 2e-16 ***\r## zipcode21 -0.2358253 0.0112927 -20.883 \u003c 2e-16 ***\r## zipcode22 -0.1539659 0.0548451 -2.807 0.00500 ** ## zipcode24 -0.6528374 0.3661251 -1.783 0.07458 . ## zipcode60 -0.3329465 0.0101308 -32.865 \u003c 2e-16 ***\r## zipcode90 -0.2835116 0.0071819 -39.476 \u003c 2e-16 ***\r## zipcode91 -0.5014514 0.0112977 -44.385 \u003c 2e-16 ***\r## zipcode92 -0.8951543 0.3661830 -2.445 0.01451 * ## zipcode93 -0.6850067 0.1017839 -6.730 1.73e-11 ***\r## zipcode94 -0.2315125 0.0091763 -25.229 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.366 on 27008 degrees of freedom\r## Multiple R-squared: 0.6588, Adjusted R-squared: 0.6584 ## F-statistic: 1931 on 27 and 27008 DF, p-value: \u003c 2.2e-16\rplot(lm4,which=1)\rplot(lm4,which=2)\r## Warning: not plotting observations with leverage one:\r## 1413, 8437, 20636\r잔차의 패턴이 어느정도 감소함을 알 수 있다.\nsummary(lm3)\r## ## Call:\r## lm(formula = log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + bedrooms + beds + zipcode, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.51722 -0.22532 -0.00299 0.22586 2.52411 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 2.813e+00 5.270e-02 53.368 \u003c 2e-16 ***\r## room_typePrivate room -5.856e-01 5.629e-03 -104.034 \u003c 2e-16 ***\r## room_typeShared room -1.138e+00 1.624e-02 -70.065 \u003c 2e-16 ***\r## property_typeHouse -1.814e-02 5.946e-03 -3.051 0.00228 ** ## property_typeOther 4.224e-02 7.230e-03 5.842 5.21e-09 ***\r## accommodates 6.693e-02 2.140e-03 31.282 \u003c 2e-16 ***\r## bathrooms 1.208e-01 4.956e-03 24.374 \u003c 2e-16 ***\r## cancellation_policy.L 2.931e-01 4.525e-02 6.477 9.49e-11 ***\r## cancellation_policy.Q 1.610e-01 3.380e-02 4.764 1.91e-06 ***\r## cancellation_policy.C 6.631e-02 1.543e-02 4.296 1.74e-05 ***\r## cleaning_feeTRUE -1.157e-03 6.536e-03 -0.177 0.85946 ## host_identity_verifiedt 1.670e-02 5.546e-03 3.011 0.00261 ** ## host_response_rate -1.368e-01 2.280e-02 -5.998 2.02e-09 ***\r## instant_bookablet -3.627e-02 4.968e-03 -7.301 2.93e-13 ***\r## number_of_reviews -3.647e-05 4.691e-05 -0.778 0.43683 ## review_scores_rating 1.881e-02 5.018e-04 37.481 \u003c 2e-16 ***\r## bedrooms 1.718e-01 4.272e-03 40.205 \u003c 2e-16 ***\r## beds -3.903e-02 3.110e-03 -12.549 \u003c 2e-16 ***\r## zipcode11 -3.585e-01 7.226e-03 -49.616 \u003c 2e-16 ***\r## zipcode1m -3.950e-01 3.661e-01 -1.079 0.28064 ## zipcode20 -2.975e-01 1.004e-02 -29.637 \u003c 2e-16 ***\r## zipcode21 -2.355e-01 1.130e-02 -20.839 \u003c 2e-16 ***\r## zipcode22 -1.539e-01 5.485e-02 -2.805 0.00503 ** ## zipcode24 -6.537e-01 3.661e-01 -1.785 0.07423 . ## zipcode60 -3.330e-01 1.014e-02 -32.848 \u003c 2e-16 ***\r## zipcode90 -2.834e-01 7.186e-03 -39.434 \u003c 2e-16 ***\r## zipcode91 -5.019e-01 1.131e-02 -44.361 \u003c 2e-16 ***\r## zipcode92 -8.944e-01 3.662e-01 -2.442 0.01460 * ## zipcode93 -6.861e-01 1.018e-01 -6.738 1.64e-11 ***\r## zipcode94 -2.309e-01 9.213e-03 -25.062 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.366 on 27006 degrees of freedom\r## Multiple R-squared: 0.6588, Adjusted R-squared: 0.6584 ## F-statistic: 1798 on 29 and 27006 DF, p-value: \u003c 2.2e-16\r다만 p-value를 구해보면 유의미하지 않은 변수가 다시 나타나므로 stepwise를 실시해 본다. 그전에 number_of_reviews가 유의미 하지 않는 다고 나온다. 하지만 number_of_reviews는 다른 숫자자료에 비해 분산이 크다. 실제로 number_of_reviews와 비교해보면 10배 정도 차이가 난다. 이를 보정해 주기 위해 나눈값을 해봐도 결과는 동일하게 유의미 하지 않는다고 나온다.\nsd(f_df$review_scores_rating, na.rm=TRUE)\r## [1] 4.655876\rsd(f_df$number_of_reviews) \r## [1] 48.80326\rf_df$standard_number_of_reviews \u003c- f_df$number_of_reviews/sd(f_df$review_scores_rating, na.rm=TRUE)\rtemp_lm = lm(log_price_ratio ~ room_type + accommodates + bathrooms + cancellation_policy + cleaning_fee + host_response_rate + instant_bookable + standard_number_of_reviews + review_scores_rating + bedrooms + beds, data = f_df)\rsummary(temp_lm)\r## ## Call:\r## lm(formula = log_price_ratio ~ room_type + accommodates + bathrooms + ## cancellation_policy + cleaning_fee + host_response_rate + ## instant_bookable + standard_number_of_reviews + review_scores_rating + ## bedrooms + beds, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.59858 -0.25391 -0.01335 0.24299 2.54936 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 2.8699063 0.0556688 51.553 \u003c 2e-16 ***\r## room_typePrivate room -0.6022161 0.0057959 -103.904 \u003c 2e-16 ***\r## room_typeShared room -1.1298223 0.0173091 -65.273 \u003c 2e-16 ***\r## accommodates 0.0634022 0.0022744 27.876 \u003c 2e-16 ***\r## bathrooms 0.1074901 0.0052103 20.630 \u003c 2e-16 ***\r## cancellation_policy.L 0.3217257 0.0481716 6.679 2.46e-11 ***\r## cancellation_policy.Q 0.1491561 0.0359783 4.146 3.40e-05 ***\r## cancellation_policy.C 0.0559756 0.0164257 3.408 0.000656 ***\r## cleaning_feeTRUE 0.0046765 0.0069336 0.674 0.500018 ## host_response_rate -0.2032507 0.0242706 -8.374 \u003c 2e-16 ***\r## instant_bookablet -0.0380744 0.0052724 -7.221 5.28e-13 ***\r## standard_number_of_reviews 0.0000952 0.0002299 0.414 0.678769 ## review_scores_rating 0.0166253 0.0005285 31.456 \u003c 2e-16 ***\r## bedrooms 0.1667024 0.0045023 37.026 \u003c 2e-16 ***\r## beds -0.0384741 0.0033131 -11.613 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.3907 on 27021 degrees of freedom\r## Multiple R-squared: 0.6111, Adjusted R-squared: 0.6109 ## F-statistic: 3032 on 14 and 27021 DF, p-value: \u003c 2.2e-16\r이제 기존의 변수를 활용하여 stepwise를 실시해 본다\nlm4 = step(lm3, direction = 'both')\r## Start: AIC=-54314.95\r## log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + cleaning_fee + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## - cleaning_fee 1 0.00 3618.2 -54317\r## - number_of_reviews 1 0.08 3618.2 -54316\r## \u003cnone\u003e 3618.2 -54315\r## - host_identity_verified 1 1.21 3619.4 -54308\r## - host_response_rate 1 4.82 3623.0 -54281\r## - instant_bookable 1 7.14 3625.3 -54264\r## - property_type 2 7.92 3626.1 -54260\r## - cancellation_policy 3 21.45 3639.6 -54161\r## - beds 1 21.10 3639.3 -54160\r## - bathrooms 1 79.59 3697.7 -53729\r## - accommodates 1 131.11 3749.3 -53355\r## - review_scores_rating 1 188.21 3806.4 -52946\r## - bedrooms 1 216.57 3834.7 -52745\r## - zipcode 12 462.42 4080.6 -51087\r## - room_type 2 1779.04 5397.2 -43507\r## ## Step: AIC=-54316.91\r## log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + number_of_reviews + ## review_scores_rating + bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## - number_of_reviews 1 0.08 3618.2 -54318\r## \u003cnone\u003e 3618.2 -54317\r## + cleaning_fee 1 0.00 3618.2 -54315\r## - host_identity_verified 1 1.21 3619.4 -54310\r## - host_response_rate 1 4.84 3623.0 -54283\r## - instant_bookable 1 7.16 3625.3 -54265\r## - property_type 2 7.92 3626.1 -54262\r## - beds 1 21.11 3639.3 -54162\r## - cancellation_policy 3 21.72 3639.9 -54161\r## - bathrooms 1 79.62 3697.8 -53730\r## - accommodates 1 131.42 3749.6 -53354\r## - review_scores_rating 1 188.30 3806.5 -52947\r## - bedrooms 1 216.56 3834.7 -52747\r## - zipcode 12 462.51 4080.7 -51089\r## - room_type 2 1827.08 5445.2 -43269\r## ## Step: AIC=-54318.33\r## log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + review_scores_rating + ## bedrooms + beds + zipcode\r## ## Df Sum of Sq RSS AIC\r## \u003cnone\u003e 3618.2 -54318\r## + number_of_reviews 1 0.08 3618.2 -54317\r## + cleaning_fee 1 0.00 3618.2 -54316\r## - host_identity_verified 1 1.16 3619.4 -54312\r## - host_response_rate 1 4.95 3623.2 -54283\r## - instant_bookable 1 7.27 3625.5 -54266\r## - property_type 2 7.97 3626.2 -54263\r## - cancellation_policy 3 21.66 3639.9 -54163\r## - beds 1 21.15 3639.4 -54163\r## - bathrooms 1 79.88 3698.1 -53730\r## - accommodates 1 131.34 3749.6 -53356\r## - review_scores_rating 1 188.57 3806.8 -52947\r## - bedrooms 1 218.14 3836.4 -52738\r## - zipcode 12 462.64 4080.9 -51089\r## - room_type 2 1828.54 5446.8 -43264\r그 결과는 다음과 같다.\nsummary(lm4)\r## ## Call:\r## lm(formula = log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + review_scores_rating + ## bedrooms + beds + zipcode, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.51557 -0.22551 -0.00287 0.22588 2.52586 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 2.8112298 0.0525988 53.447 \u003c 2e-16 ***\r## room_typePrivate room -0.5855993 0.0055699 -105.136 \u003c 2e-16 ***\r## room_typeShared room -1.1375240 0.0161312 -70.517 \u003c 2e-16 ***\r## property_typeHouse -0.0183360 0.0059407 -3.087 0.00203 ** ## property_typeOther 0.0422432 0.0072288 5.844 5.16e-09 ***\r## accommodates 0.0668736 0.0021358 31.311 \u003c 2e-16 ***\r## bathrooms 0.1209281 0.0049524 24.418 \u003c 2e-16 ***\r## cancellation_policy.L 0.2933262 0.0452494 6.482 9.18e-11 ***\r## cancellation_policy.Q 0.1616865 0.0337823 4.786 1.71e-06 ***\r## cancellation_policy.C 0.0664165 0.0154332 4.303 1.69e-05 ***\r## host_identity_verifiedt 0.0162382 0.0055087 2.948 0.00320 ** ## host_response_rate -0.1381528 0.0227264 -6.079 1.23e-09 ***\r## instant_bookablet -0.0365163 0.0049570 -7.367 1.80e-13 ***\r## review_scores_rating 0.0188152 0.0005015 37.518 \u003c 2e-16 ***\r## bedrooms 0.1719812 0.0042620 40.352 \u003c 2e-16 ***\r## beds -0.0390361 0.0031066 -12.566 \u003c 2e-16 ***\r## zipcode11 -0.3585074 0.0072259 -49.614 \u003c 2e-16 ***\r## zipcode1m -0.3950436 0.3661121 -1.079 0.28059 ## zipcode20 -0.2976181 0.0100374 -29.651 \u003c 2e-16 ***\r## zipcode21 -0.2358253 0.0112927 -20.883 \u003c 2e-16 ***\r## zipcode22 -0.1539659 0.0548451 -2.807 0.00500 ** ## zipcode24 -0.6528374 0.3661251 -1.783 0.07458 . ## zipcode60 -0.3329465 0.0101308 -32.865 \u003c 2e-16 ***\r## zipcode90 -0.2835116 0.0071819 -39.476 \u003c 2e-16 ***\r## zipcode91 -0.5014514 0.0112977 -44.385 \u003c 2e-16 ***\r## zipcode92 -0.8951543 0.3661830 -2.445 0.01451 * ## zipcode93 -0.6850067 0.1017839 -6.730 1.73e-11 ***\r## zipcode94 -0.2315125 0.0091763 -25.229 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.366 on 27008 degrees of freedom\r## Multiple R-squared: 0.6588, Adjusted R-squared: 0.6584 ## F-statistic: 1931 on 27 and 27008 DF, p-value: \u003c 2.2e-16\rplot(lm4,which=1)\rplot(lm4,which=2)\r## Warning: not plotting observations with leverage one:\r## 1413, 8437, 20636\r정규성과 등분산성에 문제가 없다. Y 변수가 아닌 잔차를 바탕으로 trellis plot을 그려보면 큰 문제는 보이지 않는다. (결과는 생략 eval=FALSE)\nxyplot(lm4$residuals ~ room_type, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ cancellation_policy, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ property_type, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ accommodates, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ bathrooms, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ host_identity_verified, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ host_response_rate, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ instant_bookable , data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ review_scores_rating, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ bedrooms, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ beds, data=f_df,panel=mypanel)\rxyplot(lm4$residuals ~ zipcode, data=f_df,panel=mypanel)\r즉 우리는 교호작용 없이 총 12개의 변수를 이용해 회귀분석 하였다.\n\rroom_type\rproperty_type\raccommodates\rbathrooms\rcancellation_policy\rhost_identity_verified\rhost_response_rate\rinstant_bookable\rreview_scores_rating\rbedrooms\rbeds\rzipcode\r\r이제 오버피팅을 막기 위해 데이터를 train set과 test set으로 나누어 모형 만들어 평가해 보도록 한다.\nnobs=nrow(f_df)\rset.seed(1234)\ri = sample(1:nobs, round(nobs*0.6)) #60% for training data, 40% for testdata\rtrain_df = f_df[i,] test_df = f_df[-i,]\rlm5 = lm(log_price_ratio ~ room_type + property_type + accommodates + bathrooms + cancellation_policy + host_identity_verified + host_response_rate + instant_bookable + review_scores_rating + bedrooms + beds + zipcode, data = f_df)\rsummary(lm5)\r## ## Call:\r## lm(formula = log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + review_scores_rating + ## bedrooms + beds + zipcode, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.51557 -0.22551 -0.00287 0.22588 2.52586 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 2.8112298 0.0525988 53.447 \u003c 2e-16 ***\r## room_typePrivate room -0.5855993 0.0055699 -105.136 \u003c 2e-16 ***\r## room_typeShared room -1.1375240 0.0161312 -70.517 \u003c 2e-16 ***\r## property_typeHouse -0.0183360 0.0059407 -3.087 0.00203 ** ## property_typeOther 0.0422432 0.0072288 5.844 5.16e-09 ***\r## accommodates 0.0668736 0.0021358 31.311 \u003c 2e-16 ***\r## bathrooms 0.1209281 0.0049524 24.418 \u003c 2e-16 ***\r## cancellation_policy.L 0.2933262 0.0452494 6.482 9.18e-11 ***\r## cancellation_policy.Q 0.1616865 0.0337823 4.786 1.71e-06 ***\r## cancellation_policy.C 0.0664165 0.0154332 4.303 1.69e-05 ***\r## host_identity_verifiedt 0.0162382 0.0055087 2.948 0.00320 ** ## host_response_rate -0.1381528 0.0227264 -6.079 1.23e-09 ***\r## instant_bookablet -0.0365163 0.0049570 -7.367 1.80e-13 ***\r## review_scores_rating 0.0188152 0.0005015 37.518 \u003c 2e-16 ***\r## bedrooms 0.1719812 0.0042620 40.352 \u003c 2e-16 ***\r## beds -0.0390361 0.0031066 -12.566 \u003c 2e-16 ***\r## zipcode11 -0.3585074 0.0072259 -49.614 \u003c 2e-16 ***\r## zipcode1m -0.3950436 0.3661121 -1.079 0.28059 ## zipcode20 -0.2976181 0.0100374 -29.651 \u003c 2e-16 ***\r## zipcode21 -0.2358253 0.0112927 -20.883 \u003c 2e-16 ***\r## zipcode22 -0.1539659 0.0548451 -2.807 0.00500 ** ## zipcode24 -0.6528374 0.3661251 -1.783 0.07458 . ## zipcode60 -0.3329465 0.0101308 -32.865 \u003c 2e-16 ***\r## zipcode90 -0.2835116 0.0071819 -39.476 \u003c 2e-16 ***\r## zipcode91 -0.5014514 0.0112977 -44.385 \u003c 2e-16 ***\r## zipcode92 -0.8951543 0.3661830 -2.445 0.01451 * ## zipcode93 -0.6850067 0.1017839 -6.730 1.73e-11 ***\r## zipcode94 -0.2315125 0.0091763 -25.229 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.366 on 27008 degrees of freedom\r## Multiple R-squared: 0.6588, Adjusted R-squared: 0.6584 ## F-statistic: 1931 on 27 and 27008 DF, p-value: \u003c 2.2e-16\rplot(lm5,which=1)\rplot(lm5,which=2)\r## Warning: not plotting observations with leverage one:\r## 1413, 8437, 20636\rp-value도 유의미하며(일부 zipcode는 유의미 하지 않으나 대부분이 유의미하므로 그대로 사용한다. 하지만 해당 zipcode에 대해서는 해석에 유의할 필요가 있다.), Adjusted R-squared도 0.6584이다. 잔차도 문제 없으므로 test데이터와 비교를 해본다.\n그 결과 예측결정계수,평균절대오차, MAPE는 순서대로 다음과 같다.\n## predicted values\rpred = predict(lm5, newdata=test_df, type='response')\r# predictive R^2\rcor(test_df$log_price_ratio, pred)^2\r## [1] 0.6595498\r# MAE\rmean(abs(test_df$log_price_ratio - pred))\r## [1] 0.280805\r# MAPE\rmean(abs(test_df$log_price_ratio - pred)/abs(test_df$log_price_ratio))*100\r## [1] 6.538482\r\r최종 결과\r마지막으로 우리의 최종 모형을 설명하고자 한다.\nsummary(lm5)\r## ## Call:\r## lm(formula = log_price_ratio ~ room_type + property_type + accommodates + ## bathrooms + cancellation_policy + host_identity_verified + ## host_response_rate + instant_bookable + review_scores_rating + ## bedrooms + beds + zipcode, data = f_df)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -2.51557 -0.22551 -0.00287 0.22588 2.52586 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u003e|t|) ## (Intercept) 2.8112298 0.0525988 53.447 \u003c 2e-16 ***\r## room_typePrivate room -0.5855993 0.0055699 -105.136 \u003c 2e-16 ***\r## room_typeShared room -1.1375240 0.0161312 -70.517 \u003c 2e-16 ***\r## property_typeHouse -0.0183360 0.0059407 -3.087 0.00203 ** ## property_typeOther 0.0422432 0.0072288 5.844 5.16e-09 ***\r## accommodates 0.0668736 0.0021358 31.311 \u003c 2e-16 ***\r## bathrooms 0.1209281 0.0049524 24.418 \u003c 2e-16 ***\r## cancellation_policy.L 0.2933262 0.0452494 6.482 9.18e-11 ***\r## cancellation_policy.Q 0.1616865 0.0337823 4.786 1.71e-06 ***\r## cancellation_policy.C 0.0664165 0.0154332 4.303 1.69e-05 ***\r## host_identity_verifiedt 0.0162382 0.0055087 2.948 0.00320 ** ## host_response_rate -0.1381528 0.0227264 -6.079 1.23e-09 ***\r## instant_bookablet -0.0365163 0.0049570 -7.367 1.80e-13 ***\r## review_scores_rating 0.0188152 0.0005015 37.518 \u003c 2e-16 ***\r## bedrooms 0.1719812 0.0042620 40.352 \u003c 2e-16 ***\r## beds -0.0390361 0.0031066 -12.566 \u003c 2e-16 ***\r## zipcode11 -0.3585074 0.0072259 -49.614 \u003c 2e-16 ***\r## zipcode1m -0.3950436 0.3661121 -1.079 0.28059 ## zipcode20 -0.2976181 0.0100374 -29.651 \u003c 2e-16 ***\r## zipcode21 -0.2358253 0.0112927 -20.883 \u003c 2e-16 ***\r## zipcode22 -0.1539659 0.0548451 -2.807 0.00500 ** ## zipcode24 -0.6528374 0.3661251 -1.783 0.07458 . ## zipcode60 -0.3329465 0.0101308 -32.865 \u003c 2e-16 ***\r## zipcode90 -0.2835116 0.0071819 -39.476 \u003c 2e-16 ***\r## zipcode91 -0.5014514 0.0112977 -44.385 \u003c 2e-16 ***\r## zipcode92 -0.8951543 0.3661830 -2.445 0.01451 * ## zipcode93 -0.6850067 0.1017839 -6.730 1.73e-11 ***\r## zipcode94 -0.2315125 0.0091763 -25.229 \u003c 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.366 on 27008 degrees of freedom\r## Multiple R-squared: 0.6588, Adjusted R-squared: 0.6584 ## F-statistic: 1931 on 27 and 27008 DF, p-value: \u003c 2.2e-16\rY변수 : log_price_ratio (가격비에 log를 취한 값)\r\rX변수 : room_type + property_type + accommodates + bathrooms + cancellation_policy + host_identity_verified + host_response_rate + instant_bookable + review_scores_rating + bedrooms + beds + zipcod\r위의 Coefficients의 Estimate를 보면 그 결과를 알 수 있으나 간단히 설명하면 다음과 같다.\n\rcancellation_policy가 “super_strict_30”에서 “flexible”으로 변하면 0.2750713만큼 log_price_ratio가 증가한다.\n\rcancellation_policy가 “super_strict_30”에서 “moderate”으로 변하면 0.1480741만큼 log_price_ratio가 증가한다.\n\rcancellation_policy가 “super_strict_30”에서 “strict”으로 변하면 0.0603244만큼 log_price_ratio가 증가한다.\n\rroom_type가 Entire home/apt에서 “Private room”으로 변하면 0.5854706만큼 log_price_ratio가 감소한다.\n\rroom_type가 Entire home/apt에서 “Shared room”으로 변하면 1.1371925만큼 log_price_ratio가 감소한다.\n\rproperty_type가 “Apartment”에서 “House”으로 변하면 0.0184556만큼 log_price_ratio가 감소한다.\n\rproperty_type가 “Apartment”에서 “Other”으로 변하면 0.0422110만큼 log_price_ratio가 증가한다.\n\rhost_identity_verified가 “f”에서 “t”으로 변하면 0.0162358만큼 log_price_ratio가 증가한다.\n\rbedrooms의 개수가 1개 증가할 수록 0.1718274만큼 log_price_ratio가 증가한다.\n\rbeds의 수가 1개 증가할수록 0.0392406만큼 log_price_ratio가 감소한다.\n\rbathrooms의 개수가 1개 증가할 수록 0.1208864만큼 log_price_ratio가 증가한다.\n\raccommodate가 1 증가할 수록 0.0670576만큼 log_price_ratio가 증가한다.\n\rhost_response_rate가 1 증가할 수록 0.1385022만큼 log_price_ratio가 감소한다.\n\rinstant_bookablet가 1 증가할 수록 0.0365038만큼 log_price_ratio가 감소한다.\n\rreview_scores_rating가 1 증가할 수록 0.0188218만큼 log_price_ratio가 증가한다.\n\rzipcode의 경우 10###에서 각 해당 코드로 변했을떄 log_price_ratio가 얼마나 감소했는지를 의미한다. 다만 zipcode1m###과 zipcode24###는 설명하지 않는다.\n\r\r\r\r","description":"","tags":["data mining","logistic"],"title":"데이터마이닝 - 로지스틱 회귀분석","uri":"/posts/data_mining/1/"}]
